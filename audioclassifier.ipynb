{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU detected\n",
      "TensorFlow is using CUDA version: 64_112\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if TensorFlow detects GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPUs detected:\", gpus)\n",
    "else:\n",
    "    print(\"No GPU detected\")\n",
    "\n",
    "# Get and print TensorFlow's CUDA version\n",
    "cuda_version = tf.sysconfig.get_build_info()['cuda_version']\n",
    "print(f\"TensorFlow is using CUDA version: {cuda_version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19892\\1379564342.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msoundfile\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to load and preprocess audio files\n",
    "def preprocess_audio(file_path, target_sr=16000, duration=5):\n",
    "    # Load the audio file\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "    \n",
    "    # Trim or pad the audio to the target duration (in seconds)\n",
    "    audio_length = duration * sr\n",
    "    if len(audio) < audio_length:\n",
    "        audio = np.pad(audio, (0, audio_length - len(audio)))\n",
    "    else:\n",
    "        audio = audio[:audio_length]\n",
    "    \n",
    "    return audio\n",
    "\n",
    "# Function to save preprocessed audio data\n",
    "def save_audio(file_path, audio_data):\n",
    "    sf.write(file_path, audio_data, 16000)\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(data_dir, labels):\n",
    "    audio_files = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for label in labels:\n",
    "        files = os.listdir(os.path.join(data_dir, label))\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                audio_file_path = os.path.join(data_dir, label, file)\n",
    "                audio = preprocess_audio(audio_file_path)\n",
    "                audio_files.append(audio)\n",
    "                labels_list.append(label)\n",
    "    \n",
    "    return np.array(audio_files), np.array(labels_list)\n",
    "\n",
    "# Example usage: Load your dataset (make sure you have your paths set up)\n",
    "data_dir = 'path_to_audio_data'  # Replace with actual path to dataset\n",
    "labels = ['cry', 'scream', 'normal']  # Categories\n",
    "audio_data, audio_labels = load_dataset(data_dir, labels)\n",
    "\n",
    "# Split into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(audio_data, audio_labels, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Fine-Tuning (YAMNet & Wav2Vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load pre-trained YAMNet model from TensorFlow Hub\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Load pre-trained YAMNet model from TensorFlow Hub\n",
    "yamnet_model = hub.load('https://tfhub.dev/google/yamnet/1')\n",
    "\n",
    "# Convert audio into features for YAMNet\n",
    "def extract_yamnet_features(audio_data):\n",
    "    # Convert audio data to a tensor\n",
    "    audio_data_tensor = tf.convert_to_tensor(audio_data, dtype=tf.float32)\n",
    "    \n",
    "    # Make predictions with YAMNet model\n",
    "    scores, embeddings, spectrogram = yamnet_model(audio_data_tensor)\n",
    "    \n",
    "    return scores.numpy()\n",
    "\n",
    "# Example: Extract features from one audio sample\n",
    "features = extract_yamnet_features(X_train[0])\n",
    "print(features.shape)  # Check shape of the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load pre-trained Wav2Vec2 model and processor\n",
    "model_name = \"facebook/wav2vec2-large-xlsr-53\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "\n",
    "# Example of preprocessing audio for Wav2Vec2\n",
    "def preprocess_for_wav2vec2(audio_data):\n",
    "    inputs = processor(audio_data, return_tensors=\"pt\", sampling_rate=16000, padding=True)\n",
    "    return inputs\n",
    "\n",
    "# Prepare dataset for training\n",
    "def prepare_data_for_training(audio_data, labels):\n",
    "    inputs = []\n",
    "    for audio in audio_data:\n",
    "        inputs.append(preprocess_for_wav2vec2(audio))\n",
    "    return inputs\n",
    "\n",
    "# Example: Prepare dataset\n",
    "train_inputs = prepare_data_for_training(X_train, y_train)\n",
    "\n",
    "# Create DataLoader for training\n",
    "train_dataloader = DataLoader(train_inputs, batch_size=16)\n",
    "\n",
    "# Example of training loop (simplified)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "model.train()\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function for Majority Voting or Averaging (Ensemble)\n",
    "def ensemble_predict(model_1_preds, model_2_preds, method='majority'):\n",
    "    if method == 'majority':\n",
    "        # Majority voting\n",
    "        combined_preds = [np.bincount([pred1, pred2]).argmax() for pred1, pred2 in zip(model_1_preds, model_2_preds)]\n",
    "    elif method == 'average':\n",
    "        # Averaging the probabilities\n",
    "        combined_preds = (model_1_preds + model_2_preds) / 2\n",
    "        combined_preds = np.argmax(combined_preds, axis=1)\n",
    "    \n",
    "    return combined_preds\n",
    "\n",
    "# Example usage:\n",
    "model_1_preds = yamnet_predictions  # Replace with actual YAMNet predictions\n",
    "model_2_preds = wav2vec2_predictions  # Replace with actual Wav2Vec2 predictions\n",
    "\n",
    "ensemble_preds = ensemble_predict(model_1_preds, model_2_preds, method='majority')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Example: Evaluate the ensemble model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, ensemble_preds))\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, ensemble_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deployment with Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from temporalio import workflow, activity\n",
    "from temporalio.client import Client\n",
    "\n",
    "@activity.defn\n",
    "async def preprocess_audio_activity(audio_file_path: str):\n",
    "    # Your audio preprocessing logic here\n",
    "    return preprocessed_audio\n",
    "\n",
    "@activity.defn\n",
    "async def classify_audio_activity(audio_data):\n",
    "    # Your classification logic using ensemble model here\n",
    "    return ensemble_predictions\n",
    "\n",
    "@workflow.defn\n",
    "class AudioClassificationWorkflow:\n",
    "    @workflow.run\n",
    "    async def run(self, audio_file_path: str):\n",
    "        # Define the workflow tasks\n",
    "        preprocessed_audio = await workflow.execute_activity(preprocess_audio_activity, audio_file_path)\n",
    "        predictions = await workflow.execute_activity(classify_audio_activity, preprocessed_audio)\n",
    "        return predictions\n",
    "\n",
    "# Start the Temporal client\n",
    "client = await Client.connect(\"localhost:7233\")\n",
    "# Create and run the workflow\n",
    "workflow_id = \"audio-classification-workflow\"\n",
    "result = await client.execute_workflow(AudioClassificationWorkflow.run, audio_file_path=\"path_to_audio_file\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
