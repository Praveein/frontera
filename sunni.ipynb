{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu118\n",
      "CUDA Available: True\n",
      "CUDA Version: 11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:1400: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "sunni punda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "import torch\n",
    "import params as yamnet_params\n",
    "import yamnet as yamnet_model\n",
    "params = yamnet_params.Params()\n",
    "yamnet_model = yamnet_model.yamnet_frames_model(params)\n",
    "yamnet_model.load_weights('yamnet.h5')\n",
    "print(\"sunni punda\")\n",
    "from transformers import Wav2Vec2Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wav2vec2_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\", torch_dtype=torch.float16).to(device)\n",
    "\n",
    "#wav2vec2_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tuning wav2vec model\n",
    "from datasets import Dataset\n",
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function to load and preprocess the audio files\n",
    "def load_audio(file_path):\n",
    "    audio, _ = librosa.load(file_path, sr=16000)\n",
    "    return audio\n",
    "\n",
    "# Define your dataset class\n",
    "def create_dataset(ontology_data, class_map):\n",
    "    data = []\n",
    "    for category in ontology_data:\n",
    "        for url in category.get(\"positive_examples\", []):\n",
    "            # Extract start and end times from the URL\n",
    "            start_time, end_time = extract_times(url)  # You'll need the function extract_times\n",
    "\n",
    "            file_name = f\"{category['name']}.wav\"\n",
    "            download_audio_from_url(url, start_time, end_time, file_name, \"audio_directory\")\n",
    "\n",
    "            # Load the audio file\n",
    "            audio_data = load_audio(os.path.join(\"audio_directory\", file_name))\n",
    "\n",
    "            # Map the class\n",
    "            label = class_map[category['id']]  # Use your CSV for this mapping\n",
    "\n",
    "            data.append({\"audio\": audio_data, \"label\": label})\n",
    "    \n",
    "    return Dataset.from_pandas(pd.DataFrame(data))  # Convert to Hugging Face dataset format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Define training arguments\u001b[39;00m\n\u001b[0;32m      4\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      5\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,          \u001b[38;5;66;03m# output directory\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,     \u001b[38;5;66;03m# evaluation strategy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m\"\u001b[39m,            \u001b[38;5;66;03m# directory for storing logs\u001b[39;00m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mwav2vec2_model,                         \u001b[38;5;66;03m# the model to be fine-tuned\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,                  \u001b[38;5;66;03m# training arguments\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_dataset\u001b[49m,         \u001b[38;5;66;03m# the training dataset\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,           \u001b[38;5;66;03m# evaluation dataset\u001b[39;00m\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",          # output directory\n",
    "    evaluation_strategy=\"epoch\",     # evaluation strategy\n",
    "    learning_rate=2e-5,              # learning rate\n",
    "    per_device_train_batch_size=16,  # batch size for training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    logging_dir=\"./logs\",            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=wav2vec2_model,                         # the model to be fine-tuned\n",
    "    args=training_args,                  # training arguments\n",
    "    train_dataset=train_dataset,         # the training dataset\n",
    "    eval_dataset=eval_dataset,           # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"wav2vec2_model\"  # Path where you want to save the model\n",
    "wav2vec2_model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Function to load and preprocess audio for YAMNet\n",
    "def preprocess_audio_yamnet(audio_path):\n",
    "    waveform, sample_rate = librosa.load(audio_path, sr=16000)  # 16 kHz\n",
    "    return waveform\n",
    "\n",
    "# Function to load and preprocess audio for Wav2Vec2\n",
    "def preprocess_audio_wav2vec2(audio_path):\n",
    "    waveform, sample_rate = librosa.load(audio_path, sr=16000)  # 16 kHz\n",
    "    return waveform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Function to load and preprocess audio for YAMNet\n",
    "def preprocess_audio_yamnet(audio_path):\n",
    "    waveform, sample_rate = librosa.load(audio_path, sr=16000)  # 16 kHz\n",
    "    return waveform\n",
    "\n",
    "# Function to load and preprocess audio for Wav2Vec2\n",
    "def preprocess_audio_wav2vec2(audio_path):\n",
    "    waveform, sample_rate = librosa.load(audio_path, sr=16000)  # 16 kHz\n",
    "    return waveform\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
