{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Speech': '/m/09x0r', 'Child speech, kid speaking': '/m/0ytgt', 'Conversation': '/m/01h8n0', 'Narration, monologue': '/m/02qldy', 'Babbling': '/m/0261r1', 'Speech synthesizer': '/m/0brhx', 'Shout': '/m/07p6fty', 'Bellow': '/m/07q4ntr', 'Whoop': '/m/07rwj3x', 'Yell': '/m/07sr1lc', 'Children shouting': '/t/dd00135', 'Screaming': '/m/03qc9zr', 'Whispering': '/m/02rtxlg', 'Laughter': '/m/01j3sz', 'Baby laughter': '/t/dd00001', 'Giggle': '/m/07r660_', 'Snicker': '/m/07s04w4', 'Belly laugh': '/m/07sq110', 'Chuckle, chortle': '/m/07rgt08', 'Crying, sobbing': '/m/0463cq4', 'Baby cry, infant cry': '/t/dd00002', 'Whimper': '/m/07qz6j3', 'Wail, moan': '/m/07qw_06', 'Sigh': '/m/07plz5l', 'Singing': '/m/015lz1', 'Choir': '/m/0l14jd', 'Yodeling': '/m/01swy6', 'Chant': '/m/02bk07', 'Mantra': '/m/01c194', 'Child singing': '/t/dd00005', 'Synthetic singing': '/t/dd00006', 'Rapping': '/m/06bxc', 'Humming': '/m/02fxyj', 'Groan': '/m/07s2xch', 'Grunt': '/m/07r4k75', 'Whistling': '/m/01w250', 'Breathing': '/m/0lyf6', 'Wheeze': '/m/07mzm6', 'Snoring': '/m/01d3sd', 'Gasp': '/m/07s0dtb', 'Pant': '/m/07pyy8b', 'Snort': '/m/07q0yl5', 'Cough': '/m/01b_21', 'Throat clearing': '/m/0dl9sf8', 'Sneeze': '/m/01hsr_', 'Sniff': '/m/07ppn3j', 'Run': '/m/06h7j', 'Shuffle': '/m/07qv_x_', 'Walk, footsteps': '/m/07pbtc8', 'Chewing, mastication': '/m/03cczk', 'Biting': '/m/07pdhp0', 'Gargling': '/m/0939n_', 'Stomach rumble': '/m/01g90h', 'Burping, eructation': '/m/03q5_w', 'Hiccup': '/m/02p3nc', 'Fart': '/m/02_nn', 'Hands': '/m/0k65p', 'Finger snapping': '/m/025_jnm', 'Clapping': '/m/0l15bq', 'Heart sounds, heartbeat': '/m/01jg02', 'Heart murmur': '/m/01jg1z', 'Cheering': '/m/053hz1', 'Applause': '/m/028ght', 'Chatter': '/m/07rkbfh', 'Crowd': '/m/03qtwd', 'Hubbub, speech noise, speech babble': '/m/07qfr4h', 'Children playing': '/t/dd00013', 'Animal': '/m/0jbk', 'Domestic animals, pets': '/m/068hy', 'Dog': '/m/0bt9lr', 'Bark': '/m/05tny_', 'Yip': '/m/07r_k2n', 'Howl': '/m/07qf0zm', 'Bow-wow': '/m/07rc7d9', 'Growling': '/m/0ghcn6', 'Whimper (dog)': '/t/dd00136', 'Cat': '/m/01yrx', 'Purr': '/m/02yds9', 'Meow': '/m/07qrkrw', 'Hiss': '/m/07rjwbb', 'Caterwaul': '/m/07r81j2', 'Livestock, farm animals, working animals': '/m/0ch8v', 'Horse': '/m/03k3r', 'Clip-clop': '/m/07rv9rh', 'Neigh, whinny': '/m/07q5rw0', 'Cattle, bovinae': '/m/01xq0k1', 'Moo': '/m/07rpkh9', 'Cowbell': '/m/0239kh', 'Pig': '/m/068zj', 'Oink': '/t/dd00018', 'Goat': '/m/03fwl', 'Bleat': '/m/07q0h5t', 'Sheep': '/m/07bgp', 'Fowl': '/m/025rv6n', 'Chicken, rooster': '/m/09b5t', 'Cluck': '/m/07st89h', 'Crowing, cock-a-doodle-doo': '/m/07qn5dc', 'Turkey': '/m/01rd7k', 'Gobble': '/m/07svc2k', 'Duck': '/m/09ddx', 'Quack': '/m/07qdb04', 'Goose': '/m/0dbvp', 'Honk': '/m/07qwf61', 'Wild animals': '/m/01280g', 'Roaring cats (lions, tigers)': '/m/0cdnk', 'Roar': '/m/04cvmfc', 'Bird': '/m/015p6', 'Bird vocalization, bird call, bird song': '/m/020bb7', 'Chirp, tweet': '/m/07pggtn', 'Squawk': '/m/07sx8x_', 'Pigeon, dove': '/m/0h0rv', 'Coo': '/m/07r_25d', 'Crow': '/m/04s8yn', 'Caw': '/m/07r5c2p', 'Owl': '/m/09d5_', 'Hoot': '/m/07r_80w', 'Bird flight, flapping wings': '/m/05_wcq', 'Canidae, dogs, wolves': '/m/01z5f', 'Rodents, rats, mice': '/m/06hps', 'Mouse': '/m/04rmv', 'Patter': '/m/07r4gkf', 'Insect': '/m/03vt0', 'Cricket': '/m/09xqv', 'Mosquito': '/m/09f96', 'Fly, housefly': '/m/0h2mp', 'Buzz': '/m/07pjwq1', 'Bee, wasp, etc.': '/m/01h3n', 'Frog': '/m/09ld4', 'Croak': '/m/07st88b', 'Snake': '/m/078jl', 'Rattle': '/m/07qn4z3', 'Whale vocalization': '/m/032n05', 'Music': '/m/04rlf', 'Musical instrument': '/m/04szw', 'Plucked string instrument': '/m/0fx80y', 'Guitar': '/m/0342h', 'Electric guitar': '/m/02sgy', 'Bass guitar': '/m/018vs', 'Acoustic guitar': '/m/042v_gx', 'Steel guitar, slide guitar': '/m/06w87', 'Tapping (guitar technique)': '/m/01glhc', 'Strum': '/m/07s0s5r', 'Banjo': '/m/018j2', 'Sitar': '/m/0jtg0', 'Mandolin': '/m/04rzd', 'Zither': '/m/01bns_', 'Ukulele': '/m/07xzm', 'Keyboard (musical)': '/m/05148p4', 'Piano': '/m/05r5c', 'Electric piano': '/m/01s0ps', 'Organ': '/m/013y1f', 'Electronic organ': '/m/03xq_f', 'Hammond organ': '/m/03gvt', 'Synthesizer': '/m/0l14qv', 'Sampler': '/m/01v1d8', 'Harpsichord': '/m/03q5t', 'Percussion': '/m/0l14md', 'Drum kit': '/m/02hnl', 'Drum machine': '/m/0cfdd', 'Drum': '/m/026t6', 'Snare drum': '/m/06rvn', 'Rimshot': '/m/03t3fj', 'Drum roll': '/m/02k_mr', 'Bass drum': '/m/0bm02', 'Timpani': '/m/011k_j', 'Tabla': '/m/01p970', 'Cymbal': '/m/01qbl', 'Hi-hat': '/m/03qtq', 'Wood block': '/m/01sm1g', 'Tambourine': '/m/07brj', 'Rattle (instrument)': '/m/05r5wn', 'Maraca': '/m/0xzly', 'Gong': '/m/0mbct', 'Tubular bells': '/m/016622', 'Mallet percussion': '/m/0j45pbj', 'Marimba, xylophone': '/m/0dwsp', 'Glockenspiel': '/m/0dwtp', 'Vibraphone': '/m/0dwt5', 'Steelpan': '/m/0l156b', 'Orchestra': '/m/05pd6', 'Brass instrument': '/m/01kcd', 'French horn': '/m/0319l', 'Trumpet': '/m/07gql', 'Trombone': '/m/07c6l', 'Bowed string instrument': '/m/0l14_3', 'String section': '/m/02qmj0d', 'Violin, fiddle': '/m/07y_7', 'Pizzicato': '/m/0d8_n', 'Cello': '/m/01xqw', 'Double bass': '/m/02fsn', 'Wind instrument, woodwind instrument': '/m/085jw', 'Flute': '/m/0l14j_', 'Saxophone': '/m/06ncr', 'Clarinet': '/m/01wy6', 'Harp': '/m/03m5k', 'Bell': '/m/0395lw', 'Church bell': '/m/03w41f', 'Jingle bell': '/m/027m70_', 'Bicycle bell': '/m/0gy1t2s', 'Tuning fork': '/m/07n_g', 'Chime': '/m/0f8s22', 'Wind chime': '/m/026fgl', 'Change ringing (campanology)': '/m/0150b9', 'Harmonica': '/m/03qjg', 'Accordion': '/m/0mkg', 'Bagpipes': '/m/0192l', 'Didgeridoo': '/m/02bxd', 'Shofar': '/m/0l14l2', 'Theremin': '/m/07kc_', 'Singing bowl': '/m/0l14t7', 'Scratching (performance technique)': '/m/01hgjl', 'Pop music': '/m/064t9', 'Hip hop music': '/m/0glt670', 'Beatboxing': '/m/02cz_7', 'Rock music': '/m/06by7', 'Heavy metal': '/m/03lty', 'Punk rock': '/m/05r6t', 'Grunge': '/m/0dls3', 'Progressive rock': '/m/0dl5d', 'Rock and roll': '/m/07sbbz2', 'Psychedelic rock': '/m/05w3f', 'Rhythm and blues': '/m/06j6l', 'Soul music': '/m/0gywn', 'Reggae': '/m/06cqb', 'Country': '/m/01lyv', 'Swing music': '/m/015y_n', 'Bluegrass': '/m/0gg8l', 'Funk': '/m/02x8m', 'Folk music': '/m/02w4v', 'Middle Eastern music': '/m/06j64v', 'Jazz': '/m/03_d0', 'Disco': '/m/026z9', 'Classical music': '/m/0ggq0m', 'Opera': '/m/05lls', 'Electronic music': '/m/02lkt', 'House music': '/m/03mb9', 'Techno': '/m/07gxw', 'Dubstep': '/m/07s72n', 'Drum and bass': '/m/0283d', 'Electronica': '/m/0m0jc', 'Electronic dance music': '/m/08cyft', 'Ambient music': '/m/0fd3y', 'Trance music': '/m/07lnk', 'Music of Latin America': '/m/0g293', 'Salsa music': '/m/0ln16', 'Flamenco': '/m/0326g', 'Blues': '/m/0155w', 'Music for children': '/m/05fw6t', 'New-age music': '/m/02v2lh', 'Vocal music': '/m/0y4f8', 'A capella': '/m/0z9c', 'Music of Africa': '/m/0164x2', 'Afrobeat': '/m/0145m', 'Christian music': '/m/02mscn', 'Gospel music': '/m/016cjb', 'Music of Asia': '/m/028sqc', 'Carnatic music': '/m/015vgc', 'Music of Bollywood': '/m/0dq0md', 'Ska': '/m/06rqw', 'Traditional music': '/m/02p0sh1', 'Independent music': '/m/05rwpb', 'Song': '/m/074ft', 'Background music': '/m/025td0t', 'Theme music': '/m/02cjck', 'Jingle (music)': '/m/03r5q_', 'Soundtrack music': '/m/0l14gg', 'Lullaby': '/m/07pkxdp', 'Video game music': '/m/01z7dr', 'Christmas music': '/m/0140xf', 'Dance music': '/m/0ggx5q', 'Wedding music': '/m/04wptg', 'Happy music': '/t/dd00031', 'Sad music': '/t/dd00033', 'Tender music': '/t/dd00034', 'Exciting music': '/t/dd00035', 'Angry music': '/t/dd00036', 'Scary music': '/t/dd00037', 'Wind': '/m/03m9d0z', 'Rustling leaves': '/m/09t49', 'Wind noise (microphone)': '/t/dd00092', 'Thunderstorm': '/m/0jb2l', 'Thunder': '/m/0ngt1', 'Water': '/m/0838f', 'Rain': '/m/06mb1', 'Raindrop': '/m/07r10fb', 'Rain on surface': '/t/dd00038', 'Stream': '/m/0j6m2', 'Waterfall': '/m/0j2kx', 'Ocean': '/m/05kq4', 'Waves, surf': '/m/034srq', 'Steam': '/m/06wzb', 'Gurgling': '/m/07swgks', 'Fire': '/m/02_41', 'Crackle': '/m/07pzfmf', 'Vehicle': '/m/07yv9', 'Boat, Water vehicle': '/m/019jd', 'Sailboat, sailing ship': '/m/0hsrw', 'Rowboat, canoe, kayak': '/m/056ks2', 'Motorboat, speedboat': '/m/02rlv9', 'Ship': '/m/06q74', 'Motor vehicle (road)': '/m/012f08', 'Car': '/m/0k4j', 'Vehicle horn, car horn, honking': '/m/0912c9', 'Toot': '/m/07qv_d5', 'Car alarm': '/m/02mfyn', 'Power windows, electric windows': '/m/04gxbd', 'Skidding': '/m/07rknqz', 'Tire squeal': '/m/0h9mv', 'Car passing by': '/t/dd00134', 'Race car, auto racing': '/m/0ltv', 'Truck': '/m/07r04', 'Air brake': '/m/0gvgw0', 'Air horn, truck horn': '/m/05x_td', 'Reversing beeps': '/m/02rhddq', 'Ice cream truck, ice cream van': '/m/03cl9h', 'Bus': '/m/01bjv', 'Emergency vehicle': '/m/03j1ly', 'Police car (siren)': '/m/04qvtq', 'Ambulance (siren)': '/m/012n7d', 'Fire engine, fire truck (siren)': '/m/012ndj', 'Motorcycle': '/m/04_sv', 'Traffic noise, roadway noise': '/m/0btp2', 'Rail transport': '/m/06d_3', 'Train': '/m/07jdr', 'Train whistle': '/m/04zmvq', 'Train horn': '/m/0284vy3', 'Railroad car, train wagon': '/m/01g50p', 'Train wheels squealing': '/t/dd00048', 'Subway, metro, underground': '/m/0195fx', 'Aircraft': '/m/0k5j', 'Aircraft engine': '/m/014yck', 'Jet engine': '/m/04229', 'Propeller, airscrew': '/m/02l6bg', 'Helicopter': '/m/09ct_', 'Fixed-wing aircraft, airplane': '/m/0cmf2', 'Bicycle': '/m/0199g', 'Skateboard': '/m/06_fw', 'Engine': '/m/02mk9', 'Light engine (high frequency)': '/t/dd00065', \"Dental drill, dentist's drill\": '/m/08j51y', 'Lawn mower': '/m/01yg9g', 'Chainsaw': '/m/01j4z9', 'Medium engine (mid frequency)': '/t/dd00066', 'Heavy engine (low frequency)': '/t/dd00067', 'Engine knocking': '/m/01h82_', 'Engine starting': '/t/dd00130', 'Idling': '/m/07pb8fc', 'Accelerating, revving, vroom': '/m/07q2z82', 'Door': '/m/02dgv', 'Doorbell': '/m/03wwcy', 'Ding-dong': '/m/07r67yg', 'Sliding door': '/m/02y_763', 'Slam': '/m/07rjzl8', 'Knock': '/m/07r4wb8', 'Tap': '/m/07qcpgn', 'Squeak': '/m/07q6cd_', 'Cupboard open or close': '/m/0642b4', 'Drawer open or close': '/m/0fqfqc', 'Dishes, pots, and pans': '/m/04brg2', 'Cutlery, silverware': '/m/023pjk', 'Chopping (food)': '/m/07pn_8q', 'Frying (food)': '/m/0dxrf', 'Microwave oven': '/m/0fx9l', 'Blender': '/m/02pjr4', 'Water tap, faucet': '/m/02jz0l', 'Sink (filling or washing)': '/m/0130jx', 'Bathtub (filling or washing)': '/m/03dnzn', 'Hair dryer': '/m/03wvsk', 'Toilet flush': '/m/01jt3m', 'Toothbrush': '/m/012xff', 'Electric toothbrush': '/m/04fgwm', 'Vacuum cleaner': '/m/0d31p', 'Zipper (clothing)': '/m/01s0vc', 'Keys jangling': '/m/03v3yw', 'Coin (dropping)': '/m/0242l', 'Scissors': '/m/01lsmm', 'Electric shaver, electric razor': '/m/02g901', 'Shuffling cards': '/m/05rj2', 'Typing': '/m/0316dw', 'Typewriter': '/m/0c2wf', 'Computer keyboard': '/m/01m2v', 'Writing': '/m/081rb', 'Alarm': '/m/07pp_mv', 'Telephone': '/m/07cx4', 'Telephone bell ringing': '/m/07pp8cl', 'Ringtone': '/m/01hnzm', 'Telephone dialing, DTMF': '/m/02c8p', 'Dial tone': '/m/015jpf', 'Busy signal': '/m/01z47d', 'Alarm clock': '/m/046dlr', 'Siren': '/m/03kmc9', 'Civil defense siren': '/m/0dgbq', 'Buzzer': '/m/030rvx', 'Smoke detector, smoke alarm': '/m/01y3hg', 'Fire alarm': '/m/0c3f7m', 'Foghorn': '/m/04fq5q', 'Whistle': '/m/0l156k', 'Steam whistle': '/m/06hck5', 'Mechanisms': '/t/dd00077', 'Ratchet, pawl': '/m/02bm9n', 'Clock': '/m/01x3z', 'Tick': '/m/07qjznt', 'Tick-tock': '/m/07qjznl', 'Gears': '/m/0l7xg', 'Pulleys': '/m/05zc1', 'Sewing machine': '/m/0llzx', 'Mechanical fan': '/m/02x984l', 'Air conditioning': '/m/025wky1', 'Cash register': '/m/024dl', 'Printer': '/m/01m4t', 'Camera': '/m/0dv5r', 'Single-lens reflex camera': '/m/07bjf', 'Tools': '/m/07k1x', 'Hammer': '/m/03l9g', 'Jackhammer': '/m/03p19w', 'Sawing': '/m/01b82r', 'Filing (rasp)': '/m/02p01q', 'Sanding': '/m/023vsd', 'Power tool': '/m/0_ksk', 'Drill': '/m/01d380', 'Explosion': '/m/014zdl', 'Gunshot, gunfire': '/m/032s66', 'Machine gun': '/m/04zjc', 'Fusillade': '/m/02z32qm', 'Artillery fire': '/m/0_1c', 'Cap gun': '/m/073cg4', 'Fireworks': '/m/0g6b5', 'Firecracker': '/g/122z_qxw', 'Burst, pop': '/m/07qsvvw', 'Eruption': '/m/07pxg6y', 'Boom': '/m/07qqyl4', 'Wood': '/m/083vt', 'Chop': '/m/07pczhz', 'Splinter': '/m/07pl1bw', 'Crack': '/m/07qs1cx', 'Glass': '/m/039jq', 'Chink, clink': '/m/07q7njn', 'Shatter': '/m/07rn7sz', 'Liquid': '/m/04k94', 'Splash, splatter': '/m/07rrlb6', 'Slosh': '/m/07p6mqd', 'Squish': '/m/07qlwh6', 'Drip': '/m/07r5v4s', 'Pour': '/m/07prgkl', 'Trickle, dribble': '/m/07pqc89', 'Gush': '/t/dd00088', 'Fill (with liquid)': '/m/07p7b8y', 'Spray': '/m/07qlf79', 'Pump (liquid)': '/m/07ptzwd', 'Stir': '/m/07ptfmf', 'Boiling': '/m/0dv3j', 'Sonar': '/m/0790c', 'Arrow': '/m/0dl83', 'Whoosh, swoosh, swish': '/m/07rqsjt', 'Thump, thud': '/m/07qnq_y', 'Thunk': '/m/07rrh0c', 'Electronic tuner': '/m/0b_fwt', 'Effects unit': '/m/02rr_', 'Chorus effect': '/m/07m2kt', 'Basketball bounce': '/m/018w8', 'Bang': '/m/07pws3f', 'Slap, smack': '/m/07ryjzk', 'Whack, thwack': '/m/07rdhzs', 'Smash, crash': '/m/07pjjrj', 'Breaking': '/m/07pc8lb', 'Bouncing': '/m/07pqn27', 'Whip': '/m/07rbp7_', 'Flap': '/m/07pyf11', 'Scratch': '/m/07qb_dv', 'Scrape': '/m/07qv4k0', 'Rub': '/m/07pdjhy', 'Roll': '/m/07s8j8t', 'Crushing': '/m/07plct2', 'Crumpling, crinkling': '/t/dd00112', 'Tearing': '/m/07qcx4z', 'Beep, bleep': '/m/02fs_r', 'Ping': '/m/07qwdck', 'Ding': '/m/07phxs1', 'Clang': '/m/07rv4dm', 'Squeal': '/m/07s02z0', 'Creak': '/m/07qh7jl', 'Rustle': '/m/07qwyj0', 'Whir': '/m/07s34ls', 'Clatter': '/m/07qmpdm', 'Sizzle': '/m/07p9k1k', 'Clicking': '/m/07qc9xj', 'Clickety-clack': '/m/07rwm0c', 'Rumble': '/m/07phhsh', 'Plop': '/m/07qyrcz', 'Jingle, tinkle': '/m/07qfgpx', 'Hum': '/m/07rcgpl', 'Zing': '/m/07p78v5', 'Boing': '/t/dd00121', 'Crunch': '/m/07s12q4', 'Silence': '/m/028v0c', 'Sine wave': '/m/01v_m0', 'Harmonic': '/m/0b9m1', 'Chirp tone': '/m/0hdsk', 'Sound effect': '/m/0c1dj', 'Pulse': '/m/07pt_g0', 'Inside, small room': '/t/dd00125', 'Inside, large room or hall': '/t/dd00126', 'Inside, public space': '/t/dd00127', 'Outside, urban or manmade': '/t/dd00128', 'Outside, rural or natural': '/t/dd00129', 'Reverberation': '/m/01b9nn', 'Echo': '/m/01jnbd', 'Noise': '/m/096m7z', 'Environmental noise': '/m/06_y0by', 'Static': '/m/07rgkc5', 'Mains hum': '/m/06xkwv', 'Distortion': '/m/0g12c5', 'Sidetone': '/m/08p9q4', 'Cacophony': '/m/07szfh9', 'White noise': '/m/0chx_', 'Pink noise': '/m/0cj0r', 'Throbbing': '/m/07p_0gm', 'Vibration': '/m/01jwx6', 'Television': '/m/07c52', 'Radio': '/m/06bz3', 'Field recording': '/m/07hvw1'}\n"
     ]
    }
   ],
   "source": [
    "class_map = pd.read_csv('yamnet_class_map.csv').set_index('display_name').to_dict()['mid']\n",
    "print(class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'/m/07pggtn': 0, '/m/01jg02': 1, '/t/dd00067': 2, '/m/06ncr': 3, '/m/0dgbq': 4, '/m/0395lw': 5, '/m/03dnzn': 6, '/m/07szfh9': 7, '/m/015lz1': 8, '/m/02rhddq': 9, '/m/04szw': 10, '/m/01v1d8': 11, '/m/07rn7sz': 12, '/m/02fsn': 13, '/m/0fqfqc': 14, '/m/03cczk': 15, '/m/03gvt': 16, '/m/07r81j2': 17, '/m/07qn5dc': 18, '/m/0b_fwt': 19, '/m/0c3f7m': 20, '/m/01v_m0': 21, '/m/07s12q4': 22, '/m/02p3nc': 23, '/m/07plct2': 24, '/t/dd00035': 25, '/m/06hps': 26, '/m/07pczhz': 27, '/m/07qfgpx': 28, '/m/07prgkl': 29, '/m/07rv4dm': 30, '/m/05rwpb': 31, '/m/019jd': 32, '/m/01g90h': 33, '/m/0ytgt': 34, '/m/04fq5q': 35, '/m/0l7xg': 36, '/m/04cvmfc': 37, '/m/0164x2': 38, '/m/07rc7d9': 39, '/m/01glhc': 40, '/m/01rd7k': 41, '/m/0h0rv': 42, '/m/0hsrw': 43, '/m/05kq4': 44, '/m/05r5c': 45, '/m/0jb2l': 46, '/m/01lyv': 47, '/m/01swy6': 48, '/m/04s8yn': 49, '/m/05rj2': 50, '/m/07bgp': 51, '/t/dd00136': 52, '/m/07cx4': 53, '/m/015p6': 54, '/m/03kmc9': 55, '/m/046dlr': 56, '/m/07s02z0': 57, '/m/07rrlb6': 58, '/m/02x8m': 59, '/m/02rlv9': 60, '/m/02lkt': 61, '/m/07st88b': 62, '/m/07s34ls': 63, '/m/0ggq0m': 64, '/m/08p9q4': 65, '/m/07lnk': 66, '/m/07p6mqd': 67, '/m/0ch8v': 68, '/m/06j6l': 69, '/m/01w250': 70, '/m/01g50p': 71, '/m/0g12c5': 72, '/m/08cyft': 73, '/m/0l156k': 74, '/m/025wky1': 75, '/m/07qlf79': 76, '/m/07rgkc5': 77, '/m/07phhsh': 78, '/m/02bk07': 79, '/m/03w41f': 80, '/t/dd00031': 81, '/m/06d_3': 82, '/m/07r660_': 83, '/m/07s04w4': 84, '/m/0642b4': 85, '/m/08j51y': 86, '/m/07r4gkf': 87, '/m/09t49': 88, '/m/07qcx4z': 89, '/m/0fx9l': 90, '/m/078jl': 91, '/m/06_y0by': 92, '/g/122z_qxw': 93, '/m/0k4j': 94, '/t/dd00077': 95, '/m/02_nn': 96, '/m/0f8s22': 97, '/m/03_d0': 98, '/m/01hsr_': 99, '/m/028sqc': 100, '/t/dd00130': 101, '/m/02p0sh1': 102, '/m/0939n_': 103, '/m/07brj': 104, '/m/06xkwv': 105, '/m/07r_25d': 106, '/m/0xzly': 107, '/m/02qmj0d': 108, '/m/01z47d': 109, '/m/042v_gx': 110, '/m/053hz1': 111, '/m/02rtxlg': 112, '/m/02pjr4': 113, '/m/07s72n': 114, '/m/07qz6j3': 115, '/m/01h3n': 116, '/m/02bm9n': 117, '/m/0l15bq': 118, '/m/0l14j_': 119, '/m/07pb8fc': 120, '/m/07r5c2p': 121, '/m/01j4z9': 122, '/m/06wzb': 123, '/m/0g6b5': 124, '/m/05148p4': 125, '/m/06bz3': 126, '/m/01yg9g': 127, '/m/0_1c': 128, '/m/07pws3f': 129, '/m/07sr1lc': 130, '/m/0ggx5q': 131, '/m/074ft': 132, '/m/05r6t': 133, '/m/02mk9': 134, '/m/02p01q': 135, '/t/dd00038': 136, '/m/03k3r': 137, '/m/0l14_3': 138, '/m/03wvsk': 139, '/t/dd00036': 140, '/m/030rvx': 141, '/m/023vsd': 142, '/m/0h9mv': 143, '/m/0gg8l': 144, '/m/012f08': 145, '/m/07rjwbb': 146, '/m/015jpf': 147, '/m/06by7': 148, '/m/0l156b': 149, '/m/0k65p': 150, '/m/01jt3m': 151, '/m/014yck': 152, '/m/01bjv': 153, '/m/02yds9': 154, '/m/0dwt5': 155, '/t/dd00005': 156, '/m/026fgl': 157, '/m/064t9': 158, '/m/07rwj3x': 159, '/m/07pyy8b': 160, '/m/07qwyj0': 161, '/m/0dbvp': 162, '/m/0838f': 163, '/m/03vt0': 164, '/m/06cqb': 165, '/m/02cjck': 166, '/t/dd00092': 167, '/m/07qjznl': 168, '/m/02sgy': 169, '/m/06j64v': 170, '/m/07pp_mv': 171, '/m/0ltv': 172, '/m/09xqv': 173, '/m/018j2': 174, '/m/07st89h': 175, '/t/dd00006': 176, '/m/07sq110': 177, '/m/07k1x': 178, '/m/04wptg': 179, '/m/03v3yw': 180, '/m/07qyrcz': 181, '/m/01b9nn': 182, '/m/07qfr4h': 183, '/m/07qdb04': 184, '/m/016622': 185, '/m/0150b9': 186, '/m/07m2kt': 187, '/m/07rdhzs': 188, '/m/01h8n0': 189, '/m/04qvtq': 190, '/m/0l14jd': 191, '/m/0284vy3': 192, '/m/023pjk': 193, '/m/07rcgpl': 194, '/m/0b9m1': 195, '/m/07rwm0c': 196, '/m/06_fw': 197, '/m/07q5rw0': 198, '/m/07qsvvw': 199, '/m/04brg2': 200, '/m/07s2xch': 201, '/m/07pdjhy': 202, '/m/01yrx': 203, '/m/0l14l2': 204, '/m/07qmpdm': 205, '/m/0mkg': 206, '/m/027m70_': 207, '/t/dd00135': 208, '/m/01d3sd': 209, '/m/01y3hg': 210, '/t/dd00002': 211, '/m/07p7b8y': 212, '/m/056ks2': 213, '/m/0790c': 214, '/t/dd00134': 215, '/m/01x3z': 216, '/m/0j6m2': 217, '/m/09f96': 218, '/m/03q5_w': 219, '/m/01b82r': 220, '/m/07rknqz': 221, '/m/07r67yg': 222, '/m/04zmvq': 223, '/m/0bm02': 224, '/m/06rvn': 225, '/m/02dgv': 226, '/m/07qlwh6': 227, '/m/02_41': 228, '/m/05r5wn': 229, '/m/0dwsp': 230, '/m/07qb_dv': 231, '/m/0242l': 232, '/m/07qs1cx': 233, '/m/07r4k75': 234, '/m/03qtwd': 235, '/m/02v2lh': 236, '/m/01xq0k1': 237, '/t/dd00001': 238, '/m/02z32qm': 239, '/m/0261r1': 240, '/m/0j45pbj': 241, '/m/01s0vc': 242, '/m/07r5v4s': 243, '/m/03m5k': 244, '/m/03r5q_': 245, '/m/07qw_06': 246, '/m/07p_0gm': 247, '/m/03wwcy': 248, '/m/0d31p': 249, '/m/07pc8lb': 250, '/m/07qrkrw': 251, '/m/0chx_': 252, '/m/07svc2k': 253, '/m/032s66': 254, '/m/01m4t': 255, '/m/03qjg': 256, '/m/01z5f': 257, '/m/07q7njn': 258, '/m/012xff': 259, '/m/07pbtc8': 260, '/m/04229': 261, '/m/04fgwm': 262, '/m/07rqsjt': 263, '/m/06q74': 264, '/m/03qtq': 265, '/m/0l14md': 266, '/m/03mb9': 267, '/t/dd00048': 268, '/m/0cdnk': 269, '/m/02fs_r': 270, '/m/026z9': 271, '/t/dd00034': 272, '/m/015y_n': 273, '/m/0hdsk': 274, '/m/0mbct': 275, '/t/dd00127': 276, '/m/0_ksk': 277, '/m/0145m': 278, '/m/0283d': 279, '/m/03t3fj': 280, '/m/07pl1bw': 281, '/m/015vgc': 282, '/m/0z9c': 283, '/m/07qh7jl': 284, '/m/05lls': 285, '/m/0326g': 286, '/m/07q4ntr': 287, '/m/07qv4k0': 288, '/m/01jwx6': 289, '/m/02y_763': 290, '/m/02mscn': 291, '/m/01b_21': 292, '/m/07ptzwd': 293, '/m/07n_g': 294, '/m/0k5j': 295, '/m/02mfyn': 296, '/m/0llzx': 297, '/m/07hvw1': 298, '/m/07p6fty': 299, '/m/07q2z82': 300, '/m/07q0yl5': 301, '/m/04rzd': 302, '/m/0316dw': 303, '/m/09x0r': 304, '/m/0cfdd': 305, '/m/0btp2': 306, '/m/0199g': 307, '/m/06mb1': 308, '/m/06hck5': 309, '/m/07rjzl8': 310, '/m/07c52': 311, '/t/dd00121': 312, '/m/07s0dtb': 313, '/m/0l14t7': 314, '/m/07ppn3j': 315, '/m/012ndj': 316, '/m/0dq0md': 317, '/m/011k_j': 318, '/m/07jdr': 319, '/m/07qv_x_': 320, '/m/0195fx': 321, '/m/02k_mr': 322, '/m/07rkbfh': 323, '/m/0gvgw0': 324, '/m/07pn_8q': 325, '/m/01jnbd': 326, '/m/04rlf': 327, '/m/07r10fb': 328, '/m/07bjf': 329, '/m/03l9g': 330, '/m/02g901': 331, '/m/07ryjzk': 332, '/m/04zjc': 333, '/m/03cl9h': 334, '/m/0h2mp': 335, '/m/0l14gg': 336, '/m/07rrh0c': 337, '/m/03lty': 338, '/m/06bxc': 339, '/m/07qf0zm': 340, '/m/012n7d': 341, '/m/0dl5d': 342, '/m/05zc1': 343, '/m/09ct_': 344, '/t/dd00033': 345, '/m/0140xf': 346, '/m/02cz_7': 347, '/m/05fw6t': 348, '/m/06w87': 349, '/m/0dl9sf8': 350, '/m/07rv9rh': 351, '/m/01j3sz': 352, '/m/07pqc89': 353, '/m/032n05': 354, '/m/016cjb': 355, '/m/0319l': 356, '/m/07kc_': 357, '/m/0ghcn6': 358, '/m/07qwf61': 359, '/m/07r_80w': 360, '/m/01c194': 361, '/m/01xqw': 362, '/m/01bns_': 363, '/m/02l6bg': 364, '/m/05tny_': 365, '/m/01lsmm': 366, '/m/0912c9': 367, '/m/07pkxdp': 368, '/m/039jq': 369, '/m/025td0t': 370, '/m/07mzm6': 371, '/m/0dxrf': 372, '/m/0c1dj': 373, '/m/0cj0r': 374, '/m/07gql': 375, '/t/dd00129': 376, '/m/07qn4z3': 377, '/m/0gy1t2s': 378, '/m/0ln16': 379, '/m/07y_7': 380, '/m/07c6l': 381, '/m/0brhx': 382, '/m/028v0c': 383, '/m/02hnl': 384, '/m/0c2wf': 385, '/t/dd00037': 386, '/m/07sx8x_': 387, '/m/07xzm': 388, '/m/0dv5r': 389, '/m/0jbk': 390, '/m/028ght': 391, '/m/081rb': 392, '/m/026t6': 393, '/m/0gywn': 394, '/m/07r_k2n': 395, '/m/07pjwq1': 396, '/m/020bb7': 397, '/m/07pxg6y': 398, '/m/09d5_': 399, '/m/07qv_d5': 400, '/m/01z7dr': 401, '/m/014zdl': 402, '/m/0fx80y': 403, '/m/07qnq_y': 404, '/m/07plz5l': 405, '/m/07q0h5t': 406, '/m/01hnzm': 407, '/m/025_jnm': 408, '/m/07rgt08': 409, '/t/dd00013': 410, '/m/03fwl': 411, '/m/068hy': 412, '/m/0j2kx': 413, '/m/07p9k1k': 414, '/m/07gxw': 415, '/m/03m9d0z': 416, '/t/dd00128': 417, '/m/0dv3j': 418, '/m/01s0ps': 419, '/m/03j1ly': 420, '/m/024dl': 421, '/m/096m7z': 422, '/m/0dwtp': 423, '/m/01hgjl': 424, '/t/dd00088': 425, '/m/02c8p': 426, '/m/01280g': 427, '/m/07ptfmf': 428, '/m/0y4f8': 429, '/m/02jz0l': 430, '/m/07q6cd_': 431, '/m/07pyf11': 432, '/m/0fd3y': 433, '/m/018w8': 434, '/m/01h82_': 435, '/t/dd00125': 436, '/m/02x984l': 437, '/m/0192l': 438, '/m/09ddx': 439, '/m/0130jx': 440, '/m/06rqw': 441, '/m/0cmf2': 442, '/m/07qcpgn': 443, '/m/02w4v': 444, '/m/07p78v5': 445, '/m/01wy6': 446, '/m/07pqn27': 447, '/m/09ld4': 448, '/t/dd00126': 449, '/m/034srq': 450, '/m/0lyf6': 451, '/m/07pzfmf': 452, '/m/0342h': 453, '/m/02bxd': 454, '/m/07s0s5r': 455, '/m/05pd6': 456, '/m/0dls3': 457, '/m/07swgks': 458, '/m/01p970': 459, '/m/02fxyj': 460, '/m/05_wcq': 461, '/t/dd00112': 462, '/m/09b5t': 463, '/m/0155w': 464, '/m/04k94': 465, '/m/03p19w': 466, '/m/04gxbd': 467, '/m/07r04': 468, '/m/02rr_': 469, '/m/03xq_f': 470, '/m/0ngt1': 471, '/m/0463cq4': 472, '/m/0g293': 473, '/m/07qqyl4': 474, '/m/01sm1g': 475, '/m/01d380': 476, '/m/018vs': 477, '/m/03qc9zr': 478, '/m/013y1f': 479, '/m/0jtg0': 480, '/m/068zj': 481, '/m/073cg4': 482, '/m/05w3f': 483, '/m/01m2v': 484, '/m/07yv9': 485, '/m/07rpkh9': 486, '/m/05x_td': 487, '/m/04_sv': 488, '/m/04rmv': 489, '/m/03q5t': 490, '/m/0l14qv': 491, '/m/07qc9xj': 492, '/m/07r4wb8': 493, '/m/0239kh': 494, '/t/dd00066': 495, '/m/085jw': 496, '/m/0dl83': 497, '/m/0bt9lr': 498, '/m/07qwdck': 499, '/m/07pdhp0': 500, '/m/01kcd': 501, '/m/0glt670': 502, '/m/02qldy': 503, '/m/01qbl': 504, '/m/0m0jc': 505, '/m/07s8j8t': 506, '/m/01jg1z': 507, '/m/025rv6n': 508, '/m/07sbbz2': 509, '/m/06h7j': 510, '/m/07qjznt': 511, '/m/07pjjrj': 512, '/m/07rbp7_': 513, '/m/07pt_g0': 514, '/t/dd00065': 515, '/m/07pp8cl': 516, '/m/07phxs1': 517, '/t/dd00018': 518, '/m/083vt': 519, '/m/0d8_n': 520}\n"
     ]
    }
   ],
   "source": [
    "print(mid_to_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()  # Clear memory cache after each training step or epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0 bytes\n",
      "Memory cached: 0 bytes\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory allocated: {torch.cuda.memory_allocated()} bytes\")\n",
    "print(f\"Memory cached: {torch.cuda.memory_reserved()} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()  \u001b[38;5;66;03m# Clears GPU cache\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()  # Clears GPU cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved model to GPU if available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n",
      "Successfully loaded audio: audiosets/ontology/Electric guitar_4.wav, shape: (2106143,), dtype: float32\n",
      "Processed audio shape: torch.Size([2106143])\n",
      "Successfully loaded audio at index 1324, shape: torch.Size([160000]), label: 47\n",
      "Successfully loaded audio: audiosets/ontology/Howl (wind)_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 530, shape: torch.Size([160000]), label: 87\n",
      "Successfully loaded audio: audiosets/ontology/Waves, surf_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 2218, shape: torch.Size([160000]), label: 105\n",
      "Successfully loaded audio: audiosets/ontology/Dog_5.wav, shape: (1473074,), dtype: float32\n",
      "Processed audio shape: torch.Size([1473074])\n",
      "Successfully loaded audio at index 1756, shape: torch.Size([160000]), label: 19\n",
      "Successfully loaded audio: audiosets/ontology/Howl_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1339, shape: torch.Size([160000]), label: 431\n",
      "Successfully loaded audio: audiosets/ontology/Organ_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2001, shape: torch.Size([160000]), label: 145\n",
      "Successfully loaded audio: audiosets/ontology/Baby cry, infant cry_8.wav, shape: (148237,), dtype: float32\n",
      "Processed audio shape: torch.Size([148237])\n",
      "Successfully loaded audio at index 928, shape: torch.Size([160000]), label: 144\n",
      "Successfully loaded audio: audiosets/ontology/Screaming_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 170, shape: torch.Size([160000]), label: 295\n",
      "Successfully loaded audio: audiosets/ontology/Motorboat, speedboat_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 256, shape: torch.Size([160000]), label: 313\n",
      "Successfully loaded audio: audiosets/ontology/Electric shaver, electric razor_6.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 156, shape: torch.Size([160000]), label: 149\n",
      "Successfully loaded audio: audiosets/ontology/Speech synthesizer_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2133, shape: torch.Size([160000]), label: 451\n",
      "Successfully loaded audio: audiosets/ontology/Drip_1.wav, shape: (1693014,), dtype: float32\n",
      "Processed audio shape: torch.Size([1693014])\n",
      "Successfully loaded audio at index 794, shape: torch.Size([160000]), label: 425\n",
      "Successfully loaded audio: audiosets/ontology/Electric guitar_2.wav, shape: (8762840,), dtype: float32\n",
      "Processed audio shape: torch.Size([8762840])\n",
      "Successfully loaded audio at index 593, shape: torch.Size([160000]), label: 47\n",
      "Successfully loaded audio: audiosets/ontology/Gunshot, gunfire_0.wav, shape: (2049858,), dtype: float32\n",
      "Processed audio shape: torch.Size([2049858])\n",
      "Successfully loaded audio at index 730, shape: torch.Size([160000]), label: 318\n",
      "Successfully loaded audio: audiosets/ontology/Owl_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2037, shape: torch.Size([160000]), label: 431\n",
      "Successfully loaded audio: audiosets/ontology/Acoustic guitar_0.wav, shape: (3946836,), dtype: float32\n",
      "Processed audio shape: torch.Size([3946836])\n",
      "Successfully loaded audio at index 1966, shape: torch.Size([160000]), label: 472\n",
      "Successfully loaded audio: audiosets/ontology/Dishes, pots, and pans_4.wav, shape: (16732857,), dtype: float32\n",
      "Processed audio shape: torch.Size([16732857])\n",
      "Successfully loaded audio at index 19, shape: torch.Size([160000]), label: 424\n",
      "Successfully loaded audio: audiosets/ontology/Rapping_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2022, shape: torch.Size([160000]), label: 173\n",
      "Successfully loaded audio: audiosets/ontology/Police car (siren)_1.wav, shape: (1026390,), dtype: float32\n",
      "Processed audio shape: torch.Size([1026390])\n",
      "Successfully loaded audio at index 450, shape: torch.Size([160000]), label: 330\n",
      "Successfully loaded audio: audiosets/ontology/Pizzicato_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1705, shape: torch.Size([160000]), label: 306\n",
      "Successfully loaded audio: audiosets/ontology/Keys jangling_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1633, shape: torch.Size([160000]), label: 510\n",
      "Successfully loaded audio: audiosets/ontology/Gurgling_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 369, shape: torch.Size([160000]), label: 63\n",
      "Successfully loaded audio: audiosets/ontology/Singing bowl_3.wav, shape: (4215072,), dtype: float32\n",
      "Processed audio shape: torch.Size([4215072])\n",
      "Successfully loaded audio at index 1809, shape: torch.Size([160000]), label: 205\n",
      "Successfully loaded audio: audiosets/ontology/Knock_0.wav, shape: (666506,), dtype: float32\n",
      "Processed audio shape: torch.Size([666506])\n",
      "Successfully loaded audio at index 1707, shape: torch.Size([160000]), label: 226\n",
      "Successfully loaded audio: audiosets/ontology/Skidding_3.wav, shape: (9735849,), dtype: float32\n",
      "Processed audio shape: torch.Size([9735849])\n",
      "Successfully loaded audio at index 1032, shape: torch.Size([160000]), label: 243\n",
      "Successfully loaded audio: audiosets/ontology/Cattle, bovinae_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 345, shape: torch.Size([160000]), label: 130\n",
      "Successfully loaded audio: audiosets/ontology/Brass instrument_8.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2125, shape: torch.Size([160000]), label: 38\n",
      "Successfully loaded audio: audiosets/ontology/Alarm clock_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2374, shape: torch.Size([160000]), label: 368\n",
      "Successfully loaded audio: audiosets/ontology/Air conditioning_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2271, shape: torch.Size([160000]), label: 134\n",
      "Successfully loaded audio: audiosets/ontology/Humming_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1873, shape: torch.Size([160000]), label: 475\n",
      "Successfully loaded audio: audiosets/ontology/Cheering_7.wav, shape: (9303958,), dtype: float32\n",
      "Processed audio shape: torch.Size([9303958])\n",
      "Successfully loaded audio at index 1074, shape: torch.Size([160000]), label: 298\n",
      "Successfully loaded audio: audiosets/ontology/Moo_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2386, shape: torch.Size([160000]), label: 183\n",
      "Successfully loaded audio: audiosets/ontology/Eruption_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2098, shape: torch.Size([160000]), label: 373\n",
      "Successfully loaded audio: audiosets/ontology/Finger snapping_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 535, shape: torch.Size([160000]), label: 173\n",
      "Successfully loaded audio: audiosets/ontology/Subway, metro, underground_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1442, shape: torch.Size([160000]), label: 375\n",
      "Successfully loaded audio: audiosets/ontology/Chirp, tweet_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1688, shape: torch.Size([160000]), label: 68\n",
      "Successfully loaded audio: audiosets/ontology/Acoustic guitar_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1285, shape: torch.Size([160000]), label: 472\n",
      "Successfully loaded audio: audiosets/ontology/Laughter_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1519, shape: torch.Size([160000]), label: 263\n",
      "Successfully loaded audio: audiosets/ontology/Bark_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2190, shape: torch.Size([160000]), label: 391\n",
      "Successfully loaded audio: audiosets/ontology/Pant_7.wav, shape: (9656344,), dtype: float32\n",
      "Processed audio shape: torch.Size([9656344])\n",
      "Successfully loaded audio at index 525, shape: torch.Size([160000]), label: 26\n",
      "Successfully loaded audio: audiosets/ontology/Car alarm_4.wav, shape: (1509669,), dtype: float32\n",
      "Processed audio shape: torch.Size([1509669])\n",
      "Successfully loaded audio at index 1137, shape: torch.Size([160000]), label: 481\n",
      "Successfully loaded audio: audiosets/ontology/Electronic organ_0.wav, shape: (1603663,), dtype: float32\n",
      "Processed audio shape: torch.Size([1603663])\n",
      "Successfully loaded audio at index 227, shape: torch.Size([160000]), label: 145\n",
      "Successfully loaded audio: audiosets/ontology/Air brake_2.wav, shape: (1097840,), dtype: float32\n",
      "Processed audio shape: torch.Size([1097840])\n",
      "Successfully loaded audio at index 1686, shape: torch.Size([160000]), label: 106\n",
      "Successfully loaded audio: audiosets/ontology/Cough_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1338, shape: torch.Size([160000]), label: 461\n",
      "Successfully loaded audio: audiosets/ontology/Crack_0.wav, shape: (2562555,), dtype: float32\n",
      "Processed audio shape: torch.Size([2562555])\n",
      "Successfully loaded audio at index 50, shape: torch.Size([160000]), label: 404\n",
      "Successfully loaded audio: audiosets/ontology/Writing_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1088, shape: torch.Size([160000]), label: 381\n",
      "Successfully loaded audio: audiosets/ontology/Church bell_7.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1006, shape: torch.Size([160000]), label: 107\n",
      "Successfully loaded audio: audiosets/ontology/Crackle_0.wav, shape: (3105530,), dtype: float32\n",
      "Processed audio shape: torch.Size([3105530])\n",
      "Successfully loaded audio at index 2419, shape: torch.Size([160000]), label: 404\n",
      "Successfully loaded audio: audiosets/ontology/Light engine (high frequency)_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2164, shape: torch.Size([160000]), label: 254\n",
      "Successfully loaded audio: audiosets/ontology/Wind instrument, woodwind instrument_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2103, shape: torch.Size([160000]), label: 38\n",
      "Successfully loaded audio: audiosets/ontology/Fire alarm_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1570, shape: torch.Size([160000]), label: 326\n",
      "Successfully loaded audio: audiosets/ontology/Busy signal_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1437, shape: torch.Size([160000]), label: 172\n",
      "Successfully loaded audio: audiosets/ontology/Hi-hat_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 2252, shape: torch.Size([160000]), label: 316\n",
      "Successfully loaded audio: audiosets/ontology/Chewing, mastication_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 869, shape: torch.Size([160000]), label: 306\n",
      "Successfully loaded audio: audiosets/ontology/Bleat_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2185, shape: torch.Size([160000]), label: 193\n",
      "Successfully loaded audio: audiosets/ontology/Fly, housefly_1.wav, shape: (4418665,), dtype: float32\n",
      "Processed audio shape: torch.Size([4418665])\n",
      "Successfully loaded audio at index 786, shape: torch.Size([160000]), label: 55\n",
      "Successfully loaded audio: audiosets/ontology/Car passing by_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1837, shape: torch.Size([160000]), label: 202\n",
      "Successfully loaded audio: audiosets/ontology/Ice cream truck, ice cream van_4.wav, shape: (3212156,), dtype: float32\n",
      "Processed audio shape: torch.Size([3212156])\n",
      "Successfully loaded audio at index 355, shape: torch.Size([160000]), label: 421\n",
      "Successfully loaded audio: audiosets/ontology/Ice cream truck, ice cream van_3.wav, shape: (156039,), dtype: float32\n",
      "Processed audio shape: torch.Size([156039])\n",
      "Successfully loaded audio at index 2048, shape: torch.Size([160000]), label: 421\n",
      "Successfully loaded audio: audiosets/ontology/Oink_6.wav, shape: (147494,), dtype: float32\n",
      "Processed audio shape: torch.Size([147494])\n",
      "Successfully loaded audio at index 398, shape: torch.Size([160000]), label: 20\n",
      "Successfully loaded audio: audiosets/ontology/Bell_5.wav, shape: (2208311,), dtype: float32\n",
      "Processed audio shape: torch.Size([2208311])\n",
      "Successfully loaded audio at index 2217, shape: torch.Size([160000]), label: 107\n",
      "Successfully loaded audio: audiosets/ontology/Mosquito_4.wav, shape: (143778,), dtype: float32\n",
      "Processed audio shape: torch.Size([143778])\n",
      "Successfully loaded audio at index 1892, shape: torch.Size([160000]), label: 286\n",
      "Successfully loaded audio: audiosets/ontology/Telephone bell ringing_6.wav, shape: (158639,), dtype: float32\n",
      "Processed audio shape: torch.Size([158639])\n",
      "Successfully loaded audio at index 839, shape: torch.Size([160000]), label: 107\n",
      "Successfully loaded audio: audiosets/ontology/Goose_5.wav, shape: (6773354,), dtype: float32\n",
      "Processed audio shape: torch.Size([6773354])\n",
      "Successfully loaded audio at index 1996, shape: torch.Size([160000]), label: 359\n",
      "Successfully loaded audio: audiosets/ontology/Squawk_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1374, shape: torch.Size([160000]), label: 415\n",
      "Successfully loaded audio: audiosets/ontology/Buzzer_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 698, shape: torch.Size([160000]), label: 325\n",
      "Successfully loaded audio: audiosets/ontology/Typing_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 293, shape: torch.Size([160000]), label: 9\n",
      "Successfully loaded audio: audiosets/ontology/Mechanical fan_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1124, shape: torch.Size([160000]), label: 167\n",
      "Successfully loaded audio: audiosets/ontology/Hubbub, speech noise, speech babble_3.wav, shape: (3888135,), dtype: float32\n",
      "Processed audio shape: torch.Size([3888135])\n",
      "Successfully loaded audio at index 1375, shape: torch.Size([160000]), label: 430\n",
      "Successfully loaded audio: audiosets/ontology/Crowing, cock-a-doodle-doo_6.wav, shape: (145636,), dtype: float32\n",
      "Processed audio shape: torch.Size([145636])\n",
      "Successfully loaded audio at index 2346, shape: torch.Size([160000]), label: 366\n",
      "Successfully loaded audio: audiosets/ontology/French horn_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2079, shape: torch.Size([160000]), label: 511\n",
      "Successfully loaded audio: audiosets/ontology/Baby cry, infant cry_6.wav, shape: (261179,), dtype: float32\n",
      "Processed audio shape: torch.Size([261179])\n",
      "Successfully loaded audio at index 1212, shape: torch.Size([160000]), label: 144\n",
      "Successfully loaded audio: audiosets/ontology/Motorboat, speedboat_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1488, shape: torch.Size([160000]), label: 313\n",
      "Successfully loaded audio: audiosets/ontology/Gong_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1249, shape: torch.Size([160000]), label: 203\n",
      "Successfully loaded audio: audiosets/ontology/Snare drum_1.wav, shape: (8128099,), dtype: float32\n",
      "Processed audio shape: torch.Size([8128099])\n",
      "Successfully loaded audio at index 554, shape: torch.Size([160000]), label: 336\n",
      "Successfully loaded audio: audiosets/ontology/Fowl_12.wav, shape: (2871659,), dtype: float32\n",
      "Processed audio shape: torch.Size([2871659])\n",
      "Successfully loaded audio at index 361, shape: torch.Size([160000]), label: 45\n",
      "Successfully loaded audio: audiosets/ontology/Crying, sobbing_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 945, shape: torch.Size([160000]), label: 79\n",
      "Successfully loaded audio: audiosets/ontology/Boom_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 73, shape: torch.Size([160000]), label: 188\n",
      "Successfully loaded audio: audiosets/ontology/Thunderstorm_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1465, shape: torch.Size([160000]), label: 519\n",
      "Successfully loaded audio: audiosets/ontology/Wind noise (microphone)_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 818, shape: torch.Size([160000]), label: 42\n",
      "Successfully loaded audio: audiosets/ontology/Helicopter_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2431, shape: torch.Size([160000]), label: 302\n",
      "Successfully loaded audio: audiosets/ontology/Female singing_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 215, shape: torch.Size([160000]), label: 423\n",
      "Successfully loaded audio: audiosets/ontology/Hiss_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2397, shape: torch.Size([160000]), label: 478\n",
      "Successfully loaded audio: audiosets/ontology/Whoop_1.wav, shape: (4567458,), dtype: float32\n",
      "Processed audio shape: torch.Size([4567458])\n",
      "Successfully loaded audio at index 1939, shape: torch.Size([160000]), label: 11\n",
      "Successfully loaded audio: audiosets/ontology/Roaring cats (lions, tigers)_4.wav, shape: (2599149,), dtype: float32\n",
      "Processed audio shape: torch.Size([2599149])\n",
      "Successfully loaded audio at index 1082, shape: torch.Size([160000]), label: 48\n",
      "Successfully loaded audio: audiosets/ontology/Clapping_0.wav, shape: (1497223,), dtype: float32\n",
      "Processed audio shape: torch.Size([1497223])\n",
      "Successfully loaded audio at index 133, shape: torch.Size([160000]), label: 3\n",
      "Successfully loaded audio: audiosets/ontology/Singing_7.wav, shape: (3518845,), dtype: float32\n",
      "Processed audio shape: torch.Size([3518845])\n",
      "Successfully loaded audio at index 966, shape: torch.Size([160000]), label: 423\n",
      "Successfully loaded audio: audiosets/ontology/Animal_6.wav, shape: (1427935,), dtype: float32\n",
      "Processed audio shape: torch.Size([1427935])\n",
      "Successfully loaded audio at index 352, shape: torch.Size([160000]), label: 124\n",
      "Successfully loaded audio: audiosets/ontology/Fireworks_5.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 793, shape: torch.Size([160000]), label: 204\n",
      "Successfully loaded audio: audiosets/ontology/Livestock, farm animals, working animals_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 2080, shape: torch.Size([160000]), label: 74\n",
      "Successfully loaded audio: audiosets/ontology/Medium engine (mid frequency)_3.wav, shape: (418183,), dtype: float32\n",
      "Processed audio shape: torch.Size([418183])\n",
      "Successfully loaded audio at index 468, shape: torch.Size([160000]), label: 455\n",
      "Successfully loaded audio: audiosets/ontology/Bass guitar_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2007, shape: torch.Size([160000]), label: 448\n",
      "Successfully loaded audio: audiosets/ontology/Tambourine_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 937, shape: torch.Size([160000]), label: 147\n",
      "Successfully loaded audio: audiosets/ontology/Bleat_6.wav, shape: (147494,), dtype: float32\n",
      "Processed audio shape: torch.Size([147494])\n",
      "Successfully loaded audio at index 1582, shape: torch.Size([160000]), label: 193\n",
      "Successfully loaded audio: audiosets/ontology/Snicker_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1373, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Bass guitar_5.wav, shape: (2306392,), dtype: float32\n",
      "Processed audio shape: torch.Size([2306392])\n",
      "Successfully loaded audio at index 519, shape: torch.Size([160000]), label: 448\n",
      "Successfully loaded audio: audiosets/ontology/Raindrop_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2076, shape: torch.Size([160000]), label: 31\n",
      "Successfully loaded audio: audiosets/ontology/Insect_1.wav, shape: (2697045,), dtype: float32\n",
      "Processed audio shape: torch.Size([2697045])\n",
      "Successfully loaded audio at index 1914, shape: torch.Size([160000]), label: 374\n",
      "Successfully loaded audio: audiosets/ontology/Narration, monologue_4.wav, shape: (2754444,), dtype: float32\n",
      "Processed audio shape: torch.Size([2754444])\n",
      "Successfully loaded audio at index 580, shape: torch.Size([160000]), label: 24\n",
      "Successfully loaded audio: audiosets/ontology/Mandolin_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 727, shape: torch.Size([160000]), label: 367\n",
      "Successfully loaded audio: audiosets/ontology/Harmonica_6.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 87, shape: torch.Size([160000]), label: 370\n",
      "Successfully loaded audio: audiosets/ontology/Guitar_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2092, shape: torch.Size([160000]), label: 401\n",
      "Successfully loaded audio: audiosets/ontology/Keyboard (musical)_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1114, shape: torch.Size([160000]), label: 354\n",
      "Successfully loaded audio: audiosets/ontology/Typing_1.wav, shape: (12556971,), dtype: float32\n",
      "Processed audio shape: torch.Size([12556971])\n",
      "Successfully loaded audio at index 217, shape: torch.Size([160000]), label: 9\n",
      "Successfully loaded audio: audiosets/ontology/Fowl_8.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1192, shape: torch.Size([160000]), label: 431\n",
      "Successfully loaded audio: audiosets/ontology/Stream_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 251, shape: torch.Size([160000]), label: 485\n",
      "Successfully loaded audio: audiosets/ontology/Clock_4.wav, shape: (2478220,), dtype: float32\n",
      "Processed audio shape: torch.Size([2478220])\n",
      "Successfully loaded audio at index 1510, shape: torch.Size([160000]), label: 276\n",
      "Successfully loaded audio: audiosets/ontology/Siren_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1903, shape: torch.Size([160000]), label: 128\n",
      "Successfully loaded audio: audiosets/ontology/Cowbell_3.wav, shape: (153438,), dtype: float32\n",
      "Processed audio shape: torch.Size([153438])\n",
      "Successfully loaded audio at index 2094, shape: torch.Size([160000]), label: 146\n",
      "Successfully loaded audio: audiosets/ontology/Microwave oven_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1866, shape: torch.Size([160000]), label: 283\n",
      "Successfully loaded audio: audiosets/ontology/Mouse_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1131, shape: torch.Size([160000]), label: 133\n",
      "Successfully loaded audio: audiosets/ontology/Snort_0.wav, shape: (1857411,), dtype: float32\n",
      "Processed audio shape: torch.Size([1857411])\n",
      "Successfully loaded audio at index 649, shape: torch.Size([160000]), label: 12\n",
      "Successfully loaded audio: audiosets/ontology/Alarm clock_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1119, shape: torch.Size([160000]), label: 384\n",
      "Successfully loaded audio: audiosets/ontology/Doorbell_1.wav, shape: (182602,), dtype: float32\n",
      "Processed audio shape: torch.Size([182602])\n",
      "Successfully loaded audio at index 1752, shape: torch.Size([160000]), label: 107\n",
      "Successfully loaded audio: audiosets/ontology/Idling_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1535, shape: torch.Size([160000]), label: 517\n",
      "Successfully loaded audio: audiosets/ontology/Electric piano_4.wav, shape: (3414699,), dtype: float32\n",
      "Processed audio shape: torch.Size([3414699])\n",
      "Successfully loaded audio at index 1924, shape: torch.Size([160000]), label: 407\n",
      "Successfully loaded audio: audiosets/ontology/Roaring cats (lions, tigers)_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 150, shape: torch.Size([160000]), label: 306\n",
      "Successfully loaded audio: audiosets/ontology/Medium engine (mid frequency)_1.wav, shape: (1067933,), dtype: float32\n",
      "Processed audio shape: torch.Size([1067933])\n",
      "Successfully loaded audio at index 151, shape: torch.Size([160000]), label: 240\n",
      "Successfully loaded audio: audiosets/ontology/Waterfall_3.wav, shape: (28881238,), dtype: float32\n",
      "Processed audio shape: torch.Size([28881238])\n",
      "Successfully loaded audio at index 1354, shape: torch.Size([160000]), label: 357\n",
      "Successfully loaded audio: audiosets/ontology/Child singing_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1033, shape: torch.Size([160000]), label: 244\n",
      "Successfully loaded audio: audiosets/ontology/Church bell_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1694, shape: torch.Size([160000]), label: 161\n",
      "Successfully loaded audio: audiosets/ontology/Wind chime_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1103, shape: torch.Size([160000]), label: 42\n",
      "Successfully loaded audio: audiosets/ontology/Doorbell_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 616, shape: torch.Size([160000]), label: 107\n",
      "Successfully loaded audio: audiosets/ontology/Duck_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1764, shape: torch.Size([160000]), label: 44\n",
      "Successfully loaded audio: audiosets/ontology/Glockenspiel_4.wav, shape: (158639,), dtype: float32\n",
      "Processed audio shape: torch.Size([158639])\n",
      "Successfully loaded audio at index 1529, shape: torch.Size([160000]), label: 427\n",
      "Successfully loaded audio: audiosets/ontology/Squish_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 117, shape: torch.Size([160000]), label: 177\n",
      "Successfully loaded audio: audiosets/ontology/Wind chime_2.wav, shape: (532573,), dtype: float32\n",
      "Processed audio shape: torch.Size([532573])\n",
      "Successfully loaded audio at index 706, shape: torch.Size([160000]), label: 457\n",
      "Successfully loaded audio: audiosets/ontology/Skateboard_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2181, shape: torch.Size([160000]), label: 8\n",
      "Successfully loaded audio: audiosets/ontology/Aircraft engine_5.wav, shape: (23697408,), dtype: float32\n",
      "Processed audio shape: torch.Size([23697408])\n",
      "Successfully loaded audio at index 1761, shape: torch.Size([160000]), label: 471\n",
      "Successfully loaded audio: audiosets/ontology/Female speech, woman speaking_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1659, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Fireworks_5.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 64, shape: torch.Size([160000]), label: 198\n",
      "Successfully loaded audio: audiosets/ontology/Train horn_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1188, shape: torch.Size([160000]), label: 436\n",
      "Successfully loaded audio: audiosets/ontology/Sawing_0.wav, shape: (504152,), dtype: float32\n",
      "Processed audio shape: torch.Size([504152])\n",
      "Successfully loaded audio at index 9, shape: torch.Size([160000]), label: 255\n",
      "Successfully loaded audio: audiosets/ontology/Wind noise (microphone)_3.wav, shape: (13654358,), dtype: float32\n",
      "Processed audio shape: torch.Size([13654358])\n",
      "Successfully loaded audio at index 321, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Guitar_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1075, shape: torch.Size([160000]), label: 401\n",
      "Successfully loaded audio: audiosets/ontology/Telephone bell ringing_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 792, shape: torch.Size([160000]), label: 148\n",
      "Successfully loaded audio: audiosets/ontology/Pour_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2078, shape: torch.Size([160000]), label: 166\n",
      "Successfully loaded audio: audiosets/ontology/Electric guitar_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 509, shape: torch.Size([160000]), label: 401\n",
      "Successfully loaded audio: audiosets/ontology/Fire alarm_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2226, shape: torch.Size([160000]), label: 204\n",
      "Successfully loaded audio: audiosets/ontology/Train horn_2.wav, shape: (13297976,), dtype: float32\n",
      "Processed audio shape: torch.Size([13297976])\n",
      "Successfully loaded audio at index 2123, shape: torch.Size([160000]), label: 436\n",
      "Successfully loaded audio: audiosets/ontology/Applause_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 666, shape: torch.Size([160000]), label: 259\n",
      "Successfully loaded audio: audiosets/ontology/Wild animals_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 128, shape: torch.Size([160000]), label: 124\n",
      "Successfully loaded audio: audiosets/ontology/Alarm clock_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2035, shape: torch.Size([160000]), label: 384\n",
      "Successfully loaded audio: audiosets/ontology/Toilet flush_4.wav, shape: (448982,), dtype: float32\n",
      "Processed audio shape: torch.Size([448982])\n",
      "Successfully loaded audio at index 473, shape: torch.Size([160000]), label: 454\n",
      "Successfully loaded audio: audiosets/ontology/Chipmunk_2.wav, shape: (1562611,), dtype: float32\n",
      "Processed audio shape: torch.Size([1562611])\n",
      "Successfully loaded audio at index 1794, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Gobble_1.wav, shape: (324337,), dtype: float32\n",
      "Processed audio shape: torch.Size([324337])\n",
      "Successfully loaded audio at index 2060, shape: torch.Size([160000]), label: 434\n",
      "Successfully loaded audio: audiosets/ontology/Organ_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2240, shape: torch.Size([160000]), label: 145\n",
      "Successfully loaded audio: audiosets/ontology/Rapping_5.wav, shape: (10187431,), dtype: float32\n",
      "Processed audio shape: torch.Size([10187431])\n",
      "Successfully loaded audio at index 116, shape: torch.Size([160000]), label: 273\n",
      "Successfully loaded audio: audiosets/ontology/Male singing_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2319, shape: torch.Size([160000]), label: 423\n",
      "Successfully loaded audio: audiosets/ontology/Microwave oven_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2186, shape: torch.Size([160000]), label: 283\n",
      "Successfully loaded audio: audiosets/ontology/Steam whistle_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2275, shape: torch.Size([160000]), label: 439\n",
      "Successfully loaded audio: audiosets/ontology/Wind noise (microphone)_2.wav, shape: (1603478,), dtype: float32\n",
      "Processed audio shape: torch.Size([1603478])\n",
      "Successfully loaded audio at index 1325, shape: torch.Size([160000]), label: 49\n",
      "Successfully loaded audio: audiosets/ontology/Mallet percussion_4.wav, shape: (1379080,), dtype: float32\n",
      "Processed audio shape: torch.Size([1379080])\n",
      "Successfully loaded audio at index 1215, shape: torch.Size([160000]), label: 268\n",
      "Successfully loaded audio: audiosets/ontology/Ratchet, pawl_2.wav, shape: (709417,), dtype: float32\n",
      "Processed audio shape: torch.Size([709417])\n",
      "Successfully loaded audio at index 785, shape: torch.Size([160000]), label: 488\n",
      "Successfully loaded audio: audiosets/ontology/Microwave oven_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1353, shape: torch.Size([160000]), label: 366\n",
      "Successfully loaded audio: audiosets/ontology/Aircraft engine_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 135, shape: torch.Size([160000]), label: 471\n",
      "Successfully loaded audio: audiosets/ontology/Screaming_4.wav, shape: (3278287,), dtype: float32\n",
      "Processed audio shape: torch.Size([3278287])\n",
      "Successfully loaded audio at index 1365, shape: torch.Size([160000]), label: 295\n",
      "Successfully loaded audio: audiosets/ontology/Air brake_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 809, shape: torch.Size([160000]), label: 106\n",
      "Successfully loaded audio: audiosets/ontology/Sneeze_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 207, shape: torch.Size([160000]), label: 320\n",
      "Successfully loaded audio: audiosets/ontology/Oink_0.wav, shape: (2366021,), dtype: float32\n",
      "Processed audio shape: torch.Size([2366021])\n",
      "Successfully loaded audio at index 1089, shape: torch.Size([160000]), label: 20\n",
      "Successfully loaded audio: audiosets/ontology/Cello_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 243, shape: torch.Size([160000]), label: 397\n",
      "Successfully loaded audio: audiosets/ontology/Mechanical fan_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 565, shape: torch.Size([160000]), label: 167\n",
      "Successfully loaded audio: audiosets/ontology/Acoustic guitar_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 659, shape: torch.Size([160000]), label: 401\n",
      "Successfully loaded audio: audiosets/ontology/Electric guitar_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 845, shape: torch.Size([160000]), label: 47\n",
      "Successfully loaded audio: audiosets/ontology/Wild animals_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1207, shape: torch.Size([160000]), label: 124\n",
      "Successfully loaded audio: audiosets/ontology/Emergency vehicle_9.wav, shape: (150837,), dtype: float32\n",
      "Processed audio shape: torch.Size([150837])\n",
      "Successfully loaded audio at index 1152, shape: torch.Size([160000]), label: 253\n",
      "Successfully loaded audio: audiosets/ontology/Flute_6.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 158, shape: torch.Size([160000]), label: 266\n",
      "Successfully loaded audio: audiosets/ontology/Tick-tock_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1268, shape: torch.Size([160000]), label: 72\n",
      "Successfully loaded audio: audiosets/ontology/Bowed string instrument_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 573, shape: torch.Size([160000]), label: 38\n",
      "Successfully loaded audio: audiosets/ontology/Female singing_5.wav, shape: (649045,), dtype: float32\n",
      "Processed audio shape: torch.Size([649045])\n",
      "Successfully loaded audio at index 205, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Electric guitar_2.wav, shape: (8762840,), dtype: float32\n",
      "Processed audio shape: torch.Size([8762840])\n",
      "Successfully loaded audio at index 920, shape: torch.Size([160000]), label: 401\n",
      "Successfully loaded audio: audiosets/ontology/Wild animals_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1896, shape: torch.Size([160000]), label: 353\n",
      "Successfully loaded audio: audiosets/ontology/Motorcycle_0.wav, shape: (1764717,), dtype: float32\n",
      "Processed audio shape: torch.Size([1764717])\n",
      "Successfully loaded audio at index 1521, shape: torch.Size([160000]), label: 473\n",
      "Successfully loaded audio: audiosets/ontology/Train horn_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 198, shape: torch.Size([160000]), label: 31\n",
      "Successfully loaded audio: audiosets/ontology/Train horn_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 958, shape: torch.Size([160000]), label: 307\n",
      "Successfully loaded audio: audiosets/ontology/Cash register_5.wav, shape: (156410,), dtype: float32\n",
      "Processed audio shape: torch.Size([156410])\n",
      "Successfully loaded audio at index 1632, shape: torch.Size([160000]), label: 71\n",
      "Successfully loaded audio: audiosets/ontology/Microwave oven_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1112, shape: torch.Size([160000]), label: 283\n",
      "Successfully loaded audio: audiosets/ontology/Crow_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2116, shape: torch.Size([160000]), label: 366\n",
      "Successfully loaded audio: audiosets/ontology/Bird flight, flapping wings_1.wav, shape: (5781955,), dtype: float32\n",
      "Processed audio shape: torch.Size([5781955])\n",
      "Successfully loaded audio at index 1880, shape: torch.Size([160000]), label: 97\n",
      "Successfully loaded audio: audiosets/ontology/Power tool_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1867, shape: torch.Size([160000]), label: 111\n",
      "Successfully loaded audio: audiosets/ontology/Police car (siren)_0.wav, shape: (497651,), dtype: float32\n",
      "Processed audio shape: torch.Size([497651])\n",
      "Successfully loaded audio at index 1110, shape: torch.Size([160000]), label: 330\n",
      "Successfully loaded audio: audiosets/ontology/Slam_0.wav, shape: (1095239,), dtype: float32\n",
      "Processed audio shape: torch.Size([1095239])\n",
      "Successfully loaded audio at index 147, shape: torch.Size([160000]), label: 76\n",
      "Successfully loaded audio: audiosets/ontology/Tools_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 885, shape: torch.Size([160000]), label: 442\n",
      "Successfully loaded audio: audiosets/ontology/Rattle (instrument)_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 141, shape: torch.Size([160000]), label: 56\n",
      "Successfully loaded audio: audiosets/ontology/Croak_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1635, shape: torch.Size([160000]), label: 319\n",
      "Successfully loaded audio: audiosets/ontology/Buzzer_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 388, shape: torch.Size([160000]), label: 325\n",
      "Successfully loaded audio: audiosets/ontology/Tapping (guitar technique)_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 61, shape: torch.Size([160000]), label: 280\n",
      "Successfully loaded audio: audiosets/ontology/Cash register_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 514, shape: torch.Size([160000]), label: 71\n",
      "Successfully loaded audio: audiosets/ontology/Wind noise (microphone)_5.wav, shape: (1042112,), dtype: float32\n",
      "Processed audio shape: torch.Size([1042112])\n",
      "Successfully loaded audio at index 137, shape: torch.Size([160000]), label: 450\n",
      "Successfully loaded audio: audiosets/ontology/Cap gun_1.wav, shape: (144150,), dtype: float32\n",
      "Processed audio shape: torch.Size([144150])\n",
      "Successfully loaded audio at index 1102, shape: torch.Size([160000]), label: 100\n",
      "Successfully loaded audio: audiosets/ontology/Bird flight, flapping wings_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1515, shape: torch.Size([160000]), label: 60\n",
      "Successfully loaded audio: audiosets/ontology/Raindrop_1.wav, shape: (9466683,), dtype: float32\n",
      "Processed audio shape: torch.Size([9466683])\n",
      "Successfully loaded audio at index 2221, shape: torch.Size([160000]), label: 66\n",
      "Successfully loaded audio: audiosets/ontology/Accelerating, revving, vroom_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1331, shape: torch.Size([160000]), label: 403\n",
      "Successfully loaded audio: audiosets/ontology/Fire engine, fire truck (siren)_7.wav, shape: (14234390,), dtype: float32\n",
      "Processed audio shape: torch.Size([14234390])\n",
      "Successfully loaded audio at index 176, shape: torch.Size([160000]), label: 84\n",
      "Successfully loaded audio: audiosets/ontology/Power windows, electric windows_0.wav, shape: (146751,), dtype: float32\n",
      "Processed audio shape: torch.Size([146751])\n",
      "Successfully loaded audio at index 720, shape: torch.Size([160000]), label: 42\n",
      "Successfully loaded audio: audiosets/ontology/Percussion_2.wav, shape: (876043,), dtype: float32\n",
      "Processed audio shape: torch.Size([876043])\n",
      "Successfully loaded audio at index 2340, shape: torch.Size([160000]), label: 292\n",
      "Successfully loaded audio: audiosets/ontology/Electric shaver, electric razor_5.wav, shape: (11238459,), dtype: float32\n",
      "Processed audio shape: torch.Size([11238459])\n",
      "Successfully loaded audio at index 163, shape: torch.Size([160000]), label: 149\n",
      "Successfully loaded audio: audiosets/ontology/Vehicle horn, car horn, honking_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 558, shape: torch.Size([160000]), label: 29\n",
      "Successfully loaded audio: audiosets/ontology/Ice cream truck, ice cream van_2.wav, shape: (145636,), dtype: float32\n",
      "Processed audio shape: torch.Size([145636])\n",
      "Successfully loaded audio at index 430, shape: torch.Size([160000]), label: 396\n",
      "Successfully loaded audio: audiosets/ontology/Drum machine_0.wav, shape: (4543681,), dtype: float32\n",
      "Processed audio shape: torch.Size([4543681])\n",
      "Successfully loaded audio at index 452, shape: torch.Size([160000]), label: 217\n",
      "Successfully loaded audio: audiosets/ontology/Fixed-wing aircraft, airplane_4.wav, shape: (4699162,), dtype: float32\n",
      "Processed audio shape: torch.Size([4699162])\n",
      "Successfully loaded audio at index 1714, shape: torch.Size([160000]), label: 460\n",
      "Successfully loaded audio: audiosets/ontology/Coo_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2317, shape: torch.Size([160000]), label: 155\n",
      "Successfully loaded audio: audiosets/ontology/Squawk_2.wav, shape: (265080,), dtype: float32\n",
      "Processed audio shape: torch.Size([265080])\n",
      "Successfully loaded audio at index 178, shape: torch.Size([160000]), label: 415\n",
      "Successfully loaded audio: audiosets/ontology/Burping, eructation_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1802, shape: torch.Size([160000]), label: 82\n",
      "Successfully loaded audio: audiosets/ontology/Computer keyboard_1.wav, shape: (9352812,), dtype: float32\n",
      "Processed audio shape: torch.Size([9352812])\n",
      "Successfully loaded audio at index 1979, shape: torch.Size([160000]), label: 515\n",
      "Successfully loaded audio: audiosets/ontology/Blender_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 991, shape: torch.Size([160000]), label: 207\n",
      "Successfully loaded audio: audiosets/ontology/Waterfall_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2264, shape: torch.Size([160000]), label: 357\n",
      "Successfully loaded audio: audiosets/ontology/Telephone bell ringing_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1577, shape: torch.Size([160000]), label: 107\n",
      "Successfully loaded audio: audiosets/ontology/Wind noise (microphone)_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1598, shape: torch.Size([160000]), label: 42\n",
      "Successfully loaded audio: audiosets/ontology/Yell_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 206, shape: torch.Size([160000]), label: 2\n",
      "Successfully loaded audio: audiosets/ontology/Hiss_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1564, shape: torch.Size([160000]), label: 478\n",
      "Successfully loaded audio: audiosets/ontology/Musical instrument_4.wav, shape: (3729497,), dtype: float32\n",
      "Processed audio shape: torch.Size([3729497])\n",
      "Successfully loaded audio at index 412, shape: torch.Size([160000]), label: 38\n",
      "Successfully loaded audio: audiosets/ontology/Wind chime_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2033, shape: torch.Size([160000]), label: 42\n",
      "Successfully loaded audio: audiosets/ontology/Fireworks_3.wav, shape: (144893,), dtype: float32\n",
      "Processed audio shape: torch.Size([144893])\n",
      "Successfully loaded audio at index 1634, shape: torch.Size([160000]), label: 204\n",
      "Successfully loaded audio: audiosets/ontology/Fire engine, fire truck (siren)_4.wav, shape: (9383277,), dtype: float32\n",
      "Processed audio shape: torch.Size([9383277])\n",
      "Successfully loaded audio at index 1747, shape: torch.Size([160000]), label: 84\n",
      "Successfully loaded audio: audiosets/ontology/Fireworks_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2013, shape: torch.Size([160000]), label: 198\n",
      "Successfully loaded audio: audiosets/ontology/Howl_5.wav, shape: (1487935,), dtype: float32\n",
      "Processed audio shape: torch.Size([1487935])\n",
      "Successfully loaded audio at index 343, shape: torch.Size([160000]), label: 431\n",
      "Successfully loaded audio: audiosets/ontology/Fire engine, fire truck (siren)_4.wav, shape: (9383277,), dtype: float32\n",
      "Processed audio shape: torch.Size([9383277])\n",
      "Successfully loaded audio at index 391, shape: torch.Size([160000]), label: 204\n",
      "Successfully loaded audio: audiosets/ontology/Slosh_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1278, shape: torch.Size([160000]), label: 116\n",
      "Successfully loaded audio: audiosets/ontology/Roaring cats (lions, tigers)_4.wav, shape: (2599149,), dtype: float32\n",
      "Processed audio shape: torch.Size([2599149])\n",
      "Successfully loaded audio at index 1387, shape: torch.Size([160000]), label: 433\n",
      "Successfully loaded audio: audiosets/ontology/Railroad car, train wagon_9.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1940, shape: torch.Size([160000]), label: 436\n",
      "Successfully loaded audio: audiosets/ontology/Hubbub, speech noise, speech babble_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 313, shape: torch.Size([160000]), label: 174\n",
      "Successfully loaded audio: audiosets/ontology/Keyboard (musical)_8.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1433, shape: torch.Size([160000]), label: 462\n",
      "Successfully loaded audio: audiosets/ontology/Train horn_2.wav, shape: (13297976,), dtype: float32\n",
      "Processed audio shape: torch.Size([13297976])\n",
      "Successfully loaded audio at index 1498, shape: torch.Size([160000]), label: 31\n",
      "Successfully loaded audio: audiosets/ontology/Alarm_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1411, shape: torch.Size([160000]), label: 384\n",
      "Successfully loaded audio: audiosets/ontology/Sniff_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1560, shape: torch.Size([160000]), label: 465\n",
      "Successfully loaded audio: audiosets/ontology/Chirp, tweet_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1724, shape: torch.Size([160000]), label: 68\n",
      "Successfully loaded audio: audiosets/ontology/Burping, eructation_5.wav, shape: (2691658,), dtype: float32\n",
      "Processed audio shape: torch.Size([2691658])\n",
      "Successfully loaded audio at index 1480, shape: torch.Size([160000]), label: 173\n",
      "Successfully loaded audio: audiosets/ontology/Walk, footsteps_7.wav, shape: (145636,), dtype: float32\n",
      "Processed audio shape: torch.Size([145636])\n",
      "Successfully loaded audio at index 1658, shape: torch.Size([160000]), label: 369\n",
      "Successfully loaded audio: audiosets/ontology/Whimper (dog)_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 229, shape: torch.Size([160000]), label: 19\n",
      "Successfully loaded audio: audiosets/ontology/Clapping_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 763, shape: torch.Size([160000]), label: 173\n",
      "Successfully loaded audio: audiosets/ontology/Train whistle_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1363, shape: torch.Size([160000]), label: 31\n",
      "Successfully loaded audio: audiosets/ontology/Coin (dropping)_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 122, shape: torch.Size([160000]), label: 176\n",
      "Successfully loaded audio: audiosets/ontology/Sitar_1.wav, shape: (3577731,), dtype: float32\n",
      "Processed audio shape: torch.Size([3577731])\n",
      "Successfully loaded audio at index 759, shape: torch.Size([160000]), label: 419\n",
      "Successfully loaded audio: audiosets/ontology/Flute_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 671, shape: torch.Size([160000]), label: 266\n",
      "Successfully loaded audio: audiosets/ontology/Frog_0.wav, shape: (147494,), dtype: float32\n",
      "Processed audio shape: torch.Size([147494])\n",
      "Successfully loaded audio at index 654, shape: torch.Size([160000]), label: 428\n",
      "Successfully loaded audio: audiosets/ontology/Railroad car, train wagon_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1289, shape: torch.Size([160000]), label: 436\n",
      "Successfully loaded audio: audiosets/ontology/Light engine (high frequency)_2.wav, shape: (1360133,), dtype: float32\n",
      "Processed audio shape: torch.Size([1360133])\n",
      "Successfully loaded audio at index 1143, shape: torch.Size([160000]), label: 455\n",
      "Successfully loaded audio: audiosets/ontology/Belly laugh_5.wav, shape: (13671083,), dtype: float32\n",
      "Processed audio shape: torch.Size([13671083])\n",
      "Successfully loaded audio at index 1571, shape: torch.Size([160000]), label: 107\n",
      "Successfully loaded audio: audiosets/ontology/Aircraft engine_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 776, shape: torch.Size([160000]), label: 455\n",
      "Successfully loaded audio: audiosets/ontology/Thunderstorm_5.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 2234, shape: torch.Size([160000]), label: 432\n",
      "Successfully loaded audio: audiosets/ontology/Single-lens reflex camera_3.wav, shape: (2829491,), dtype: float32\n",
      "Processed audio shape: torch.Size([2829491])\n",
      "Successfully loaded audio at index 1531, shape: torch.Size([160000]), label: 27\n",
      "Successfully loaded audio: audiosets/ontology/Vehicle horn, car horn, honking_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1026, shape: torch.Size([160000]), label: 233\n",
      "Successfully loaded audio: audiosets/ontology/Train horn_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 833, shape: torch.Size([160000]), label: 31\n",
      "Successfully loaded audio: audiosets/ontology/Vacuum cleaner_2.wav, shape: (19515351,), dtype: float32\n",
      "Processed audio shape: torch.Size([19515351])\n",
      "Successfully loaded audio at index 1177, shape: torch.Size([160000]), label: 494\n",
      "Successfully loaded audio: audiosets/ontology/Race car, auto racing_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 876, shape: torch.Size([160000]), label: 29\n",
      "Successfully loaded audio: audiosets/ontology/Tambourine_2.wav, shape: (6820723,), dtype: float32\n",
      "Processed audio shape: torch.Size([6820723])\n",
      "Successfully loaded audio at index 592, shape: torch.Size([160000]), label: 147\n",
      "Successfully loaded audio: audiosets/ontology/Aircraft engine_6.wav, shape: (160086,), dtype: float32\n",
      "Processed audio shape: torch.Size([160086])\n",
      "Successfully loaded audio at index 984, shape: torch.Size([160000]), label: 471\n",
      "Successfully loaded audio: audiosets/ontology/Subway, metro, underground_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1719, shape: torch.Size([160000]), label: 375\n",
      "Successfully loaded audio: audiosets/ontology/Dental drill, dentist's drill_1.wav, shape: (7564319,), dtype: float32\n",
      "Processed audio shape: torch.Size([7564319])\n",
      "Successfully loaded audio at index 734, shape: torch.Size([160000]), label: 103\n",
      "Successfully loaded audio: audiosets/ontology/Bark_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 537, shape: torch.Size([160000]), label: 391\n",
      "Successfully loaded audio: audiosets/ontology/Liquid_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1323, shape: torch.Size([160000]), label: 328\n",
      "Successfully loaded audio: audiosets/ontology/Domestic animals, pets_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 267, shape: torch.Size([160000]), label: 124\n",
      "Successfully loaded audio: audiosets/ontology/Piano_2.wav, shape: (14393029,), dtype: float32\n",
      "Processed audio shape: torch.Size([14393029])\n",
      "Successfully loaded audio at index 2051, shape: torch.Size([160000]), label: 386\n",
      "Successfully loaded audio: audiosets/ontology/Scratching (performance technique)_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1435, shape: torch.Size([160000]), label: 104\n",
      "Successfully loaded audio: audiosets/ontology/Water_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1754, shape: torch.Size([160000]), label: 357\n",
      "Successfully loaded audio: audiosets/ontology/Steel guitar, slide guitar_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 324, shape: torch.Size([160000]), label: 238\n",
      "Successfully loaded audio: audiosets/ontology/Crowing, cock-a-doodle-doo_7.wav, shape: (360003,), dtype: float32\n",
      "Processed audio shape: torch.Size([360003])\n",
      "Successfully loaded audio at index 764, shape: torch.Size([160000]), label: 366\n",
      "Successfully loaded audio: audiosets/ontology/Female speech, woman speaking_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2026, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Fixed-wing aircraft, airplane_6.wav, shape: (148608,), dtype: float32\n",
      "Processed audio shape: torch.Size([148608])\n",
      "Successfully loaded audio at index 27, shape: torch.Size([160000]), label: 150\n",
      "Successfully loaded audio: audiosets/ontology/Skateboard_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 75, shape: torch.Size([160000]), label: 495\n",
      "Successfully loaded audio: audiosets/ontology/Wind instrument, woodwind instrument_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1327, shape: torch.Size([160000]), label: 38\n",
      "Successfully loaded audio: audiosets/ontology/Drum_2.wav, shape: (3405824,), dtype: float32\n",
      "Processed audio shape: torch.Size([3405824])\n",
      "Successfully loaded audio at index 225, shape: torch.Size([160000]), label: 93\n",
      "Successfully loaded audio: audiosets/ontology/Male speech, man speaking_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1216, shape: torch.Size([160000]), label: 430\n",
      "Successfully loaded audio: audiosets/ontology/Howl (wind)_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1840, shape: torch.Size([160000]), label: 431\n",
      "Successfully loaded audio: audiosets/ontology/Electric piano_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2002, shape: torch.Size([160000]), label: 407\n",
      "Successfully loaded audio: audiosets/ontology/Snap_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2062, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Boom_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1029, shape: torch.Size([160000]), label: 188\n",
      "Successfully loaded audio: audiosets/ontology/Explosion_1.wav, shape: (4816748,), dtype: float32\n",
      "Processed audio shape: torch.Size([4816748])\n",
      "Successfully loaded audio at index 2020, shape: torch.Size([160000]), label: 64\n",
      "Successfully loaded audio: audiosets/ontology/Wind instrument, woodwind instrument_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 762, shape: torch.Size([160000]), label: 192\n",
      "Successfully loaded audio: audiosets/ontology/Railroad car, train wagon_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1868, shape: torch.Size([160000]), label: 436\n",
      "Successfully loaded audio: audiosets/ontology/Artillery fire_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 200, shape: torch.Size([160000]), label: 204\n",
      "Successfully loaded audio: audiosets/ontology/Theremin_1.wav, shape: (4229004,), dtype: float32\n",
      "Processed audio shape: torch.Size([4229004])\n",
      "Successfully loaded audio at index 320, shape: torch.Size([160000]), label: 33\n",
      "Successfully loaded audio: audiosets/ontology/Plucked string instrument_4.wav, shape: (3561384,), dtype: float32\n",
      "Processed audio shape: torch.Size([3561384])\n",
      "Successfully loaded audio at index 470, shape: torch.Size([160000]), label: 208\n",
      "Successfully loaded audio: audiosets/ontology/Cattle, bovinae_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1669, shape: torch.Size([160000]), label: 306\n",
      "Successfully loaded audio: audiosets/ontology/Duck_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 810, shape: torch.Size([160000]), label: 44\n",
      "Successfully loaded audio: audiosets/ontology/Horse_4.wav, shape: (145265,), dtype: float32\n",
      "Processed audio shape: torch.Size([145265])\n",
      "Successfully loaded audio at index 1720, shape: torch.Size([160000]), label: 410\n",
      "Successfully loaded audio: audiosets/ontology/Cough_0.wav, shape: (2108930,), dtype: float32\n",
      "Processed audio shape: torch.Size([2108930])\n",
      "Successfully loaded audio at index 642, shape: torch.Size([160000]), label: 461\n",
      "Successfully loaded audio: audiosets/ontology/Duck_1.wav, shape: (3136552,), dtype: float32\n",
      "Processed audio shape: torch.Size([3136552])\n",
      "Successfully loaded audio at index 1085, shape: torch.Size([160000]), label: 44\n",
      "Successfully loaded audio: audiosets/ontology/Crow_3.wav, shape: (1707132,), dtype: float32\n",
      "Processed audio shape: torch.Size([1707132])\n",
      "Successfully loaded audio at index 1701, shape: torch.Size([160000]), label: 366\n",
      "Successfully loaded audio: audiosets/ontology/Artillery fire_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 989, shape: torch.Size([160000]), label: 204\n",
      "Successfully loaded audio: audiosets/ontology/Electric toothbrush_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1664, shape: torch.Size([160000]), label: 129\n",
      "Successfully loaded audio: audiosets/ontology/Sheep_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 317, shape: torch.Size([160000]), label: 514\n",
      "Successfully loaded audio: audiosets/ontology/Plucked string instrument_4.wav, shape: (3561384,), dtype: float32\n",
      "Processed audio shape: torch.Size([3561384])\n",
      "Successfully loaded audio at index 169, shape: torch.Size([160000]), label: 38\n",
      "Successfully loaded audio: audiosets/ontology/Bird flight, flapping wings_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1895, shape: torch.Size([160000]), label: 459\n",
      "Successfully loaded audio: audiosets/ontology/Waves, surf_0.wav, shape: (8251072,), dtype: float32\n",
      "Processed audio shape: torch.Size([8251072])\n",
      "Successfully loaded audio at index 162, shape: torch.Size([160000]), label: 105\n",
      "Successfully loaded audio: audiosets/ontology/Dishes, pots, and pans_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1600, shape: torch.Size([160000]), label: 424\n",
      "Successfully loaded audio: audiosets/ontology/Mechanical fan_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2304, shape: torch.Size([160000]), label: 167\n",
      "Successfully loaded audio: audiosets/ontology/Harmonica_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1072, shape: torch.Size([160000]), label: 370\n",
      "Successfully loaded audio: audiosets/ontology/Crackle_0.wav, shape: (3105530,), dtype: float32\n",
      "Processed audio shape: torch.Size([3105530])\n",
      "Successfully loaded audio at index 2200, shape: torch.Size([160000]), label: 18\n",
      "Successfully loaded audio: audiosets/ontology/Goat_0.wav, shape: (7101591,), dtype: float32\n",
      "Processed audio shape: torch.Size([7101591])\n",
      "Successfully loaded audio at index 2004, shape: torch.Size([160000]), label: 284\n",
      "Successfully loaded audio: audiosets/ontology/Canidae, dogs, wolves_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2126, shape: torch.Size([160000]), label: 19\n",
      "Successfully loaded audio: audiosets/ontology/Humming_2.wav, shape: (160086,), dtype: float32\n",
      "Processed audio shape: torch.Size([160086])\n",
      "Successfully loaded audio at index 1438, shape: torch.Size([160000]), label: 475\n",
      "Successfully loaded audio: audiosets/ontology/Female singing_8.wav, shape: (8435531,), dtype: float32\n",
      "Processed audio shape: torch.Size([8435531])\n",
      "Successfully loaded audio at index 752, shape: torch.Size([160000]), label: 423\n",
      "Successfully loaded audio: audiosets/ontology/Medium engine (mid frequency)_0.wav, shape: (528672,), dtype: float32\n",
      "Processed audio shape: torch.Size([528672])\n",
      "Successfully loaded audio at index 374, shape: torch.Size([160000]), label: 455\n",
      "Successfully loaded audio: audiosets/ontology/Doorbell_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 257, shape: torch.Size([160000]), label: 452\n",
      "Successfully loaded audio: audiosets/ontology/Sneeze_2.wav, shape: (699392,), dtype: float32\n",
      "Processed audio shape: torch.Size([699392])\n",
      "Successfully loaded audio at index 684, shape: torch.Size([160000]), label: 320\n",
      "Successfully loaded audio: audiosets/ontology/Water tap, faucet_0.wav, shape: (2992402,), dtype: float32\n",
      "Processed audio shape: torch.Size([2992402])\n",
      "Successfully loaded audio at index 1210, shape: torch.Size([160000]), label: 280\n",
      "Successfully loaded audio: audiosets/ontology/Telephone bell ringing_6.wav, shape: (158639,), dtype: float32\n",
      "Processed audio shape: torch.Size([158639])\n",
      "Successfully loaded audio at index 1182, shape: torch.Size([160000]), label: 376\n",
      "Successfully loaded audio: audiosets/ontology/Shuffling cards_4.wav, shape: (3101258,), dtype: float32\n",
      "Processed audio shape: torch.Size([3101258])\n",
      "Successfully loaded audio at index 513, shape: torch.Size([160000]), label: 81\n",
      "Successfully loaded audio: audiosets/ontology/Jackhammer_2.wav, shape: (905393,), dtype: float32\n",
      "Processed audio shape: torch.Size([905393])\n",
      "Successfully loaded audio at index 2129, shape: torch.Size([160000]), label: 269\n",
      "Successfully loaded audio: audiosets/ontology/Thunder_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1297, shape: torch.Size([160000]), label: 519\n",
      "Successfully loaded audio: audiosets/ontology/Yell_2.wav, shape: (1335798,), dtype: float32\n",
      "Processed audio shape: torch.Size([1335798])\n",
      "Successfully loaded audio at index 2149, shape: torch.Size([160000]), label: 2\n",
      "Successfully loaded audio: audiosets/ontology/Rain on surface_1.wav, shape: (7360912,), dtype: float32\n",
      "Processed audio shape: torch.Size([7360912])\n",
      "Successfully loaded audio at index 115, shape: torch.Size([160000]), label: 31\n",
      "Successfully loaded audio: audiosets/ontology/Sink (filling or washing)_4.wav, shape: (1602920,), dtype: float32\n",
      "Processed audio shape: torch.Size([1602920])\n",
      "Successfully loaded audio at index 772, shape: torch.Size([160000]), label: 394\n",
      "Successfully loaded audio: audiosets/ontology/Single-lens reflex camera_3.wav, shape: (2829491,), dtype: float32\n",
      "Processed audio shape: torch.Size([2829491])\n",
      "Successfully loaded audio at index 854, shape: torch.Size([160000]), label: 449\n",
      "Successfully loaded audio: audiosets/ontology/Vehicle horn, car horn, honking_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 931, shape: torch.Size([160000]), label: 25\n",
      "Successfully loaded audio: audiosets/ontology/Insect_0.wav, shape: (13135064,), dtype: float32\n",
      "Processed audio shape: torch.Size([13135064])\n",
      "Successfully loaded audio at index 118, shape: torch.Size([160000]), label: 374\n",
      "Successfully loaded audio: audiosets/ontology/Howl (wind)_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1083, shape: torch.Size([160000]), label: 431\n",
      "Successfully loaded audio: audiosets/ontology/Steel guitar, slide guitar_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 331, shape: torch.Size([160000]), label: 401\n",
      "Successfully loaded audio: audiosets/ontology/Squeak_1.wav, shape: (6207345,), dtype: float32\n",
      "Processed audio shape: torch.Size([6207345])\n",
      "Successfully loaded audio at index 337, shape: torch.Size([160000]), label: 216\n",
      "Successfully loaded audio: audiosets/ontology/Hubbub, speech noise, speech babble_1.wav, shape: (15067150,), dtype: float32\n",
      "Processed audio shape: torch.Size([15067150])\n",
      "Successfully loaded audio at index 2108, shape: torch.Size([160000]), label: 450\n",
      "Successfully loaded audio: audiosets/ontology/Keyboard (musical)_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1448, shape: torch.Size([160000]), label: 462\n",
      "Successfully loaded audio: audiosets/ontology/Power tool_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 252, shape: torch.Size([160000]), label: 111\n",
      "Successfully loaded audio: audiosets/ontology/Walk, footsteps_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1534, shape: torch.Size([160000]), label: 369\n",
      "Successfully loaded audio: audiosets/ontology/Goat_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 609, shape: torch.Size([160000]), label: 284\n",
      "Successfully loaded audio: audiosets/ontology/Buzzer_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1040, shape: torch.Size([160000]), label: 65\n",
      "Successfully loaded audio: audiosets/ontology/Wood block_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 125, shape: torch.Size([160000]), label: 492\n",
      "Successfully loaded audio: audiosets/ontology/Female singing_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2208, shape: torch.Size([160000]), label: 423\n",
      "Successfully loaded audio: audiosets/ontology/Cellphone buzz, vibrating alert_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2097, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Percussion_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2414, shape: torch.Size([160000]), label: 292\n",
      "Successfully loaded audio: audiosets/ontology/Bird_2.wav, shape: (939201,), dtype: float32\n",
      "Processed audio shape: torch.Size([939201])\n",
      "Successfully loaded audio at index 1286, shape: torch.Size([160000]), label: 97\n",
      "Successfully loaded audio: audiosets/ontology/Dog_2.wav, shape: (1463296,), dtype: float32\n",
      "Processed audio shape: torch.Size([1463296])\n",
      "Successfully loaded audio at index 2408, shape: torch.Size([160000]), label: 19\n",
      "Successfully loaded audio: audiosets/ontology/Pant_2.wav, shape: (1116416,), dtype: float32\n",
      "Processed audio shape: torch.Size([1116416])\n",
      "Successfully loaded audio at index 1650, shape: torch.Size([160000]), label: 26\n",
      "Successfully loaded audio: audiosets/ontology/Car alarm_1.wav, shape: (2974755,), dtype: float32\n",
      "Processed audio shape: torch.Size([2974755])\n",
      "Successfully loaded audio at index 1421, shape: torch.Size([160000]), label: 29\n",
      "Successfully loaded audio: audiosets/ontology/Rattle_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 260, shape: torch.Size([160000]), label: 219\n",
      "Successfully loaded audio: audiosets/ontology/Wild animals_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 442, shape: torch.Size([160000]), label: 353\n",
      "Successfully loaded audio: audiosets/ontology/Civil defense siren_2.wav, shape: (2042613,), dtype: float32\n",
      "Processed audio shape: torch.Size([2042613])\n",
      "Successfully loaded audio at index 275, shape: torch.Size([160000]), label: 70\n",
      "Successfully loaded audio: audiosets/ontology/Drill_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1079, shape: torch.Size([160000]), label: 426\n",
      "Successfully loaded audio: audiosets/ontology/Mallet percussion_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 506, shape: torch.Size([160000]), label: 268\n",
      "Successfully loaded audio: audiosets/ontology/Fowl_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 344, shape: torch.Size([160000]), label: 45\n",
      "Successfully loaded audio: audiosets/ontology/Vacuum cleaner_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1954, shape: torch.Size([160000]), label: 494\n",
      "Successfully loaded audio: audiosets/ontology/Hubbub, speech noise, speech babble_3.wav, shape: (3888135,), dtype: float32\n",
      "Processed audio shape: torch.Size([3888135])\n",
      "Successfully loaded audio at index 936, shape: torch.Size([160000]), label: 450\n",
      "Successfully loaded audio: audiosets/ontology/Insect_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 661, shape: torch.Size([160000]), label: 374\n",
      "Successfully loaded audio: audiosets/ontology/Railroad car, train wagon_9.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1208, shape: torch.Size([160000]), label: 271\n",
      "Successfully loaded audio: audiosets/ontology/Water_8.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1548, shape: torch.Size([160000]), label: 357\n",
      "Successfully loaded audio: audiosets/ontology/Scratching (performance technique)_7.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 952, shape: torch.Size([160000]), label: 479\n",
      "Successfully loaded audio: audiosets/ontology/Finger snapping_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 410, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Car_1.wav, shape: (160086,), dtype: float32\n",
      "Processed audio shape: torch.Size([160086])\n",
      "Successfully loaded audio at index 1512, shape: torch.Size([160000]), label: 29\n",
      "Successfully loaded audio: audiosets/ontology/Railroad car, train wagon_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 38, shape: torch.Size([160000]), label: 29\n",
      "Successfully loaded audio: audiosets/ontology/Bowed string instrument_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1805, shape: torch.Size([160000]), label: 38\n",
      "Successfully loaded audio: audiosets/ontology/Belly laugh_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 696, shape: torch.Size([160000]), label: 107\n",
      "Successfully loaded audio: audiosets/ontology/Babbling_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 725, shape: torch.Size([160000]), label: 178\n",
      "Successfully loaded audio: audiosets/ontology/Jackhammer_4.wav, shape: (1927257,), dtype: float32\n",
      "Processed audio shape: torch.Size([1927257])\n",
      "Successfully loaded audio at index 932, shape: torch.Size([160000]), label: 487\n",
      "Successfully loaded audio: audiosets/ontology/Bowed string instrument_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 190, shape: torch.Size([160000]), label: 38\n",
      "Successfully loaded audio: audiosets/ontology/Howl (wind)_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 743, shape: torch.Size([160000]), label: 42\n",
      "Successfully loaded audio: audiosets/ontology/Child singing_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2401, shape: torch.Size([160000]), label: 423\n",
      "Successfully loaded audio: audiosets/ontology/Singing bowl_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1053, shape: torch.Size([160000]), label: 205\n",
      "Successfully loaded audio: audiosets/ontology/Motor vehicle (road)_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1583, shape: torch.Size([160000]), label: 497\n",
      "Successfully loaded audio: audiosets/ontology/Bus_5.wav, shape: (157153,), dtype: float32\n",
      "Processed audio shape: torch.Size([157153])\n",
      "Successfully loaded audio at index 1579, shape: torch.Size([160000]), label: 453\n",
      "Successfully loaded audio: audiosets/ontology/Cello_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1194, shape: torch.Size([160000]), label: 397\n",
      "Successfully loaded audio: audiosets/ontology/Police car (siren)_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1788, shape: torch.Size([160000]), label: 330\n",
      "Successfully loaded audio: audiosets/ontology/Accelerating, revving, vroom_3.wav, shape: (608921,), dtype: float32\n",
      "Processed audio shape: torch.Size([608921])\n",
      "Successfully loaded audio at index 985, shape: torch.Size([160000]), label: 403\n",
      "Successfully loaded audio: audiosets/ontology/Race car, auto racing_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 239, shape: torch.Size([160000]), label: 110\n",
      "Successfully loaded audio: audiosets/ontology/Clip-clop_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 729, shape: torch.Size([160000]), label: 502\n",
      "Successfully loaded audio: audiosets/ontology/Jingle bell_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1017, shape: torch.Size([160000]), label: 107\n",
      "Successfully loaded audio: audiosets/ontology/Snicker_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 754, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Buzzer_4.wav, shape: (5823488,), dtype: float32\n",
      "Processed audio shape: torch.Size([5823488])\n",
      "Successfully loaded audio at index 668, shape: torch.Size([160000]), label: 325\n",
      "Successfully loaded audio: audiosets/ontology/Bagpipes_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 259, shape: torch.Size([160000]), label: 67\n",
      "Successfully loaded audio: audiosets/ontology/Electronic organ_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2031, shape: torch.Size([160000]), label: 145\n",
      "Successfully loaded audio: audiosets/ontology/Car passing by_1.wav, shape: (1009790,), dtype: float32\n",
      "Processed audio shape: torch.Size([1009790])\n",
      "Successfully loaded audio at index 269, shape: torch.Size([160000]), label: 29\n",
      "Successfully loaded audio: audiosets/ontology/Water_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 626, shape: torch.Size([160000]), label: 357\n",
      "Successfully loaded audio: audiosets/ontology/Speech synthesizer_0.wav, shape: (2966396,), dtype: float32\n",
      "Processed audio shape: torch.Size([2966396])\n",
      "Successfully loaded audio at index 1882, shape: torch.Size([160000]), label: 451\n",
      "Successfully loaded audio: audiosets/ontology/Machine gun_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 285, shape: torch.Size([160000]), label: 156\n",
      "Successfully loaded audio: audiosets/ontology/Brass instrument_8.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 657, shape: torch.Size([160000]), label: 308\n",
      "Successfully loaded audio: audiosets/ontology/Clock_5.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1746, shape: torch.Size([160000]), label: 276\n",
      "Successfully loaded audio: audiosets/ontology/Firecracker_2.wav, shape: (4145413,), dtype: float32\n",
      "Processed audio shape: torch.Size([4145413])\n",
      "Successfully loaded audio at index 1024, shape: torch.Size([160000]), label: 204\n",
      "Successfully loaded audio: audiosets/ontology/Aircraft engine_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 428, shape: torch.Size([160000]), label: 471\n",
      "Successfully loaded audio: audiosets/ontology/Hammond organ_5.wav, shape: (1154125,), dtype: float32\n",
      "Processed audio shape: torch.Size([1154125])\n",
      "Successfully loaded audio at index 2223, shape: torch.Size([160000]), label: 311\n",
      "Successfully loaded audio: audiosets/ontology/Filing (rasp)_1.wav, shape: (8418256,), dtype: float32\n",
      "Processed audio shape: torch.Size([8418256])\n",
      "Successfully loaded audio at index 1617, shape: torch.Size([160000]), label: 274\n",
      "Successfully loaded audio: audiosets/ontology/Harp_0.wav, shape: (4583991,), dtype: float32\n",
      "Processed audio shape: torch.Size([4583991])\n",
      "Successfully loaded audio at index 1047, shape: torch.Size([160000]), label: 420\n",
      "Successfully loaded audio: audiosets/ontology/Crow_5.wav, shape: (6651310,), dtype: float32\n",
      "Processed audio shape: torch.Size([6651310])\n",
      "Successfully loaded audio at index 95, shape: torch.Size([160000]), label: 366\n",
      "Successfully loaded audio: audiosets/ontology/Speech synthesizer_0.wav, shape: (2966396,), dtype: float32\n",
      "Processed audio shape: torch.Size([2966396])\n",
      "Successfully loaded audio at index 1644, shape: torch.Size([160000]), label: 430\n",
      "Successfully loaded audio: audiosets/ontology/Drum_1.wav, shape: (3139153,), dtype: float32\n",
      "Processed audio shape: torch.Size([3139153])\n",
      "Successfully loaded audio at index 1790, shape: torch.Size([160000]), label: 93\n",
      "Successfully loaded audio: audiosets/ontology/Rattle (instrument)_1.wav, shape: (3809559,), dtype: float32\n",
      "Processed audio shape: torch.Size([3809559])\n",
      "Successfully loaded audio at index 2280, shape: torch.Size([160000]), label: 38\n",
      "Successfully loaded audio: audiosets/ontology/Hands_8.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 596, shape: torch.Size([160000]), label: 458\n",
      "Successfully loaded audio: audiosets/ontology/Male speech, man speaking_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1230, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Scratching (performance technique)_7.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1812, shape: torch.Size([160000]), label: 104\n",
      "Successfully loaded audio: audiosets/ontology/Drawer open or close_3.wav, shape: (3770921,), dtype: float32\n",
      "Processed audio shape: torch.Size([3770921])\n",
      "Successfully loaded audio at index 948, shape: torch.Size([160000]), label: 447\n",
      "Successfully loaded audio: audiosets/ontology/Boat, Water vehicle_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 272, shape: torch.Size([160000]), label: 357\n",
      "Successfully loaded audio: audiosets/ontology/Light engine (high frequency)_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1925, shape: torch.Size([160000]), label: 455\n",
      "Successfully loaded audio: audiosets/ontology/Cymbal_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1489, shape: torch.Size([160000]), label: 59\n",
      "Successfully loaded audio: audiosets/ontology/Burping, eructation_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1683, shape: torch.Size([160000]), label: 82\n",
      "Successfully loaded audio: audiosets/ontology/Electric guitar_3.wav, shape: (10330280,), dtype: float32\n",
      "Processed audio shape: torch.Size([10330280])\n",
      "Successfully loaded audio at index 1466, shape: torch.Size([160000]), label: 401\n",
      "Successfully loaded audio: audiosets/ontology/Goat_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2023, shape: torch.Size([160000]), label: 284\n",
      "Successfully loaded audio: audiosets/ontology/Chopping (food)_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1146, shape: torch.Size([160000]), label: 408\n",
      "Successfully loaded audio: audiosets/ontology/Motorcycle_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 731, shape: torch.Size([160000]), label: 473\n",
      "Successfully loaded audio: audiosets/ontology/Drum machine_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1870, shape: torch.Size([160000]), label: 93\n",
      "Successfully loaded audio: audiosets/ontology/Child speech, kid speaking_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2032, shape: torch.Size([160000]), label: 430\n",
      "Successfully loaded audio: audiosets/ontology/Livestock, farm animals, working animals_4.wav, shape: (1133692,), dtype: float32\n",
      "Processed audio shape: torch.Size([1133692])\n",
      "Successfully loaded audio at index 1313, shape: torch.Size([160000]), label: 124\n",
      "Successfully loaded audio: audiosets/ontology/Whistle_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 446, shape: torch.Size([160000]), label: 439\n",
      "Successfully loaded audio: audiosets/ontology/Bus_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1711, shape: torch.Size([160000]), label: 453\n",
      "Successfully loaded audio: audiosets/ontology/Electric piano_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1319, shape: torch.Size([160000]), label: 386\n",
      "Successfully loaded audio: audiosets/ontology/Civil defense siren_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2102, shape: torch.Size([160000]), label: 128\n",
      "Successfully loaded audio: audiosets/ontology/Frog_1.wav, shape: (4071852,), dtype: float32\n",
      "Processed audio shape: torch.Size([4071852])\n",
      "Successfully loaded audio at index 1308, shape: torch.Size([160000]), label: 428\n",
      "Successfully loaded audio: audiosets/ontology/Sitar_0.wav, shape: (4803745,), dtype: float32\n",
      "Processed audio shape: torch.Size([4803745])\n",
      "Successfully loaded audio at index 2407, shape: torch.Size([160000]), label: 419\n",
      "Successfully loaded audio: audiosets/ontology/Toot_1.wav, shape: (10589229,), dtype: float32\n",
      "Processed audio shape: torch.Size([10589229])\n",
      "Successfully loaded audio at index 1483, shape: torch.Size([160000]), label: 160\n",
      "Successfully loaded audio: audiosets/ontology/Rapping_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 560, shape: torch.Size([160000]), label: 173\n",
      "Successfully loaded audio: audiosets/ontology/Pant_4.wav, shape: (1658539,), dtype: float32\n",
      "Processed audio shape: torch.Size([1658539])\n",
      "Successfully loaded audio at index 557, shape: torch.Size([160000]), label: 26\n",
      "Successfully loaded audio: audiosets/ontology/Electronic organ_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 892, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Trumpet_1.wav, shape: (1754686,), dtype: float32\n",
      "Processed audio shape: torch.Size([1754686])\n",
      "Successfully loaded audio at index 844, shape: torch.Size([160000]), label: 170\n",
      "Successfully loaded audio: audiosets/ontology/Yell_0.wav, shape: (829782,), dtype: float32\n",
      "Processed audio shape: torch.Size([829782])\n",
      "Successfully loaded audio at index 1175, shape: torch.Size([160000]), label: 2\n",
      "Successfully loaded audio: audiosets/ontology/Hammond organ_1.wav, shape: (2858841,), dtype: float32\n",
      "Processed audio shape: torch.Size([2858841])\n",
      "Successfully loaded audio at index 1041, shape: torch.Size([160000]), label: 145\n",
      "Successfully loaded audio: audiosets/ontology/Howl_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1960, shape: torch.Size([160000]), label: 87\n",
      "Successfully loaded audio: audiosets/ontology/Conversation_5.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 949, shape: torch.Size([160000]), label: 257\n",
      "Successfully loaded audio: audiosets/ontology/Ship_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1485, shape: torch.Size([160000]), label: 232\n",
      "Successfully loaded audio: audiosets/ontology/Fire alarm_5.wav, shape: (3732097,), dtype: float32\n",
      "Processed audio shape: torch.Size([3732097])\n",
      "Successfully loaded audio at index 1891, shape: torch.Size([160000]), label: 384\n",
      "Successfully loaded audio: audiosets/ontology/Thunderstorm_3.wav, shape: (347743,), dtype: float32\n",
      "Processed audio shape: torch.Size([347743])\n",
      "Successfully loaded audio at index 120, shape: torch.Size([160000]), label: 519\n",
      "Successfully loaded audio: audiosets/ontology/Tick-tock_1.wav, shape: (998644,), dtype: float32\n",
      "Processed audio shape: torch.Size([998644])\n",
      "Successfully loaded audio at index 1739, shape: torch.Size([160000]), label: 418\n",
      "Successfully loaded audio: audiosets/ontology/Bird vocalization, bird call, bird song_3.wav, shape: (6209016,), dtype: float32\n",
      "Processed audio shape: torch.Size([6209016])\n",
      "Successfully loaded audio at index 638, shape: torch.Size([160000]), label: 114\n",
      "Successfully loaded audio: audiosets/ontology/Harpsichord_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1786, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Police car (siren)_5.wav, shape: (150837,), dtype: float32\n",
      "Processed audio shape: torch.Size([150837])\n",
      "Successfully loaded audio at index 2396, shape: torch.Size([160000]), label: 29\n",
      "Successfully loaded audio: audiosets/ontology/Buzzer_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 942, shape: torch.Size([160000]), label: 65\n",
      "Successfully loaded audio: audiosets/ontology/Skateboard_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 550, shape: torch.Size([160000]), label: 495\n",
      "Successfully loaded audio: audiosets/ontology/Crowing, cock-a-doodle-doo_8.wav, shape: (151209,), dtype: float32\n",
      "Processed audio shape: torch.Size([151209])\n",
      "Successfully loaded audio at index 1476, shape: torch.Size([160000]), label: 366\n",
      "Successfully loaded audio: audiosets/ontology/Acoustic guitar_9.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 547, shape: torch.Size([160000]), label: 472\n",
      "Successfully loaded audio: audiosets/ontology/Brass instrument_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1386, shape: torch.Size([160000]), label: 308\n",
      "Successfully loaded audio: audiosets/ontology/Boom_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1993, shape: torch.Size([160000]), label: 188\n",
      "Successfully loaded audio: audiosets/ontology/Fire_0.wav, shape: (871399,), dtype: float32\n",
      "Processed audio shape: torch.Size([871399])\n",
      "Successfully loaded audio at index 242, shape: torch.Size([160000]), label: 204\n",
      "Successfully loaded audio: audiosets/ontology/Aircraft_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 641, shape: torch.Size([160000]), label: 460\n",
      "Successfully loaded audio: audiosets/ontology/Glass_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 674, shape: torch.Size([160000]), label: 489\n",
      "Successfully loaded audio: audiosets/ontology/Drill_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 102, shape: torch.Size([160000]), label: 426\n",
      "Successfully loaded audio: audiosets/ontology/Train_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1012, shape: torch.Size([160000]), label: 436\n",
      "Successfully loaded audio: audiosets/ontology/Caterwaul_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1449, shape: torch.Size([160000]), label: 387\n",
      "Successfully loaded audio: audiosets/ontology/Drum kit_5.wav, shape: (3296120,), dtype: float32\n",
      "Processed audio shape: torch.Size([3296120])\n",
      "Successfully loaded audio at index 553, shape: torch.Size([160000]), label: 93\n",
      "Successfully loaded audio: audiosets/ontology/Traffic noise, roadway noise_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1881, shape: torch.Size([160000]), label: 450\n",
      "Successfully loaded audio: audiosets/ontology/Electric shaver, electric razor_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 2315, shape: torch.Size([160000]), label: 149\n",
      "Successfully loaded audio: audiosets/ontology/Crow_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2270, shape: torch.Size([160000]), label: 366\n",
      "Successfully loaded audio: audiosets/ontology/Mosquito_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1639, shape: torch.Size([160000]), label: 286\n",
      "Successfully loaded audio: audiosets/ontology/Musical instrument_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1270, shape: torch.Size([160000]), label: 462\n",
      "Successfully loaded audio: audiosets/ontology/Ship_2.wav, shape: (8006612,), dtype: float32\n",
      "Processed audio shape: torch.Size([8006612])\n",
      "Successfully loaded audio at index 477, shape: torch.Size([160000]), label: 232\n",
      "Successfully loaded audio: audiosets/ontology/Vehicle_6.wav, shape: (13221628,), dtype: float32\n",
      "Processed audio shape: torch.Size([13221628])\n",
      "Successfully loaded audio at index 1665, shape: torch.Size([160000]), label: 25\n",
      "Successfully loaded audio: audiosets/ontology/Railroad car, train wagon_8.wav, shape: (5533966,), dtype: float32\n",
      "Processed audio shape: torch.Size([5533966])\n",
      "Successfully loaded audio at index 138, shape: torch.Size([160000]), label: 29\n",
      "Successfully loaded audio: audiosets/ontology/Speech synthesizer_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1239, shape: torch.Size([160000]), label: 430\n",
      "Successfully loaded audio: audiosets/ontology/Sheep_5.wav, shape: (156410,), dtype: float32\n",
      "Processed audio shape: torch.Size([156410])\n",
      "Successfully loaded audio at index 356, shape: torch.Size([160000]), label: 514\n",
      "Successfully loaded audio: audiosets/ontology/Steam whistle_5.wav, shape: (6059851,), dtype: float32\n",
      "Processed audio shape: torch.Size([6059851])\n",
      "Successfully loaded audio at index 1854, shape: torch.Size([160000]), label: 439\n",
      "Successfully loaded audio: audiosets/ontology/Wildfire_1.wav, shape: (4305352,), dtype: float32\n",
      "Processed audio shape: torch.Size([4305352])\n",
      "Successfully loaded audio at index 895, shape: torch.Size([160000]), label: 204\n",
      "Successfully loaded audio: audiosets/ontology/Fixed-wing aircraft, airplane_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 598, shape: torch.Size([160000]), label: 460\n",
      "Successfully loaded audio: audiosets/ontology/Canidae, dogs, wolves_6.wav, shape: (1507328,), dtype: float32\n",
      "Processed audio shape: torch.Size([1507328])\n",
      "Successfully loaded audio at index 1806, shape: torch.Size([160000]), label: 19\n",
      "Successfully loaded audio: audiosets/ontology/Chink, clink_2.wav, shape: (153066,), dtype: float32\n",
      "Processed audio shape: torch.Size([153066])\n",
      "Successfully loaded audio at index 341, shape: torch.Size([160000]), label: 312\n",
      "Successfully loaded audio: audiosets/ontology/Telephone bell ringing_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2053, shape: torch.Size([160000]), label: 148\n",
      "Successfully loaded audio: audiosets/ontology/Zither_0.wav, shape: (8280236,), dtype: float32\n",
      "Processed audio shape: torch.Size([8280236])\n",
      "Successfully loaded audio at index 1988, shape: torch.Size([160000]), label: 281\n",
      "Successfully loaded audio: audiosets/ontology/Rattle_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1013, shape: torch.Size([160000]), label: 219\n",
      "Successfully loaded audio: audiosets/ontology/Hi-hat_3.wav, shape: (2509427,), dtype: float32\n",
      "Processed audio shape: torch.Size([2509427])\n",
      "Successfully loaded audio at index 861, shape: torch.Size([160000]), label: 316\n",
      "Successfully loaded audio: audiosets/ontology/Kettle whistle_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 492, shape: torch.Size([160000]), label: 439\n",
      "Successfully loaded audio: audiosets/ontology/Tools_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 534, shape: torch.Size([160000]), label: 442\n",
      "Successfully loaded audio: audiosets/ontology/Child speech, kid speaking_8.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 295, shape: torch.Size([160000]), label: 346\n",
      "Successfully loaded audio: audiosets/ontology/Bird flight, flapping wings_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1408, shape: torch.Size([160000]), label: 459\n",
      "Successfully loaded audio: audiosets/ontology/Pigeon, dove_8.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1654, shape: torch.Size([160000]), label: 260\n",
      "Successfully loaded audio: audiosets/ontology/Frying (food)_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1417, shape: torch.Size([160000]), label: 51\n",
      "Successfully loaded audio: audiosets/ontology/Machine gun_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2291, shape: torch.Size([160000]), label: 156\n",
      "Successfully loaded audio: audiosets/ontology/Bus_3.wav, shape: (9626065,), dtype: float32\n",
      "Processed audio shape: torch.Size([9626065])\n",
      "Successfully loaded audio at index 83, shape: torch.Size([160000]), label: 453\n",
      "Successfully loaded audio: audiosets/ontology/Rapping_8.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 212, shape: torch.Size([160000]), label: 273\n",
      "Successfully loaded audio: audiosets/ontology/Air horn, truck horn_6.wav, shape: (16647036,), dtype: float32\n",
      "Processed audio shape: torch.Size([16647036])\n",
      "Successfully loaded audio at index 1031, shape: torch.Size([160000]), label: 132\n",
      "Successfully loaded audio: audiosets/ontology/Finger snapping_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1015, shape: torch.Size([160000]), label: 482\n",
      "Successfully loaded audio: audiosets/ontology/Caw_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2313, shape: torch.Size([160000]), label: 163\n",
      "Successfully loaded audio: audiosets/ontology/Music_8.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1490, shape: torch.Size([160000]), label: 462\n",
      "Successfully loaded audio: audiosets/ontology/Rain on surface_2.wav, shape: (541490,), dtype: float32\n",
      "Processed audio shape: torch.Size([541490])\n",
      "Successfully loaded audio at index 132, shape: torch.Size([160000]), label: 175\n",
      "Successfully loaded audio: audiosets/ontology/Car_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2067, shape: torch.Size([160000]), label: 29\n",
      "Successfully loaded audio: audiosets/ontology/Sanding_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1817, shape: torch.Size([160000]), label: 15\n",
      "Successfully loaded audio: audiosets/ontology/Wind instrument, woodwind instrument_5.wav, shape: (6540412,), dtype: float32\n",
      "Processed audio shape: torch.Size([6540412])\n",
      "Successfully loaded audio at index 65, shape: torch.Size([160000]), label: 38\n",
      "Successfully loaded audio: audiosets/ontology/Engine starting_2.wav, shape: (156039,), dtype: float32\n",
      "Processed audio shape: torch.Size([156039])\n",
      "Successfully loaded audio at index 864, shape: torch.Size([160000]), label: 455\n",
      "Successfully loaded audio: audiosets/ontology/Whoop_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 888, shape: torch.Size([160000]), label: 11\n",
      "Successfully loaded audio: audiosets/ontology/Chewing, mastication_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1116, shape: torch.Size([160000]), label: 91\n",
      "Successfully loaded audio: audiosets/ontology/Telephone bell ringing_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 840, shape: torch.Size([160000]), label: 107\n",
      "Successfully loaded audio: audiosets/ontology/Baby laughter_6.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1556, shape: torch.Size([160000]), label: 372\n",
      "Successfully loaded audio: audiosets/ontology/Electronic organ_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1798, shape: torch.Size([160000]), label: 41\n",
      "Successfully loaded audio: audiosets/ontology/Drawer open or close_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 127, shape: torch.Size([160000]), label: 447\n",
      "Successfully loaded audio: audiosets/ontology/Maraca_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 482, shape: torch.Size([160000]), label: 416\n",
      "Successfully loaded audio: audiosets/ontology/Rapping_9.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2178, shape: torch.Size([160000]), label: 273\n",
      "Successfully loaded audio: audiosets/ontology/Rattle (instrument)_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2388, shape: torch.Size([160000]), label: 56\n",
      "Successfully loaded audio: audiosets/ontology/Children playing_3.wav, shape: (2458715,), dtype: float32\n",
      "Processed audio shape: torch.Size([2458715])\n",
      "Successfully loaded audio at index 1000, shape: torch.Size([160000]), label: 498\n",
      "Successfully loaded audio: audiosets/ontology/Bird flight, flapping wings_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 742, shape: torch.Size([160000]), label: 97\n",
      "Successfully loaded audio: audiosets/ontology/Brass instrument_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 954, shape: torch.Size([160000]), label: 38\n",
      "Successfully loaded audio: audiosets/ontology/Aircraft engine_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 247, shape: torch.Size([160000]), label: 471\n",
      "Successfully loaded audio: audiosets/ontology/Drum kit_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 693, shape: torch.Size([160000]), label: 21\n",
      "Successfully loaded audio: audiosets/ontology/Baby cry, infant cry_1.wav, shape: (155667,), dtype: float32\n",
      "Processed audio shape: torch.Size([155667])\n",
      "Successfully loaded audio at index 173, shape: torch.Size([160000]), label: 144\n",
      "Successfully loaded audio: audiosets/ontology/Singing_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1117, shape: torch.Size([160000]), label: 423\n",
      "Successfully loaded audio: audiosets/ontology/Wind chime_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 2365, shape: torch.Size([160000]), label: 80\n",
      "Successfully loaded audio: audiosets/ontology/Drawer open or close_2.wav, shape: (2040199,), dtype: float32\n",
      "Processed audio shape: torch.Size([2040199])\n",
      "Successfully loaded audio at index 1350, shape: torch.Size([160000]), label: 447\n",
      "Successfully loaded audio: audiosets/ontology/Bird flight, flapping wings_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 304, shape: torch.Size([160000]), label: 173\n",
      "Successfully loaded audio: audiosets/ontology/Sliding door_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1484, shape: torch.Size([160000]), label: 248\n",
      "Successfully loaded audio: audiosets/ontology/Boat, Water vehicle_6.wav, shape: (12422827,), dtype: float32\n",
      "Processed audio shape: torch.Size([12422827])\n",
      "Successfully loaded audio at index 2402, shape: torch.Size([160000]), label: 25\n",
      "Successfully loaded audio: audiosets/ontology/Velcro, hook and loop fastener_1.wav, shape: (488548,), dtype: float32\n",
      "Processed audio shape: torch.Size([488548])\n",
      "Successfully loaded audio at index 2387, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Hands_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1773, shape: torch.Size([160000]), label: 458\n",
      "Successfully loaded audio: audiosets/ontology/Female speech, woman speaking_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1779, shape: torch.Size([160000]), label: 430\n",
      "Successfully loaded audio: audiosets/ontology/Coin (dropping)_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1884, shape: torch.Size([160000]), label: 176\n",
      "Successfully loaded audio: audiosets/ontology/Acoustic guitar_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1803, shape: torch.Size([160000]), label: 401\n",
      "Successfully loaded audio: audiosets/ontology/Railroad car, train wagon_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2054, shape: torch.Size([160000]), label: 29\n",
      "Successfully loaded audio: audiosets/ontology/Bleat_4.wav, shape: (148980,), dtype: float32\n",
      "Processed audio shape: torch.Size([148980])\n",
      "Successfully loaded audio at index 92, shape: torch.Size([160000]), label: 193\n",
      "Successfully loaded audio: audiosets/ontology/Finger snapping_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1532, shape: torch.Size([160000]), label: 482\n",
      "Successfully loaded audio: audiosets/ontology/Double bass_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1266, shape: torch.Size([160000]), label: 102\n",
      "Successfully loaded audio: audiosets/ontology/Steam whistle_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 144, shape: torch.Size([160000]), label: 288\n",
      "Successfully loaded audio: audiosets/ontology/Pulleys_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1653, shape: torch.Size([160000]), label: 158\n",
      "Successfully loaded audio: audiosets/ontology/Propeller, airscrew_1.wav, shape: (28950454,), dtype: float32\n",
      "Processed audio shape: torch.Size([28950454])\n",
      "Successfully loaded audio at index 951, shape: torch.Size([160000]), label: 335\n",
      "Successfully loaded audio: audiosets/ontology/Bird vocalization, bird call, bird song_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 72, shape: torch.Size([160000]), label: 114\n",
      "Successfully loaded audio: audiosets/ontology/Skidding_2.wav, shape: (10691768,), dtype: float32\n",
      "Processed audio shape: torch.Size([10691768])\n",
      "Successfully loaded audio at index 191, shape: torch.Size([160000]), label: 15\n",
      "Successfully loaded audio: audiosets/ontology/Chicken, rooster_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1414, shape: torch.Size([160000]), label: 413\n",
      "Successfully loaded audio: audiosets/ontology/Gargling_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 423, shape: torch.Size([160000]), label: 36\n",
      "Successfully loaded audio: audiosets/ontology/Emergency vehicle_5.wav, shape: (8075529,), dtype: float32\n",
      "Processed audio shape: torch.Size([8075529])\n",
      "Successfully loaded audio at index 1383, shape: torch.Size([160000]), label: 253\n",
      "Successfully loaded audio: audiosets/ontology/Bird flight, flapping wings_4.wav, shape: (6224434,), dtype: float32\n",
      "Processed audio shape: torch.Size([6224434])\n",
      "Successfully loaded audio at index 1957, shape: torch.Size([160000]), label: 459\n",
      "Successfully loaded audio: audiosets/ontology/Bass drum_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2246, shape: torch.Size([160000]), label: 247\n",
      "Successfully loaded audio: audiosets/ontology/Alarm clock_1.wav, shape: (1837056,), dtype: float32\n",
      "Processed audio shape: torch.Size([1837056])\n",
      "Successfully loaded audio at index 1901, shape: torch.Size([160000]), label: 384\n",
      "Successfully loaded audio: audiosets/ontology/Electric piano_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1063, shape: torch.Size([160000]), label: 407\n",
      "Successfully loaded audio: audiosets/ontology/Stream_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2039, shape: torch.Size([160000]), label: 485\n",
      "Successfully loaded audio: audiosets/ontology/Fixed-wing aircraft, airplane_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 218, shape: torch.Size([160000]), label: 150\n",
      "Successfully loaded audio: audiosets/ontology/Toilet flush_5.wav, shape: (723163,), dtype: float32\n",
      "Processed audio shape: torch.Size([723163])\n",
      "Successfully loaded audio at index 1698, shape: torch.Size([160000]), label: 454\n",
      "Successfully loaded audio: audiosets/ontology/Piano_6.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1538, shape: torch.Size([160000]), label: 386\n",
      "Successfully loaded audio: audiosets/ontology/Cowbell_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 411, shape: torch.Size([160000]), label: 107\n",
      "Successfully loaded audio: audiosets/ontology/Kettle whistle_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1834, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Music_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 326, shape: torch.Size([160000]), label: 462\n",
      "Successfully loaded audio: audiosets/ontology/Gunshot, gunfire_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 736, shape: torch.Size([160000]), label: 318\n",
      "Successfully loaded audio: audiosets/ontology/Female speech, woman speaking_8.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2046, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Baby laughter_0.wav, shape: (1603663,), dtype: float32\n",
      "Processed audio shape: torch.Size([1603663])\n",
      "Successfully loaded audio at index 690, shape: torch.Size([160000]), label: 263\n",
      "Successfully loaded audio: audiosets/ontology/Tools_7.wav, shape: (14015937,), dtype: float32\n",
      "Processed audio shape: torch.Size([14015937])\n",
      "Successfully loaded audio at index 1525, shape: torch.Size([160000]), label: 442\n",
      "Successfully loaded audio: audiosets/ontology/Water tap, faucet_5.wav, shape: (160086,), dtype: float32\n",
      "Processed audio shape: torch.Size([160086])\n",
      "Successfully loaded audio at index 1011, shape: torch.Size([160000]), label: 187\n",
      "Successfully loaded audio: audiosets/ontology/Cattle, bovinae_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1252, shape: torch.Size([160000]), label: 306\n",
      "Successfully loaded audio: audiosets/ontology/Double bass_1.wav, shape: (4094143,), dtype: float32\n",
      "Processed audio shape: torch.Size([4094143])\n",
      "Successfully loaded audio at index 1864, shape: torch.Size([160000]), label: 102\n",
      "Successfully loaded audio: audiosets/ontology/Telephone bell ringing_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2074, shape: torch.Size([160000]), label: 376\n",
      "Successfully loaded audio: audiosets/ontology/Snare drum_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1765, shape: torch.Size([160000]), label: 336\n",
      "Successfully loaded audio: audiosets/ontology/Fixed-wing aircraft, airplane_6.wav, shape: (148608,), dtype: float32\n",
      "Processed audio shape: torch.Size([148608])\n",
      "Successfully loaded audio at index 2312, shape: torch.Size([160000]), label: 460\n",
      "Successfully loaded audio: audiosets/ontology/Wind noise (microphone)_3.wav, shape: (13654358,), dtype: float32\n",
      "Processed audio shape: torch.Size([13654358])\n",
      "Successfully loaded audio at index 969, shape: torch.Size([160000]), label: 450\n",
      "Successfully loaded audio: audiosets/ontology/Ship_3.wav, shape: (2851225,), dtype: float32\n",
      "Processed audio shape: torch.Size([2851225])\n",
      "Successfully loaded audio at index 1231, shape: torch.Size([160000]), label: 232\n",
      "Successfully loaded audio: audiosets/ontology/Power windows, electric windows_1.wav, shape: (988242,), dtype: float32\n",
      "Processed audio shape: torch.Size([988242])\n",
      "Successfully loaded audio at index 1122, shape: torch.Size([160000]), label: 42\n",
      "Successfully loaded audio: audiosets/ontology/Fire alarm_5.wav, shape: (3732097,), dtype: float32\n",
      "Processed audio shape: torch.Size([3732097])\n",
      "Successfully loaded audio at index 1843, shape: torch.Size([160000]), label: 326\n",
      "Successfully loaded audio: audiosets/ontology/Stomach rumble_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1463, shape: torch.Size([160000]), label: 53\n",
      "Successfully loaded audio: audiosets/ontology/Shuffle_0.wav, shape: (157896,), dtype: float32\n",
      "Processed audio shape: torch.Size([157896])\n",
      "Successfully loaded audio at index 940, shape: torch.Size([160000]), label: 329\n",
      "Successfully loaded audio: audiosets/ontology/Wildfire_0.wav, shape: (5924990,), dtype: float32\n",
      "Processed audio shape: torch.Size([5924990])\n",
      "Successfully loaded audio at index 1454, shape: torch.Size([160000]), label: 204\n",
      "Successfully loaded audio: audiosets/ontology/Acoustic guitar_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1409, shape: torch.Size([160000]), label: 401\n",
      "Successfully loaded audio: audiosets/ontology/Walk, footsteps_6.wav, shape: (5261271,), dtype: float32\n",
      "Processed audio shape: torch.Size([5261271])\n",
      "Successfully loaded audio at index 1963, shape: torch.Size([160000]), label: 369\n",
      "Successfully loaded audio: audiosets/ontology/Ocean_2.wav, shape: (24999161,), dtype: float32\n",
      "Processed audio shape: torch.Size([24999161])\n",
      "Successfully loaded audio at index 365, shape: torch.Size([160000]), label: 352\n",
      "Successfully loaded audio: audiosets/ontology/Alarm clock_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1233, shape: torch.Size([160000]), label: 276\n",
      "Successfully loaded audio: audiosets/ontology/Chopping (food)_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1043, shape: torch.Size([160000]), label: 173\n",
      "Successfully loaded audio: audiosets/ontology/Quack_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 898, shape: torch.Size([160000]), label: 115\n",
      "Successfully loaded audio: audiosets/ontology/Wind_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1784, shape: torch.Size([160000]), label: 42\n",
      "Successfully loaded audio: audiosets/ontology/Wildfire_0.wav, shape: (5924990,), dtype: float32\n",
      "Processed audio shape: torch.Size([5924990])\n",
      "Successfully loaded audio at index 367, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Rattle (instrument)_1.wav, shape: (3809559,), dtype: float32\n",
      "Processed audio shape: torch.Size([3809559])\n",
      "Successfully loaded audio at index 1912, shape: torch.Size([160000]), label: 56\n",
      "Successfully loaded audio: audiosets/ontology/Aircraft engine_9.wav, shape: (3801200,), dtype: float32\n",
      "Processed audio shape: torch.Size([3801200])\n",
      "Successfully loaded audio at index 1958, shape: torch.Size([160000]), label: 455\n",
      "Successfully loaded audio: audiosets/ontology/Dial tone_1.wav, shape: (3148441,), dtype: float32\n",
      "Processed audio shape: torch.Size([3148441])\n",
      "Successfully loaded audio at index 302, shape: torch.Size([160000]), label: 508\n",
      "Successfully loaded audio: audiosets/ontology/Crowd_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2072, shape: torch.Size([160000]), label: 366\n",
      "Successfully loaded audio: audiosets/ontology/Motorboat, speedboat_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2239, shape: torch.Size([160000]), label: 313\n",
      "Successfully loaded audio: audiosets/ontology/Electric toothbrush_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 656, shape: torch.Size([160000]), label: 262\n",
      "Successfully loaded audio: audiosets/ontology/Drum machine_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2417, shape: torch.Size([160000]), label: 93\n",
      "Successfully loaded audio: audiosets/ontology/Cheering_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2300, shape: torch.Size([160000]), label: 298\n",
      "Successfully loaded audio: audiosets/ontology/Fixed-wing aircraft, airplane_4.wav, shape: (4699162,), dtype: float32\n",
      "Processed audio shape: torch.Size([4699162])\n",
      "Successfully loaded audio at index 700, shape: torch.Size([160000]), label: 150\n",
      "Successfully loaded audio: audiosets/ontology/Steelpan_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 983, shape: torch.Size([160000]), label: 299\n",
      "Successfully loaded audio: audiosets/ontology/Boat, Water vehicle_1.wav, shape: (8294540,), dtype: float32\n",
      "Processed audio shape: torch.Size([8294540])\n",
      "Successfully loaded audio at index 1781, shape: torch.Size([160000]), label: 293\n",
      "Successfully loaded audio: audiosets/ontology/Burping, eructation_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 466, shape: torch.Size([160000]), label: 173\n",
      "Successfully loaded audio: audiosets/ontology/Rail transport_2.wav, shape: (8882098,), dtype: float32\n",
      "Processed audio shape: torch.Size([8882098])\n",
      "Successfully loaded audio at index 1972, shape: torch.Size([160000]), label: 16\n",
      "Successfully loaded audio: audiosets/ontology/Rodents, rats, mice_4.wav, shape: (8162836,), dtype: float32\n",
      "Processed audio shape: torch.Size([8162836])\n",
      "Successfully loaded audio at index 2292, shape: torch.Size([160000]), label: 267\n",
      "Successfully loaded audio: audiosets/ontology/Mandolin_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 309, shape: torch.Size([160000]), label: 367\n",
      "Successfully loaded audio: audiosets/ontology/Singing bowl_1.wav, shape: (12583916,), dtype: float32\n",
      "Processed audio shape: torch.Size([12583916])\n",
      "Successfully loaded audio at index 359, shape: torch.Size([160000]), label: 205\n",
      "Successfully loaded audio: audiosets/ontology/Liquid_3.wav, shape: (6183939,), dtype: float32\n",
      "Processed audio shape: torch.Size([6183939])\n",
      "Successfully loaded audio at index 658, shape: torch.Size([160000]), label: 328\n",
      "Successfully loaded audio: audiosets/ontology/Police car (siren)_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1470, shape: torch.Size([160000]), label: 128\n",
      "Successfully loaded audio: audiosets/ontology/Waves, surf_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 461, shape: torch.Size([160000]), label: 105\n",
      "Successfully loaded audio: audiosets/ontology/Sink (filling or washing)_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 5, shape: torch.Size([160000]), label: 394\n",
      "Successfully loaded audio: audiosets/ontology/Train horn_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2124, shape: torch.Size([160000]), label: 31\n",
      "Successfully loaded audio: audiosets/ontology/Yak_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1513, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Sewing machine_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 86, shape: torch.Size([160000]), label: 496\n",
      "Successfully loaded audio: audiosets/ontology/Fire engine, fire truck (siren)_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 161, shape: torch.Size([160000]), label: 204\n",
      "Successfully loaded audio: audiosets/ontology/Wind noise (microphone)_5.wav, shape: (1042112,), dtype: float32\n",
      "Processed audio shape: torch.Size([1042112])\n",
      "Successfully loaded audio at index 1943, shape: torch.Size([160000]), label: 49\n",
      "Successfully loaded audio: audiosets/ontology/Cello_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1398, shape: torch.Size([160000]), label: 397\n",
      "Successfully loaded audio: audiosets/ontology/Cowbell_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 434, shape: torch.Size([160000]), label: 107\n",
      "Successfully loaded audio: audiosets/ontology/Bird vocalization, bird call, bird song_0.wav, shape: (2028310,), dtype: float32\n",
      "Processed audio shape: torch.Size([2028310])\n",
      "Successfully loaded audio at index 1923, shape: torch.Size([160000]), label: 61\n",
      "Successfully loaded audio: audiosets/ontology/Child singing_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 824, shape: torch.Size([160000]), label: 423\n",
      "Successfully loaded audio: audiosets/ontology/Steel guitar, slide guitar_2.wav, shape: (7929515,), dtype: float32\n",
      "Processed audio shape: torch.Size([7929515])\n",
      "Successfully loaded audio at index 1222, shape: torch.Size([160000]), label: 401\n",
      "Successfully loaded audio: audiosets/ontology/Narration, monologue_8.wav, shape: (1769361,), dtype: float32\n",
      "Processed audio shape: torch.Size([1769361])\n",
      "Successfully loaded audio at index 1696, shape: torch.Size([160000]), label: 24\n",
      "Successfully loaded audio: audiosets/ontology/Narration, monologue_5.wav, shape: (6007096,), dtype: float32\n",
      "Processed audio shape: torch.Size([6007096])\n",
      "Successfully loaded audio at index 1616, shape: torch.Size([160000]), label: 24\n",
      "Successfully loaded audio: audiosets/ontology/Skateboard_2.wav, shape: (1957536,), dtype: float32\n",
      "Processed audio shape: torch.Size([1957536])\n",
      "Successfully loaded audio at index 1242, shape: torch.Size([160000]), label: 8\n",
      "Successfully loaded audio: audiosets/ontology/Bird_5.wav, shape: (1916483,), dtype: float32\n",
      "Processed audio shape: torch.Size([1916483])\n",
      "Successfully loaded audio at index 2410, shape: torch.Size([160000]), label: 97\n",
      "Successfully loaded audio: audiosets/ontology/Motorboat, speedboat_5.wav, shape: (158268,), dtype: float32\n",
      "Processed audio shape: torch.Size([158268])\n",
      "Successfully loaded audio at index 1336, shape: torch.Size([160000]), label: 313\n",
      "Successfully loaded audio: audiosets/ontology/Raindrop_2.wav, shape: (250404,), dtype: float32\n",
      "Processed audio shape: torch.Size([250404])\n",
      "Successfully loaded audio at index 2423, shape: torch.Size([160000]), label: 31\n",
      "Successfully loaded audio: audiosets/ontology/Bass drum_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1343, shape: torch.Size([160000]), label: 93\n",
      "Successfully loaded audio: audiosets/ontology/Wind chime_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1345, shape: torch.Size([160000]), label: 457\n",
      "Successfully loaded audio: audiosets/ontology/Fire_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1962, shape: torch.Size([160000]), label: 204\n",
      "Successfully loaded audio: audiosets/ontology/Police car (siren)_1.wav, shape: (1026390,), dtype: float32\n",
      "Processed audio shape: torch.Size([1026390])\n",
      "Successfully loaded audio at index 2310, shape: torch.Size([160000]), label: 29\n",
      "Successfully loaded audio: audiosets/ontology/Bee, wasp, etc._4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 800, shape: torch.Size([160000]), label: 456\n",
      "Successfully loaded audio: audiosets/ontology/Gong_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1265, shape: torch.Size([160000]), label: 203\n",
      "Successfully loaded audio: audiosets/ontology/Sewing machine_1.wav, shape: (3054260,), dtype: float32\n",
      "Processed audio shape: torch.Size([3054260])\n",
      "Successfully loaded audio at index 2256, shape: torch.Size([160000]), label: 496\n",
      "Successfully loaded audio: audiosets/ontology/Explosion_7.wav, shape: (4012966,), dtype: float32\n",
      "Processed audio shape: torch.Size([4012966])\n",
      "Successfully loaded audio at index 699, shape: torch.Size([160000]), label: 64\n",
      "Successfully loaded audio: audiosets/ontology/Accelerating, revving, vroom_5.wav, shape: (684711,), dtype: float32\n",
      "Processed audio shape: torch.Size([684711])\n",
      "Successfully loaded audio at index 1004, shape: torch.Size([160000]), label: 403\n",
      "Successfully loaded audio: audiosets/ontology/Velcro, hook and loop fastener_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 538, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Didgeridoo_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1830, shape: torch.Size([160000]), label: 136\n",
      "Successfully loaded audio: audiosets/ontology/Breathing_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1946, shape: torch.Size([160000]), label: 164\n",
      "Successfully loaded audio: audiosets/ontology/Coo_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 237, shape: torch.Size([160000]), label: 155\n",
      "Successfully loaded audio: audiosets/ontology/Fly, housefly_0.wav, shape: (23138304,), dtype: float32\n",
      "Processed audio shape: torch.Size([23138304])\n",
      "Successfully loaded audio at index 819, shape: torch.Size([160000]), label: 55\n",
      "Successfully loaded audio: audiosets/ontology/Canidae, dogs, wolves_0.wav, shape: (1985585,), dtype: float32\n",
      "Processed audio shape: torch.Size([1985585])\n",
      "Successfully loaded audio at index 70, shape: torch.Size([160000]), label: 19\n",
      "Successfully loaded audio: audiosets/ontology/Chatter_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1930, shape: torch.Size([160000]), label: 201\n",
      "Successfully loaded audio: audiosets/ontology/Percussion_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1068, shape: torch.Size([160000]), label: 292\n",
      "Successfully loaded audio: audiosets/ontology/Hair dryer_0.wav, shape: (11024092,), dtype: float32\n",
      "Processed audio shape: torch.Size([11024092])\n",
      "Successfully loaded audio at index 1340, shape: torch.Size([160000]), label: 137\n",
      "Successfully loaded audio: audiosets/ontology/Aircraft engine_1.wav, shape: (3352776,), dtype: float32\n",
      "Processed audio shape: torch.Size([3352776])\n",
      "Successfully loaded audio at index 389, shape: torch.Size([160000]), label: 471\n",
      "Successfully loaded audio: audiosets/ontology/Crack_2.wav, shape: (13814945,), dtype: float32\n",
      "Processed audio shape: torch.Size([13814945])\n",
      "Successfully loaded audio at index 1423, shape: torch.Size([160000]), label: 404\n",
      "Successfully loaded audio: audiosets/ontology/Harmonica_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1020, shape: torch.Size([160000]), label: 402\n",
      "Successfully loaded audio: audiosets/ontology/Bird flight, flapping wings_4.wav, shape: (6224434,), dtype: float32\n",
      "Processed audio shape: torch.Size([6224434])\n",
      "Successfully loaded audio at index 172, shape: torch.Size([160000]), label: 60\n",
      "Successfully loaded audio: audiosets/ontology/Male speech, man speaking_4.wav, shape: (3897609,), dtype: float32\n",
      "Processed audio shape: torch.Size([3897609])\n",
      "Successfully loaded audio at index 1793, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Fowl_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1126, shape: torch.Size([160000]), label: 431\n",
      "Successfully loaded audio: audiosets/ontology/Ambulance (siren)_0.wav, shape: (4804859,), dtype: float32\n",
      "Processed audio shape: torch.Size([4804859])\n",
      "Successfully loaded audio at index 2406, shape: torch.Size([160000]), label: 128\n",
      "Successfully loaded audio: audiosets/ontology/Roar_0.wav, shape: (3931046,), dtype: float32\n",
      "Processed audio shape: torch.Size([3931046])\n",
      "Successfully loaded audio at index 1929, shape: torch.Size([160000]), label: 48\n",
      "Successfully loaded audio: audiosets/ontology/Tools_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1983, shape: torch.Size([160000]), label: 442\n",
      "Successfully loaded audio: audiosets/ontology/Hubbub, speech noise, speech babble_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 868, shape: torch.Size([160000]), label: 174\n",
      "Successfully loaded audio: audiosets/ontology/Train horn_5.wav, shape: (734308,), dtype: float32\n",
      "Processed audio shape: torch.Size([734308])\n",
      "Successfully loaded audio at index 2321, shape: torch.Size([160000]), label: 436\n",
      "Successfully loaded audio: audiosets/ontology/Bow-wow_1.wav, shape: (1247191,), dtype: float32\n",
      "Processed audio shape: torch.Size([1247191])\n",
      "Successfully loaded audio at index 588, shape: torch.Size([160000]), label: 363\n",
      "Successfully loaded audio: audiosets/ontology/Sailboat, sailing ship_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 927, shape: torch.Size([160000]), label: 232\n",
      "Successfully loaded audio: audiosets/ontology/Water tap, faucet_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1357, shape: torch.Size([160000]), label: 187\n",
      "Successfully loaded audio: audiosets/ontology/Rowboat, canoe, kayak_2.wav, shape: (150466,), dtype: float32\n",
      "Processed audio shape: torch.Size([150466])\n",
      "Successfully loaded audio at index 1723, shape: torch.Size([160000]), label: 152\n",
      "Successfully loaded audio: audiosets/ontology/Train_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2142, shape: torch.Size([160000]), label: 31\n",
      "Successfully loaded audio: audiosets/ontology/Acoustic guitar_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2289, shape: torch.Size([160000]), label: 472\n",
      "Successfully loaded audio: audiosets/ontology/Wheeze_2.wav, shape: (436224,), dtype: float32\n",
      "Processed audio shape: torch.Size([436224])\n",
      "Successfully loaded audio at index 476, shape: torch.Size([160000]), label: 211\n",
      "Successfully loaded audio: audiosets/ontology/Bird vocalization, bird call, bird song_1.wav, shape: (11449853,), dtype: float32\n",
      "Processed audio shape: torch.Size([11449853])\n",
      "Successfully loaded audio at index 294, shape: torch.Size([160000]), label: 61\n",
      "Successfully loaded audio: audiosets/ontology/Busy signal_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 957, shape: torch.Size([160000]), label: 453\n",
      "Successfully loaded audio: audiosets/ontology/Horse_2.wav, shape: (6535768,), dtype: float32\n",
      "Processed audio shape: torch.Size([6535768])\n",
      "Successfully loaded audio at index 563, shape: torch.Size([160000]), label: 410\n",
      "Successfully loaded audio: audiosets/ontology/Child speech, kid speaking_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2231, shape: torch.Size([160000]), label: 430\n",
      "Successfully loaded audio: audiosets/ontology/Firecracker_4.wav, shape: (2166515,), dtype: float32\n",
      "Processed audio shape: torch.Size([2166515])\n",
      "Successfully loaded audio at index 919, shape: torch.Size([160000]), label: 412\n",
      "Successfully loaded audio: audiosets/ontology/Thunderstorm_3.wav, shape: (347743,), dtype: float32\n",
      "Processed audio shape: torch.Size([347743])\n",
      "Successfully loaded audio at index 2179, shape: torch.Size([160000]), label: 432\n",
      "Successfully loaded audio: audiosets/ontology/Fire alarm_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 974, shape: torch.Size([160000]), label: 384\n",
      "Successfully loaded audio: audiosets/ontology/Jet engine_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1699, shape: torch.Size([160000]), label: 455\n",
      "Successfully loaded audio: audiosets/ontology/Idling_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 866, shape: torch.Size([160000]), label: 517\n",
      "Successfully loaded audio: audiosets/ontology/Alarm_5.wav, shape: (6021585,), dtype: float32\n",
      "Processed audio shape: torch.Size([6021585])\n",
      "Successfully loaded audio at index 1280, shape: torch.Size([160000]), label: 384\n",
      "Successfully loaded audio: audiosets/ontology/Sewing machine_5.wav, shape: (8979250,), dtype: float32\n",
      "Processed audio shape: torch.Size([8979250])\n",
      "Successfully loaded audio at index 1284, shape: torch.Size([160000]), label: 496\n",
      "Successfully loaded audio: audiosets/ontology/Bass drum_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 136, shape: torch.Size([160000]), label: 247\n",
      "Successfully loaded audio: audiosets/ontology/Electric toothbrush_2.wav, shape: (147865,), dtype: float32\n",
      "Processed audio shape: torch.Size([147865])\n",
      "Successfully loaded audio at index 1282, shape: torch.Size([160000]), label: 262\n",
      "Successfully loaded audio: audiosets/ontology/Honk_1.wav, shape: (149723,), dtype: float32\n",
      "Processed audio shape: torch.Size([149723])\n",
      "Successfully loaded audio at index 1283, shape: torch.Size([160000]), label: 230\n",
      "Successfully loaded audio: audiosets/ontology/Steam_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 851, shape: torch.Size([160000]), label: 288\n",
      "Successfully loaded audio: audiosets/ontology/Hubbub, speech noise, speech babble_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 877, shape: torch.Size([160000]), label: 174\n",
      "Successfully loaded audio: audiosets/ontology/Zipper (clothing)_5.wav, shape: (1761373,), dtype: float32\n",
      "Processed audio shape: torch.Size([1761373])\n",
      "Successfully loaded audio at index 2418, shape: torch.Size([160000]), label: 185\n",
      "Successfully loaded audio: audiosets/ontology/Purr_0.wav, shape: (13986958,), dtype: float32\n",
      "Processed audio shape: torch.Size([13986958])\n",
      "Successfully loaded audio at index 2109, shape: torch.Size([160000]), label: 212\n",
      "Successfully loaded audio: audiosets/ontology/Pizzicato_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1229, shape: torch.Size([160000]), label: 94\n",
      "Successfully loaded audio: audiosets/ontology/Drum_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1982, shape: torch.Size([160000]), label: 93\n",
      "Successfully loaded audio: audiosets/ontology/Hi-hat_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1656, shape: torch.Size([160000]), label: 316\n",
      "Successfully loaded audio: audiosets/ontology/Trombone_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 929, shape: torch.Size([160000]), label: 242\n",
      "Successfully loaded audio: audiosets/ontology/Hiccup_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 12, shape: torch.Size([160000]), label: 414\n",
      "Successfully loaded audio: audiosets/ontology/Rimshot_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1589, shape: torch.Size([160000]), label: 279\n",
      "Successfully loaded audio: audiosets/ontology/Steelpan_5.wav, shape: (10207864,), dtype: float32\n",
      "Processed audio shape: torch.Size([10207864])\n",
      "Successfully loaded audio at index 1715, shape: torch.Size([160000]), label: 299\n",
      "Successfully loaded audio: audiosets/ontology/Fire alarm_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1605, shape: torch.Size([160000]), label: 204\n",
      "Successfully loaded audio: audiosets/ontology/Aircraft engine_1.wav, shape: (3352776,), dtype: float32\n",
      "Processed audio shape: torch.Size([3352776])\n",
      "Successfully loaded audio at index 1703, shape: torch.Size([160000]), label: 460\n",
      "Successfully loaded audio: audiosets/ontology/Buzzer_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1154, shape: torch.Size([160000]), label: 325\n",
      "Successfully loaded audio: audiosets/ontology/Motor vehicle (road)_4.wav, shape: (4071109,), dtype: float32\n",
      "Processed audio shape: torch.Size([4071109])\n",
      "Successfully loaded audio at index 1913, shape: torch.Size([160000]), label: 25\n",
      "Successfully loaded audio: audiosets/ontology/Thunderstorm_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1762, shape: torch.Size([160000]), label: 432\n",
      "Successfully loaded audio: audiosets/ontology/Rattle_0.wav, shape: (218268,), dtype: float32\n",
      "Processed audio shape: torch.Size([218268])\n",
      "Successfully loaded audio at index 639, shape: torch.Size([160000]), label: 219\n",
      "Successfully loaded audio: audiosets/ontology/Booing_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 685, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Motorcycle_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2166, shape: torch.Size([160000]), label: 473\n",
      "Successfully loaded audio: audiosets/ontology/Bagpipes_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2136, shape: torch.Size([160000]), label: 67\n",
      "Successfully loaded audio: audiosets/ontology/Engine_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1872, shape: torch.Size([160000]), label: 455\n",
      "Successfully loaded audio: audiosets/ontology/Microwave oven_1.wav, shape: (2968064,), dtype: float32\n",
      "Processed audio shape: torch.Size([2968064])\n",
      "Successfully loaded audio at index 25, shape: torch.Size([160000]), label: 283\n",
      "Successfully loaded audio: audiosets/ontology/Clapping_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 2222, shape: torch.Size([160000]), label: 173\n",
      "Successfully loaded audio: audiosets/ontology/Brass instrument_7.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1543, shape: torch.Size([160000]), label: 308\n",
      "Successfully loaded audio: audiosets/ontology/Emergency vehicle_3.wav, shape: (453291,), dtype: float32\n",
      "Processed audio shape: torch.Size([453291])\n",
      "Successfully loaded audio at index 1555, shape: torch.Size([160000]), label: 253\n",
      "Successfully loaded audio: audiosets/ontology/Wind chime_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 2261, shape: torch.Size([160000]), label: 457\n",
      "Successfully loaded audio: audiosets/ontology/Medium engine (mid frequency)_0.wav, shape: (528672,), dtype: float32\n",
      "Processed audio shape: torch.Size([528672])\n",
      "Successfully loaded audio at index 157, shape: torch.Size([160000]), label: 240\n",
      "Successfully loaded audio: audiosets/ontology/Meow_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 882, shape: torch.Size([160000]), label: 28\n",
      "Successfully loaded audio: audiosets/ontology/Harp_1.wav, shape: (2354875,), dtype: float32\n",
      "Processed audio shape: torch.Size([2354875])\n",
      "Successfully loaded audio at index 500, shape: torch.Size([160000]), label: 420\n",
      "Successfully loaded audio: audiosets/ontology/Sliding door_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2034, shape: torch.Size([160000]), label: 248\n",
      "Successfully loaded audio: audiosets/ontology/Keyboard (musical)_1.wav, shape: (3773708,), dtype: float32\n",
      "Processed audio shape: torch.Size([3773708])\n",
      "Successfully loaded audio at index 211, shape: torch.Size([160000]), label: 354\n",
      "Successfully loaded audio: audiosets/ontology/Domestic animals, pets_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 192, shape: torch.Size([160000]), label: 429\n",
      "Successfully loaded audio: audiosets/ontology/Snare drum_4.wav, shape: (2483235,), dtype: float32\n",
      "Processed audio shape: torch.Size([2483235])\n",
      "Successfully loaded audio at index 568, shape: torch.Size([160000]), label: 336\n",
      "Successfully loaded audio: audiosets/ontology/Singing_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 78, shape: torch.Size([160000]), label: 423\n",
      "Successfully loaded audio: audiosets/ontology/Mechanical fan_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 935, shape: torch.Size([160000]), label: 167\n",
      "Successfully loaded audio: audiosets/ontology/Wind noise (microphone)_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 787, shape: torch.Size([160000]), label: 450\n",
      "Successfully loaded audio: audiosets/ontology/Plucked string instrument_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 870, shape: torch.Size([160000]), label: 38\n",
      "Successfully loaded audio: audiosets/ontology/Police car (siren)_2.wav, shape: (7164587,), dtype: float32\n",
      "Processed audio shape: torch.Size([7164587])\n",
      "Successfully loaded audio at index 1496, shape: torch.Size([160000]), label: 330\n",
      "Successfully loaded audio: audiosets/ontology/Piano_0.wav, shape: (1948248,), dtype: float32\n",
      "Processed audio shape: torch.Size([1948248])\n",
      "Successfully loaded audio at index 209, shape: torch.Size([160000]), label: 386\n",
      "Successfully loaded audio: audiosets/ontology/Baby laughter_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 108, shape: torch.Size([160000]), label: 263\n",
      "Successfully loaded audio: audiosets/ontology/Coin (dropping)_3.wav, shape: (4730880,), dtype: float32\n",
      "Processed audio shape: torch.Size([4730880])\n",
      "Successfully loaded audio at index 1945, shape: torch.Size([160000]), label: 176\n",
      "Successfully loaded audio: audiosets/ontology/Sampler_3.wav, shape: (3540765,), dtype: float32\n",
      "Processed audio shape: torch.Size([3540765])\n",
      "Successfully loaded audio at index 2052, shape: torch.Size([160000]), label: 505\n",
      "Successfully loaded audio: audiosets/ontology/Applause_3.wav, shape: (699386,), dtype: float32\n",
      "Processed audio shape: torch.Size([699386])\n",
      "Successfully loaded audio at index 1170, shape: torch.Size([160000]), label: 259\n",
      "Successfully loaded audio: audiosets/ontology/Squish_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1776, shape: torch.Size([160000]), label: 177\n",
      "Successfully loaded audio: audiosets/ontology/Whoop_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1851, shape: torch.Size([160000]), label: 11\n",
      "Successfully loaded audio: audiosets/ontology/Smoke detector, smoke alarm_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 789, shape: torch.Size([160000]), label: 384\n",
      "Successfully loaded audio: audiosets/ontology/Bell_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 2180, shape: torch.Size([160000]), label: 107\n",
      "Successfully loaded audio: audiosets/ontology/Ice cream truck, ice cream van_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 512, shape: torch.Size([160000]), label: 396\n",
      "Successfully loaded audio: audiosets/ontology/Air horn, truck horn_0.wav, shape: (1319080,), dtype: float32\n",
      "Processed audio shape: torch.Size([1319080])\n",
      "Successfully loaded audio at index 2243, shape: torch.Size([160000]), label: 132\n",
      "Successfully loaded audio: audiosets/ontology/Cattle, bovinae_7.wav, shape: (3383612,), dtype: float32\n",
      "Processed audio shape: torch.Size([3383612])\n",
      "Successfully loaded audio at index 1646, shape: torch.Size([160000]), label: 306\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 192\u001b[0m\n\u001b[0;32m    190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_values, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m    191\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m--> 192\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backward pass to compute gradients\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# Clear GPU cache to prevent memory overflow\u001b[39;00m\n\u001b[0;32m    195\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "import torch\n",
    "import librosa\n",
    "from collections import namedtuple\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Set to block CUDA errors\n",
    "\n",
    "# Load the class map CSV (mapping from mid to index)\n",
    "class_map_df = pd.read_csv('yamnet_class_map.csv')\n",
    "class_map = pd.read_csv('yamnet_class_map.csv').set_index('display_name').to_dict()['mid']\n",
    "\n",
    "# Create a mapping from mid (string) to index (integer)\n",
    "mid_to_index = {mid: idx for idx, mid in enumerate(set(class_map.values()))}\n",
    "\n",
    "# Initialize the model and processor\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained('facebook/wav2vec2-base-960h', num_labels=521)\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Moved model to GPU if available\")\n",
    "\n",
    "# Define a namedtuple for dataset items\n",
    "AudioSample = namedtuple(\"AudioSample\", [\"input_values\", \"labels\"])\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_directory, ontology_file, mid_to_index):\n",
    "        with open(ontology_file, 'r') as f:\n",
    "            self.ontology_data = json.load(f)\n",
    "\n",
    "        self.mid_to_index = mid_to_index\n",
    "        self.audio_directory = audio_directory\n",
    "        self.audio_files = glob.glob(os.path.join(self.audio_directory, '**', '*.wav'), recursive=True)\n",
    "        \n",
    "        # Populate the dataset by calling prepare_data\n",
    "        self.data = self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        data = []\n",
    "        for category in self.ontology_data:\n",
    "            if \"positive_examples\" in category:\n",
    "                category_name = category[\"name\"]\n",
    "                mid = category[\"id\"]  # Get the mid for the current category\n",
    "\n",
    "                # Use the mid to get the index from the mid_to_index\n",
    "                if mid in self.mid_to_index:\n",
    "                    label = self.mid_to_index[mid]  # Get the integer index as the label\n",
    "                else:\n",
    "                    label = -1  # Default to -1 if not found\n",
    "\n",
    "                for audio_file in self.audio_files:\n",
    "                    if category_name.lower() in audio_file.lower():\n",
    "                        audio_file = audio_file.replace(\"\\\\\", \"/\")\n",
    "                        data.append({\"audio\": audio_file, \"label\": label})\n",
    "        return data\n",
    "    \n",
    "    def load_audio(self, file_path):\n",
    "        \"\"\"Load and preprocess audio using Wav2Vec2Processor.\"\"\"\n",
    "        try:\n",
    "            if not os.path.isfile(file_path):\n",
    "                raise FileNotFoundError(f\"WAV file not found: {file_path}\")\n",
    "\n",
    "            # Load audio using librosa and resample to 16kHz\n",
    "            audio_data, sr = librosa.load(file_path, sr=16000)\n",
    "            print(f\"Successfully loaded audio: {file_path}, shape: {audio_data.shape}, dtype: {audio_data.dtype}\")\n",
    "\n",
    "            # Preprocess using Wav2Vec2Processor\n",
    "            inputs = processor(audio_data, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "            processed_audio = inputs.input_values.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "            print(f\"Processed audio shape: {processed_audio.shape}\")\n",
    "\n",
    "            # Return processed audio\n",
    "            return processed_audio\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get one item (audio, label) for the dataset.\"\"\"\n",
    "        sample = self.data[idx]\n",
    "        audio_data = self.load_audio(sample[\"audio\"])\n",
    "        label = sample[\"label\"]\n",
    "\n",
    "        # Ensure audio_data is valid\n",
    "        if audio_data is None:\n",
    "            print(f\"Error loading audio at index {idx}, returning dummy data.\")\n",
    "            return {\"input_values\": torch.zeros(1), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "        # Trim or pad audio to max_length\n",
    "        max_length = 160000  # Set a max_length for padding/truncating\n",
    "        if audio_data.shape[0] < max_length:\n",
    "            padding = torch.zeros(max_length - audio_data.shape[0])\n",
    "            audio_data = torch.cat([audio_data, padding])\n",
    "        else:\n",
    "            audio_data = audio_data[:max_length]\n",
    "\n",
    "        print(f\"Successfully loaded audio at index {idx}, shape: {audio_data.shape}, label: {label}\")\n",
    "        \n",
    "        return {\"input_values\": audio_data.clone().detach(), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Initialize the dataset and dataloaders\n",
    "audio_directory = r\"audiosets/ontology\"\n",
    "ontology_file = 'ontology.json'\n",
    "\n",
    "# Initialize dataset and prepare data\n",
    "dataset = AudioDataset(audio_directory, ontology_file, mid_to_index)\n",
    "\n",
    "# Now split the dataset into train and test sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(dataset.data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize train and test datasets using the split data\n",
    "train_dataset = AudioDataset(audio_directory, ontology_file, mid_to_index)\n",
    "test_dataset = AudioDataset(audio_directory, ontology_file, mid_to_index)\n",
    "\n",
    "# Assign the split data to the datasets\n",
    "train_dataset.data = train_data\n",
    "test_dataset.data = test_data\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    save_steps=10,\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Initialize train and test dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Pass the datasets without specifying the dataloaders in Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Training loop with handling for -1 labels\n",
    "for epoch in range(int(training_args.num_train_epochs)):\n",
    "    print(f\"Training epoch {epoch + 1}\")\n",
    "    model.train()  # Set model to training mode\n",
    "    for batch in train_dataloader:\n",
    "        input_values = batch['input_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Skip batches where label is -1\n",
    "        valid_indices = labels != -1\n",
    "        if valid_indices.sum() == 0:\n",
    "            continue  # Skip this batch if all labels are -1\n",
    "\n",
    "        # Only select valid indices\n",
    "        input_values = input_values[valid_indices]\n",
    "        labels = labels[valid_indices]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()  # Backward pass to compute gradients\n",
    "\n",
    "        # Clear GPU cache to prevent memory overflow\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Run evaluation after each epoch (optional)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Skip batches where label is -1\n",
    "            valid_indices = labels != -1\n",
    "            if valid_indices.sum() == 0:\n",
    "                continue  # Skip this batch if all labels are -1\n",
    "\n",
    "            # Only select valid indices\n",
    "            input_values = input_values[valid_indices]\n",
    "            labels = labels[valid_indices]\n",
    "\n",
    "            outputs = model(input_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            print(f\"Evaluation loss: {loss.item()}\")\n",
    "\n",
    "    torch.cuda.empty_cache()  # Clear GPU cache after each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved model to GPU if available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Evaluating:   0%|          | 1/610 [00:04<46:03,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Harpsichord_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 0, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Dishes, pots, and pans_6.wav, shape: (4317054,), dtype: float32\n",
      "Processed audio shape: torch.Size([4317054])\n",
      "Successfully loaded audio at index 1, shape: torch.Size([160000]), label: 237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 3/610 [00:05<13:16,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Train wheels squealing_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 2, shape: torch.Size([160000]), label: 431\n",
      "Successfully loaded audio: audiosets/ontology/Chewing, mastication_7.wav, shape: (7670573,), dtype: float32\n",
      "Processed audio shape: torch.Size([7670573])\n",
      "Successfully loaded audio at index 3, shape: torch.Size([160000]), label: 293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 5/610 [00:06<08:09,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Tick-tock_3.wav, shape: (1850710,), dtype: float32\n",
      "Processed audio shape: torch.Size([1850710])\n",
      "Successfully loaded audio at index 4, shape: torch.Size([160000]), label: 478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 6/610 [00:07<08:40,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Mallet percussion_0.wav, shape: (9949287,), dtype: float32\n",
      "Processed audio shape: torch.Size([9949287])\n",
      "Successfully loaded audio at index 5, shape: torch.Size([160000]), label: 97\n",
      "Successfully loaded audio: audiosets/ontology/Electric shaver, electric razor_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 6, shape: torch.Size([160000]), label: 385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|          | 7/610 [00:07<06:18,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Steel guitar, slide guitar_2.wav, shape: (7929515,), dtype: float32\n",
      "Processed audio shape: torch.Size([7929515])\n",
      "Successfully loaded audio at index 7, shape: torch.Size([160000]), label: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   1%|â–         | 9/610 [00:08<05:34,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Sailboat, sailing ship_2.wav, shape: (1399328,), dtype: float32\n",
      "Processed audio shape: torch.Size([1399328])\n",
      "Successfully loaded audio at index 8, shape: torch.Size([160000]), label: 193\n",
      "Successfully loaded audio: audiosets/ontology/Power tool_8.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 9, shape: torch.Size([160000]), label: 504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   2%|â–         | 11/610 [00:09<05:36,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Keyboard (musical)_0.wav, shape: (9789440,), dtype: float32\n",
      "Processed audio shape: torch.Size([9789440])\n",
      "Successfully loaded audio at index 10, shape: torch.Size([160000]), label: 332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   2%|â–         | 12/610 [00:09<04:28,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Whimper_6.wav, shape: (963721,), dtype: float32\n",
      "Processed audio shape: torch.Size([963721])\n",
      "Successfully loaded audio at index 11, shape: torch.Size([160000]), label: 501\n",
      "Successfully loaded audio: audiosets/ontology/Sliding door_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 12, shape: torch.Size([160000]), label: 214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   2%|â–         | 14/610 [00:10<04:21,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Train_3.wav, shape: (6224249,), dtype: float32\n",
      "Processed audio shape: torch.Size([6224249])\n",
      "Successfully loaded audio at index 13, shape: torch.Size([160000]), label: 431\n",
      "Successfully loaded audio: audiosets/ontology/Didgeridoo_8.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 14, shape: torch.Size([160000]), label: 182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   3%|â–Ž         | 16/610 [00:11<03:35,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Zipper (clothing)_0.wav, shape: (4112719,), dtype: float32\n",
      "Processed audio shape: torch.Size([4112719])\n",
      "Successfully loaded audio at index 15, shape: torch.Size([160000]), label: 375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   3%|â–Ž         | 17/610 [00:11<03:25,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Bicycle bell_2.wav, shape: (1946018,), dtype: float32\n",
      "Processed audio shape: torch.Size([1946018])\n",
      "Successfully loaded audio at index 16, shape: torch.Size([160000]), label: 439\n",
      "Successfully loaded audio: audiosets/ontology/Water tap, faucet_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 17, shape: torch.Size([160000]), label: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   3%|â–Ž         | 19/610 [00:11<02:23,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Power tool_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 18, shape: torch.Size([160000]), label: 504\n",
      "Successfully loaded audio: audiosets/ontology/Coin (dropping)_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 19, shape: torch.Size([160000]), label: 249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   3%|â–Ž         | 20/610 [00:11<02:05,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Hair dryer_3.wav, shape: (14354205,), dtype: float32\n",
      "Processed audio shape: torch.Size([14354205])\n",
      "Successfully loaded audio at index 20, shape: torch.Size([160000]), label: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   3%|â–Ž         | 21/610 [00:13<04:30,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Truck_0.wav, shape: (7823453,), dtype: float32\n",
      "Processed audio shape: torch.Size([7823453])\n",
      "Successfully loaded audio at index 21, shape: torch.Size([160000]), label: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   4%|â–         | 23/610 [00:13<04:21,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Wood block_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 22, shape: torch.Size([160000]), label: 433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   4%|â–         | 24/610 [00:14<03:47,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Tick-tock_3.wav, shape: (1850710,), dtype: float32\n",
      "Processed audio shape: torch.Size([1850710])\n",
      "Successfully loaded audio at index 23, shape: torch.Size([160000]), label: 324\n",
      "Successfully loaded audio: audiosets/ontology/Fly, housefly_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 24, shape: torch.Size([160000]), label: 428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   4%|â–         | 25/610 [00:14<03:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Steam_4.wav, shape: (17491871,), dtype: float32\n",
      "Processed audio shape: torch.Size([17491871])\n",
      "Successfully loaded audio at index 25, shape: torch.Size([160000]), label: 373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   4%|â–         | 27/610 [00:16<05:47,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Fowl_1.wav, shape: (1422176,), dtype: float32\n",
      "Processed audio shape: torch.Size([1422176])\n",
      "Successfully loaded audio at index 26, shape: torch.Size([160000]), label: 382\n",
      "Successfully loaded audio: audiosets/ontology/Truck_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 27, shape: torch.Size([160000]), label: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   5%|â–         | 29/610 [00:16<03:30,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Meow_4.wav, shape: (189327,), dtype: float32\n",
      "Processed audio shape: torch.Size([189327])\n",
      "Successfully loaded audio at index 28, shape: torch.Size([160000]), label: 483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   5%|â–         | 30/610 [00:16<03:30,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Lawn mower_4.wav, shape: (2838528,), dtype: float32\n",
      "Processed audio shape: torch.Size([2838528])\n",
      "Successfully loaded audio at index 29, shape: torch.Size([160000]), label: 252\n",
      "Successfully loaded audio: audiosets/ontology/Animal_5.wav, shape: (152323,), dtype: float32\n",
      "Processed audio shape: torch.Size([152323])\n",
      "Successfully loaded audio at index 30, shape: torch.Size([160000]), label: 358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   5%|â–Œ         | 32/610 [00:17<02:20,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Burst, pop_2.wav, shape: (156039,), dtype: float32\n",
      "Processed audio shape: torch.Size([156039])\n",
      "Successfully loaded audio at index 31, shape: torch.Size([160000]), label: 73\n",
      "Successfully loaded audio: audiosets/ontology/Brass instrument_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 32, shape: torch.Size([160000]), label: 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   6%|â–Œ         | 34/610 [00:17<02:48,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Drum_4.wav, shape: (3468319,), dtype: float32\n",
      "Processed audio shape: torch.Size([3468319])\n",
      "Successfully loaded audio at index 33, shape: torch.Size([160000]), label: 300\n",
      "Successfully loaded audio: audiosets/ontology/Child singing_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 34, shape: torch.Size([160000]), label: 181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   6%|â–Œ         | 36/610 [00:18<01:55,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Orchestra_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 35, shape: torch.Size([160000]), label: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   6%|â–Œ         | 37/610 [00:18<01:55,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Howl_0.wav, shape: (666506,), dtype: float32\n",
      "Processed audio shape: torch.Size([666506])\n",
      "Successfully loaded audio at index 36, shape: torch.Size([160000]), label: 191\n",
      "Successfully loaded audio: audiosets/ontology/Crowing, cock-a-doodle-doo_5.wav, shape: (150837,), dtype: float32\n",
      "Processed audio shape: torch.Size([150837])\n",
      "Successfully loaded audio at index 37, shape: torch.Size([160000]), label: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   6%|â–‹         | 39/610 [00:18<01:50,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Livestock, farm animals, working animals_4.wav, shape: (1133692,), dtype: float32\n",
      "Processed audio shape: torch.Size([1133692])\n",
      "Successfully loaded audio at index 38, shape: torch.Size([160000]), label: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   7%|â–‹         | 40/610 [00:18<01:54,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Bus_4.wav, shape: (969666,), dtype: float32\n",
      "Processed audio shape: torch.Size([969666])\n",
      "Successfully loaded audio at index 39, shape: torch.Size([160000]), label: 28\n",
      "Successfully loaded audio: audiosets/ontology/Motor vehicle (road)_8.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 40, shape: torch.Size([160000]), label: 198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   7%|â–‹         | 42/610 [00:19<01:32,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Howl_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 41, shape: torch.Size([160000]), label: 157\n",
      "Successfully loaded audio: audiosets/ontology/Fire engine, fire truck (siren)_1.wav, shape: (10044953,), dtype: float32\n",
      "Processed audio shape: torch.Size([10044953])\n",
      "Successfully loaded audio at index 42, shape: torch.Size([160000]), label: 343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   7%|â–‹         | 44/610 [00:20<03:11,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Alarm clock_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 43, shape: torch.Size([160000]), label: 4\n",
      "Successfully loaded audio: audiosets/ontology/Cash register_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 44, shape: torch.Size([160000]), label: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   8%|â–Š         | 46/610 [00:20<02:47,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Caterwaul_3.wav, shape: (2496239,), dtype: float32\n",
      "Processed audio shape: torch.Size([2496239])\n",
      "Successfully loaded audio at index 45, shape: torch.Size([160000]), label: 345\n",
      "Successfully loaded audio: audiosets/ontology/Battle cry_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 46, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Hubbub, speech noise, speech babble_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 47, shape: torch.Size([160000]), label: 495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   8%|â–Š         | 49/610 [00:21<01:37,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Engine starting_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 48, shape: torch.Size([160000]), label: 230\n",
      "Successfully loaded audio: audiosets/ontology/Ice cream truck, ice cream van_3.wav, shape: (156039,), dtype: float32\n",
      "Processed audio shape: torch.Size([156039])\n",
      "Successfully loaded audio at index 49, shape: torch.Size([160000]), label: 153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   8%|â–Š         | 51/610 [00:21<01:21,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Pour_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 50, shape: torch.Size([160000]), label: 0\n",
      "Successfully loaded audio: audiosets/ontology/Eruption_1.wav, shape: (9364701,), dtype: float32\n",
      "Processed audio shape: torch.Size([9364701])\n",
      "Successfully loaded audio at index 51, shape: torch.Size([160000]), label: 443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   9%|â–Š         | 53/610 [00:22<02:44,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Percussion_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 52, shape: torch.Size([160000]), label: 97\n",
      "Successfully loaded audio: audiosets/ontology/Child speech, kid speaking_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 53, shape: torch.Size([160000]), label: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   9%|â–‰         | 56/610 [00:22<01:55,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Growling_3.wav, shape: (1494437,), dtype: float32\n",
      "Processed audio shape: torch.Size([1494437])\n",
      "Successfully loaded audio at index 54, shape: torch.Size([160000]), label: 85\n",
      "Successfully loaded audio: audiosets/ontology/Acoustic guitar_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 55, shape: torch.Size([160000]), label: 186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   9%|â–‰         | 57/610 [00:23<03:31,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Mandolin_4.wav, shape: (9115412,), dtype: float32\n",
      "Processed audio shape: torch.Size([9115412])\n",
      "Successfully loaded audio at index 56, shape: torch.Size([160000]), label: 408\n",
      "Successfully loaded audio: audiosets/ontology/Narration, monologue_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 57, shape: torch.Size([160000]), label: 81\n",
      "Successfully loaded audio: audiosets/ontology/Music_0.wav, shape: (9253431,), dtype: float32\n",
      "Processed audio shape: torch.Size([9253431])\n",
      "Successfully loaded audio at index 58, shape: torch.Size([160000]), label: 332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  10%|â–‰         | 60/610 [00:24<03:33,  2.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Bicycle bell_4.wav, shape: (670779,), dtype: float32\n",
      "Processed audio shape: torch.Size([670779])\n",
      "Successfully loaded audio at index 59, shape: torch.Size([160000]), label: 413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  10%|â–ˆ         | 61/610 [00:25<04:30,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Female speech, woman speaking_0.wav, shape: (8943616,), dtype: float32\n",
      "Processed audio shape: torch.Size([8943616])\n",
      "Successfully loaded audio at index 60, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Printer_4.wav, shape: (865455,), dtype: float32\n",
      "Processed audio shape: torch.Size([865455])\n",
      "Successfully loaded audio at index 61, shape: torch.Size([160000]), label: 216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  10%|â–ˆ         | 63/610 [00:26<03:12,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Change ringing (campanology)_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 62, shape: torch.Size([160000]), label: 114\n",
      "Successfully loaded audio: audiosets/ontology/Synthetic singing_1.wav, shape: (732637,), dtype: float32\n",
      "Processed audio shape: torch.Size([732637])\n",
      "Successfully loaded audio at index 63, shape: torch.Size([160000]), label: 399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  11%|â–ˆ         | 65/610 [00:26<02:56,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Grunt_0.wav, shape: (3089555,), dtype: float32\n",
      "Processed audio shape: torch.Size([3089555])\n",
      "Successfully loaded audio at index 64, shape: torch.Size([160000]), label: 100\n",
      "Successfully loaded audio: audiosets/ontology/Engine starting_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 65, shape: torch.Size([160000]), label: 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  11%|â–ˆ         | 67/610 [00:26<02:01,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Scratching (performance technique)_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 66, shape: torch.Size([160000]), label: 520\n",
      "Successfully loaded audio: audiosets/ontology/Honk_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 67, shape: torch.Size([160000]), label: 180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  11%|â–ˆ         | 68/610 [00:27<01:44,  5.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Chainsaw_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 68, shape: torch.Size([160000]), label: 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  11%|â–ˆâ–        | 70/610 [00:27<02:06,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Subway, metro, underground_4.wav, shape: (3778166,), dtype: float32\n",
      "Processed audio shape: torch.Size([3778166])\n",
      "Successfully loaded audio at index 69, shape: torch.Size([160000]), label: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  12%|â–ˆâ–        | 71/610 [00:27<02:09,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Crackle_5.wav, shape: (1461929,), dtype: float32\n",
      "Processed audio shape: torch.Size([1461929])\n",
      "Successfully loaded audio at index 70, shape: torch.Size([160000]), label: 51\n",
      "Successfully loaded audio: audiosets/ontology/Siren_4.wav, shape: (239259,), dtype: float32\n",
      "Processed audio shape: torch.Size([239259])\n",
      "Successfully loaded audio at index 71, shape: torch.Size([160000]), label: 411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  12%|â–ˆâ–        | 73/610 [00:28<01:35,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Shuffling cards_5.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 72, shape: torch.Size([160000]), label: 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  12%|â–ˆâ–        | 74/610 [00:28<02:25,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Church bell_4.wav, shape: (4743744,), dtype: float32\n",
      "Processed audio shape: torch.Size([4743744])\n",
      "Successfully loaded audio at index 73, shape: torch.Size([160000]), label: 183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  12%|â–ˆâ–        | 75/610 [00:29<02:59,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Baby laughter_7.wav, shape: (3932904,), dtype: float32\n",
      "Processed audio shape: torch.Size([3932904])\n",
      "Successfully loaded audio at index 74, shape: torch.Size([160000]), label: 487\n",
      "Successfully loaded audio: audiosets/ontology/Typewriter_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 75, shape: torch.Size([160000]), label: 366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  13%|â–ˆâ–Ž        | 77/610 [00:29<02:04,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Clip-clop_2.wav, shape: (148980,), dtype: float32\n",
      "Processed audio shape: torch.Size([148980])\n",
      "Successfully loaded audio at index 76, shape: torch.Size([160000]), label: 518\n",
      "Successfully loaded audio: audiosets/ontology/Electronic organ_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 77, shape: torch.Size([160000]), label: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  13%|â–ˆâ–Ž        | 79/610 [00:29<01:46,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Fowl_1.wav, shape: (1422176,), dtype: float32\n",
      "Processed audio shape: torch.Size([1422176])\n",
      "Successfully loaded audio at index 78, shape: torch.Size([160000]), label: 157\n",
      "Successfully loaded audio: audiosets/ontology/Bicycle bell_4.wav, shape: (670779,), dtype: float32\n",
      "Processed audio shape: torch.Size([670779])\n",
      "Successfully loaded audio at index 79, shape: torch.Size([160000]), label: 183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  13%|â–ˆâ–Ž        | 81/610 [00:30<03:14,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Telephone dialing, DTMF_0.wav, shape: (9267363,), dtype: float32\n",
      "Processed audio shape: torch.Size([9267363])\n",
      "Successfully loaded audio at index 80, shape: torch.Size([160000]), label: 310\n",
      "Successfully loaded audio: audiosets/ontology/Doorbell_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 81, shape: torch.Size([160000]), label: 183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  14%|â–ˆâ–Ž        | 83/610 [00:30<02:17,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Chainsaw_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 82, shape: torch.Size([160000]), label: 132\n",
      "Successfully loaded audio: audiosets/ontology/Harp_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 83, shape: torch.Size([160000]), label: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  14%|â–ˆâ–        | 85/610 [00:31<01:38,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Police car (siren)_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 84, shape: torch.Size([160000]), label: 411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  14%|â–ˆâ–        | 86/610 [00:31<01:48,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Baby laughter_0.wav, shape: (1603663,), dtype: float32\n",
      "Processed audio shape: torch.Size([1603663])\n",
      "Successfully loaded audio at index 85, shape: torch.Size([160000]), label: 487\n",
      "Successfully loaded audio: audiosets/ontology/Bird vocalization, bird call, bird song_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 86, shape: torch.Size([160000]), label: 158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  14%|â–ˆâ–        | 88/610 [00:31<01:51,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Plucked string instrument_7.wav, shape: (1965337,), dtype: float32\n",
      "Processed audio shape: torch.Size([1965337])\n",
      "Successfully loaded audio at index 87, shape: torch.Size([160000]), label: 212\n",
      "Successfully loaded audio: audiosets/ontology/Belly laugh_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 88, shape: torch.Size([160000]), label: 126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  15%|â–ˆâ–        | 89/610 [00:31<01:34,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Female singing_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 89, shape: torch.Size([160000]), label: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  15%|â–ˆâ–        | 91/610 [00:32<02:05,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Fire engine, fire truck (siren)_5.wav, shape: (4816748,), dtype: float32\n",
      "Processed audio shape: torch.Size([4816748])\n",
      "Successfully loaded audio at index 90, shape: torch.Size([160000]), label: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  15%|â–ˆâ–Œ        | 92/610 [00:33<03:06,  2.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Mechanical fan_1.wav, shape: (6675645,), dtype: float32\n",
      "Processed audio shape: torch.Size([6675645])\n",
      "Successfully loaded audio at index 91, shape: torch.Size([160000]), label: 451\n",
      "Successfully loaded audio: audiosets/ontology/Sliding door_1.wav, shape: (146008,), dtype: float32\n",
      "Processed audio shape: torch.Size([146008])\n",
      "Successfully loaded audio at index 92, shape: torch.Size([160000]), label: 214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  15%|â–ˆâ–Œ        | 94/610 [00:33<02:08,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Whimper_1.wav, shape: (147494,), dtype: float32\n",
      "Processed audio shape: torch.Size([147494])\n",
      "Successfully loaded audio at index 93, shape: torch.Size([160000]), label: 501\n",
      "Successfully loaded audio: audiosets/ontology/Child speech, kid speaking_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 94, shape: torch.Size([160000]), label: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  16%|â–ˆâ–Œ        | 97/610 [00:33<01:20,  6.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Electronic organ_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 95, shape: torch.Size([160000]), label: 380\n",
      "Successfully loaded audio: audiosets/ontology/Snicker_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 96, shape: torch.Size([160000]), label: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  16%|â–ˆâ–Œ        | 98/610 [00:33<01:23,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Walk, footsteps_2.wav, shape: (797652,), dtype: float32\n",
      "Processed audio shape: torch.Size([797652])\n",
      "Successfully loaded audio at index 97, shape: torch.Size([160000]), label: 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  16%|â–ˆâ–Œ        | 99/610 [00:34<01:35,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Child singing_0.wav, shape: (1479762,), dtype: float32\n",
      "Processed audio shape: torch.Size([1479762])\n",
      "Successfully loaded audio at index 98, shape: torch.Size([160000]), label: 470\n",
      "Successfully loaded audio: audiosets/ontology/Race car, auto racing_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 99, shape: torch.Size([160000]), label: 43\n",
      "Successfully loaded audio: audiosets/ontology/Motorboat, speedboat_6.wav, shape: (148980,), dtype: float32\n",
      "Processed audio shape: torch.Size([148980])\n",
      "Successfully loaded audio at index 100, shape: torch.Size([160000]), label: 498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  17%|â–ˆâ–‹        | 102/610 [00:34<01:56,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Aircraft engine_9.wav, shape: (3801200,), dtype: float32\n",
      "Processed audio shape: torch.Size([3801200])\n",
      "Successfully loaded audio at index 101, shape: torch.Size([160000]), label: 278\n",
      "Successfully loaded audio: audiosets/ontology/Meow_8.wav, shape: (144893,), dtype: float32\n",
      "Processed audio shape: torch.Size([144893])\n",
      "Successfully loaded audio at index 102, shape: torch.Size([160000]), label: 483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  17%|â–ˆâ–‹        | 104/610 [00:35<01:50,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Child singing_5.wav, shape: (2232831,), dtype: float32\n",
      "Processed audio shape: torch.Size([2232831])\n",
      "Successfully loaded audio at index 103, shape: torch.Size([160000]), label: 470\n",
      "Successfully loaded audio: audiosets/ontology/Piano_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 104, shape: torch.Size([160000]), label: 386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  17%|â–ˆâ–‹        | 106/610 [00:35<01:27,  5.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Boat, Water vehicle_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 105, shape: torch.Size([160000]), label: 198\n",
      "Successfully loaded audio: audiosets/ontology/Artillery fire_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 106, shape: torch.Size([160000]), label: 383\n",
      "Successfully loaded audio: audiosets/ontology/Singing_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 107, shape: torch.Size([160000]), label: 181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  18%|â–ˆâ–Š        | 108/610 [00:35<01:13,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Child speech, kid speaking_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 108, shape: torch.Size([160000]), label: 423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  18%|â–ˆâ–Š        | 110/610 [00:36<01:43,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Church bell_4.wav, shape: (4743744,), dtype: float32\n",
      "Processed audio shape: torch.Size([4743744])\n",
      "Successfully loaded audio at index 109, shape: torch.Size([160000]), label: 242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  18%|â–ˆâ–Š        | 111/610 [00:36<02:20,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Ambulance (siren)_6.wav, shape: (5362138,), dtype: float32\n",
      "Processed audio shape: torch.Size([5362138])\n",
      "Successfully loaded audio at index 110, shape: torch.Size([160000]), label: 190\n",
      "Successfully loaded audio: audiosets/ontology/Water tap, faucet_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 111, shape: torch.Size([160000]), label: 184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  19%|â–ˆâ–Š        | 113/610 [00:37<01:48,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Caterwaul_0.wav, shape: (202664,), dtype: float32\n",
      "Processed audio shape: torch.Size([202664])\n",
      "Successfully loaded audio at index 112, shape: torch.Size([160000]), label: 345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  19%|â–ˆâ–Š        | 114/610 [00:37<02:01,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Snare drum_4.wav, shape: (2483235,), dtype: float32\n",
      "Processed audio shape: torch.Size([2483235])\n",
      "Successfully loaded audio at index 113, shape: torch.Size([160000]), label: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  19%|â–ˆâ–‰        | 115/610 [00:38<03:01,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Rustling leaves_0.wav, shape: (7091932,), dtype: float32\n",
      "Processed audio shape: torch.Size([7091932])\n",
      "Successfully loaded audio at index 114, shape: torch.Size([160000]), label: 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  19%|â–ˆâ–‰        | 116/610 [00:38<02:44,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Fire alarm_2.wav, shape: (1054744,), dtype: float32\n",
      "Processed audio shape: torch.Size([1054744])\n",
      "Successfully loaded audio at index 115, shape: torch.Size([160000]), label: 297\n",
      "Successfully loaded audio: audiosets/ontology/Crowing, cock-a-doodle-doo_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 116, shape: torch.Size([160000]), label: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  19%|â–ˆâ–‰        | 118/610 [00:39<02:29,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Fowl_12.wav, shape: (2871659,), dtype: float32\n",
      "Processed audio shape: torch.Size([2871659])\n",
      "Successfully loaded audio at index 117, shape: torch.Size([160000]), label: 157\n",
      "Successfully loaded audio: audiosets/ontology/Groan_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 118, shape: torch.Size([160000]), label: 336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|â–ˆâ–‰        | 120/610 [00:39<01:44,  4.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Artillery fire_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 119, shape: torch.Size([160000]), label: 383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|â–ˆâ–‰        | 121/610 [00:39<01:42,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Run_0.wav, shape: (1329482,), dtype: float32\n",
      "Processed audio shape: torch.Size([1329482])\n",
      "Successfully loaded audio at index 120, shape: torch.Size([160000]), label: 100\n",
      "Successfully loaded audio: audiosets/ontology/Speech_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 121, shape: torch.Size([160000]), label: 423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|â–ˆâ–ˆ        | 124/610 [00:39<01:08,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Hands_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 122, shape: torch.Size([160000]), label: 211\n",
      "Successfully loaded audio: audiosets/ontology/Shuffling cards_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 123, shape: torch.Size([160000]), label: 398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|â–ˆâ–ˆ        | 125/610 [00:39<01:05,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Hubbub, speech noise, speech babble_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 124, shape: torch.Size([160000]), label: 423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  21%|â–ˆâ–ˆ        | 126/610 [00:40<02:25,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Whoop_5.wav, shape: (5619044,), dtype: float32\n",
      "Processed audio shape: torch.Size([5619044])\n",
      "Successfully loaded audio at index 125, shape: torch.Size([160000]), label: 213\n",
      "Successfully loaded audio: audiosets/ontology/Sanding_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 126, shape: torch.Size([160000]), label: 407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  21%|â–ˆâ–ˆ        | 129/610 [00:40<01:22,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Cricket_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 127, shape: torch.Size([160000]), label: 338\n",
      "Successfully loaded audio: audiosets/ontology/Bleat_2.wav, shape: (147122,), dtype: float32\n",
      "Processed audio shape: torch.Size([147122])\n",
      "Successfully loaded audio at index 128, shape: torch.Size([160000]), label: 262\n",
      "Successfully loaded audio: audiosets/ontology/Harpsichord_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 129, shape: torch.Size([160000]), label: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  21%|â–ˆâ–ˆâ–       | 131/610 [00:41<01:07,  7.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Wind instrument, woodwind instrument_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 130, shape: torch.Size([160000]), label: 296\n",
      "Successfully loaded audio: audiosets/ontology/Keyboard (musical)_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 131, shape: torch.Size([160000]), label: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  22%|â–ˆâ–ˆâ–       | 133/610 [00:41<00:58,  8.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Bird vocalization, bird call, bird song_5.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 132, shape: torch.Size([160000]), label: 269\n",
      "Successfully loaded audio: audiosets/ontology/Banjo_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 133, shape: torch.Size([160000]), label: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  22%|â–ˆâ–ˆâ–       | 135/610 [00:41<00:58,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Children playing_1.wav, shape: (662048,), dtype: float32\n",
      "Processed audio shape: torch.Size([662048])\n",
      "Successfully loaded audio at index 134, shape: torch.Size([160000]), label: 219\n",
      "Successfully loaded audio: audiosets/ontology/Livestock, farm animals, working animals_3.wav, shape: (153066,), dtype: float32\n",
      "Processed audio shape: torch.Size([153066])\n",
      "Successfully loaded audio at index 135, shape: torch.Size([160000]), label: 358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  22%|â–ˆâ–ˆâ–       | 137/610 [00:41<00:55,  8.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Air horn, truck horn_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 136, shape: torch.Size([160000]), label: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 138/610 [00:42<01:49,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Medium engine (mid frequency)_5.wav, shape: (5689261,), dtype: float32\n",
      "Processed audio shape: torch.Size([5689261])\n",
      "Successfully loaded audio at index 137, shape: torch.Size([160000]), label: 87\n",
      "Successfully loaded audio: audiosets/ontology/Telephone_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 138, shape: torch.Size([160000]), label: 310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 139/610 [00:42<01:36,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Duck_5.wav, shape: (14558726,), dtype: float32\n",
      "Processed audio shape: torch.Size([14558726])\n",
      "Successfully loaded audio at index 139, shape: torch.Size([160000]), label: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 141/610 [00:44<03:56,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Steam whistle_2.wav, shape: (1866513,), dtype: float32\n",
      "Processed audio shape: torch.Size([1866513])\n",
      "Successfully loaded audio at index 140, shape: torch.Size([160000]), label: 188\n",
      "Successfully loaded audio: audiosets/ontology/Tap_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 141, shape: torch.Size([160000]), label: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  23%|â–ˆâ–ˆâ–Ž       | 143/610 [00:45<03:25,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Heavy engine (low frequency)_4.wav, shape: (4222503,), dtype: float32\n",
      "Processed audio shape: torch.Size([4222503])\n",
      "Successfully loaded audio at index 142, shape: torch.Size([160000]), label: 16\n",
      "Successfully loaded audio: audiosets/ontology/Chirp, tweet_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 143, shape: torch.Size([160000]), label: 203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  24%|â–ˆâ–ˆâ–       | 145/610 [00:45<03:01,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Sigh_1.wav, shape: (3688072,), dtype: float32\n",
      "Processed audio shape: torch.Size([3688072])\n",
      "Successfully loaded audio at index 144, shape: torch.Size([160000]), label: 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  24%|â–ˆâ–ˆâ–       | 146/610 [00:46<03:14,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Snare drum_6.wav, shape: (3334572,), dtype: float32\n",
      "Processed audio shape: torch.Size([3334572])\n",
      "Successfully loaded audio at index 145, shape: torch.Size([160000]), label: 300\n",
      "Successfully loaded audio: audiosets/ontology/Male singing_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 146, shape: torch.Size([160000]), label: 181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  24%|â–ˆâ–ˆâ–       | 148/610 [00:46<02:05,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Microwave oven_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 147, shape: torch.Size([160000]), label: 173\n",
      "Successfully loaded audio: audiosets/ontology/Domestic animals, pets_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 148, shape: torch.Size([160000]), label: 287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  24%|â–ˆâ–ˆâ–       | 149/610 [00:46<01:42,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Tambourine_6.wav, shape: (9487860,), dtype: float32\n",
      "Processed audio shape: torch.Size([9487860])\n",
      "Successfully loaded audio at index 149, shape: torch.Size([160000]), label: 509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  25%|â–ˆâ–ˆâ–       | 151/610 [00:48<03:08,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Mallet percussion_5.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 150, shape: torch.Size([160000]), label: 97\n",
      "Successfully loaded audio: audiosets/ontology/Toilet flush_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 151, shape: torch.Size([160000]), label: 215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 153/610 [00:48<02:01,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Owl_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 152, shape: torch.Size([160000]), label: 157\n",
      "Successfully loaded audio: audiosets/ontology/Glass_8.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 153, shape: torch.Size([160000]), label: 318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  25%|â–ˆâ–ˆâ–Œ       | 155/610 [00:48<01:26,  5.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Camera_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 154, shape: torch.Size([160000]), label: 127\n",
      "Successfully loaded audio: audiosets/ontology/Pulleys_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 155, shape: torch.Size([160000]), label: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 157/610 [00:48<01:12,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Railroad car, train wagon_9.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 156, shape: torch.Size([160000]), label: 255\n",
      "Successfully loaded audio: audiosets/ontology/Skateboard_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 157, shape: torch.Size([160000]), label: 159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  26%|â–ˆâ–ˆâ–Œ       | 159/610 [00:49<01:01,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Female singing_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 158, shape: torch.Size([160000]), label: 181\n",
      "Successfully loaded audio: audiosets/ontology/Cat_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 159, shape: torch.Size([160000]), label: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  26%|â–ˆâ–ˆâ–‹       | 161/610 [00:49<00:58,  7.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Motor vehicle (road)_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 160, shape: torch.Size([160000]), label: 198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  27%|â–ˆâ–ˆâ–‹       | 162/610 [00:49<01:34,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Drum kit_7.wav, shape: (2799770,), dtype: float32\n",
      "Processed audio shape: torch.Size([2799770])\n",
      "Successfully loaded audio at index 161, shape: torch.Size([160000]), label: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  27%|â–ˆâ–ˆâ–‹       | 163/610 [00:49<01:43,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Trumpet_4.wav, shape: (1470474,), dtype: float32\n",
      "Processed audio shape: torch.Size([1470474])\n",
      "Successfully loaded audio at index 162, shape: torch.Size([160000]), label: 195\n",
      "Successfully loaded audio: audiosets/ontology/Walk, footsteps_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 163, shape: torch.Size([160000]), label: 44\n",
      "Successfully loaded audio: audiosets/ontology/Bird flight, flapping wings_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 164, shape: torch.Size([160000]), label: 231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  27%|â–ˆâ–ˆâ–‹       | 167/610 [00:50<00:55,  7.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Snap_4.wav, shape: (298702,), dtype: float32\n",
      "Processed audio shape: torch.Size([298702])\n",
      "Successfully loaded audio at index 165, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Water tap, faucet_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 166, shape: torch.Size([160000]), label: 78\n",
      "Successfully loaded audio: audiosets/ontology/Harp_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 167, shape: torch.Size([160000]), label: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  28%|â–ˆâ–ˆâ–Š       | 169/610 [00:50<01:08,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Mouse_0.wav, shape: (2318838,), dtype: float32\n",
      "Processed audio shape: torch.Size([2318838])\n",
      "Successfully loaded audio at index 168, shape: torch.Size([160000]), label: 19\n",
      "Successfully loaded audio: audiosets/ontology/Motorcycle_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 169, shape: torch.Size([160000]), label: 507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  28%|â–ˆâ–ˆâ–Š       | 171/610 [00:50<00:59,  7.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Plucked string instrument_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 170, shape: torch.Size([160000]), label: 142\n",
      "Successfully loaded audio: audiosets/ontology/Quack_2.wav, shape: (148237,), dtype: float32\n",
      "Processed audio shape: torch.Size([148237])\n",
      "Successfully loaded audio at index 171, shape: torch.Size([160000]), label: 394\n",
      "Successfully loaded audio: audiosets/ontology/Light engine (high frequency)_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 172, shape: torch.Size([160000]), label: 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  29%|â–ˆâ–ˆâ–Š       | 174/610 [00:51<00:52,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Car alarm_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 173, shape: torch.Size([160000]), label: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  29%|â–ˆâ–ˆâ–Š       | 175/610 [00:51<01:13,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Female speech, woman speaking_1.wav, shape: (3785410,), dtype: float32\n",
      "Processed audio shape: torch.Size([3785410])\n",
      "Successfully loaded audio at index 174, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Helicopter_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 175, shape: torch.Size([160000]), label: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  29%|â–ˆâ–ˆâ–‰       | 177/610 [00:51<01:20,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Roaring cats (lions, tigers)_4.wav, shape: (2599149,), dtype: float32\n",
      "Processed audio shape: torch.Size([2599149])\n",
      "Successfully loaded audio at index 176, shape: torch.Size([160000]), label: 48\n",
      "Successfully loaded audio: audiosets/ontology/Emergency vehicle_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 177, shape: torch.Size([160000]), label: 198\n",
      "Successfully loaded audio: audiosets/ontology/Shuffling cards_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 178, shape: torch.Size([160000]), label: 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  30%|â–ˆâ–ˆâ–‰       | 180/610 [00:52<01:25,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Crowd_1.wav, shape: (3221444,), dtype: float32\n",
      "Processed audio shape: torch.Size([3221444])\n",
      "Successfully loaded audio at index 179, shape: torch.Size([160000]), label: 271\n",
      "Successfully loaded audio: audiosets/ontology/Emergency vehicle_6.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 180, shape: torch.Size([160000]), label: 198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  30%|â–ˆâ–ˆâ–‰       | 182/610 [00:52<01:09,  6.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Liquid_5.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 181, shape: torch.Size([160000]), label: 21\n",
      "Successfully loaded audio: audiosets/ontology/Railroad car, train wagon_5.wav, shape: (540190,), dtype: float32\n",
      "Processed audio shape: torch.Size([540190])\n",
      "Successfully loaded audio at index 182, shape: torch.Size([160000]), label: 431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  30%|â–ˆâ–ˆâ–ˆ       | 185/610 [00:53<00:56,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Child singing_8.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 183, shape: torch.Size([160000]), label: 181\n",
      "Successfully loaded audio: audiosets/ontology/Slosh_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 184, shape: torch.Size([160000]), label: 201\n",
      "Successfully loaded audio: audiosets/ontology/Finger snapping_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 185, shape: torch.Size([160000]), label: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 187/610 [00:53<00:46,  9.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Cough_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 186, shape: torch.Size([160000]), label: 499\n",
      "Successfully loaded audio: audiosets/ontology/Dog_8.wav, shape: (6734716,), dtype: float32\n",
      "Processed audio shape: torch.Size([6734716])\n",
      "Successfully loaded audio at index 187, shape: torch.Size([160000]), label: 432\n",
      "Successfully loaded audio: audiosets/ontology/Traffic noise, roadway noise_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 188, shape: torch.Size([160000]), label: 495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  31%|â–ˆâ–ˆâ–ˆ       | 190/610 [00:54<01:17,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Gargling_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 189, shape: torch.Size([160000]), label: 354\n",
      "Successfully loaded audio: audiosets/ontology/Gasp_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 190, shape: torch.Size([160000]), label: 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  31%|â–ˆâ–ˆâ–ˆâ–      | 192/610 [00:54<01:03,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Change ringing (campanology)_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 191, shape: torch.Size([160000]), label: 114\n",
      "Successfully loaded audio: audiosets/ontology/Glass_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 192, shape: torch.Size([160000]), label: 318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 194/610 [00:54<01:15,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Snare drum_6.wav, shape: (3334572,), dtype: float32\n",
      "Processed audio shape: torch.Size([3334572])\n",
      "Successfully loaded audio at index 193, shape: torch.Size([160000]), label: 312\n",
      "Successfully loaded audio: audiosets/ontology/Camera_6.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 194, shape: torch.Size([160000]), label: 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 196/610 [00:55<01:39,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Roaring cats (lions, tigers)_5.wav, shape: (5891925,), dtype: float32\n",
      "Processed audio shape: torch.Size([5891925])\n",
      "Successfully loaded audio at index 195, shape: torch.Size([160000]), label: 105\n",
      "Successfully loaded audio: audiosets/ontology/Wind noise (microphone)_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 196, shape: torch.Size([160000]), label: 495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  32%|â–ˆâ–ˆâ–ˆâ–      | 198/610 [00:55<01:19,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Sitar_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 197, shape: torch.Size([160000]), label: 99\n",
      "Successfully loaded audio: audiosets/ontology/Roar_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 198, shape: torch.Size([160000]), label: 105\n",
      "Successfully loaded audio: audiosets/ontology/Tire squeal_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 199, shape: torch.Size([160000]), label: 178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 202/610 [00:55<00:54,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Heart sounds, heartbeat_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 200, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Singing bowl_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 201, shape: torch.Size([160000]), label: 157\n",
      "Successfully loaded audio: audiosets/ontology/Coin (dropping)_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 202, shape: torch.Size([160000]), label: 249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 204/610 [00:56<00:49,  8.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Children playing_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 203, shape: torch.Size([160000]), label: 219\n",
      "Successfully loaded audio: audiosets/ontology/Bicycle_5.wav, shape: (9600987,), dtype: float32\n",
      "Processed audio shape: torch.Size([9600987])\n",
      "Successfully loaded audio at index 204, shape: torch.Size([160000]), label: 439\n",
      "Successfully loaded audio: audiosets/ontology/Police car (siren)_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 205, shape: torch.Size([160000]), label: 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 207/610 [00:57<01:31,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Whimper_3.wav, shape: (877529,), dtype: float32\n",
      "Processed audio shape: torch.Size([877529])\n",
      "Successfully loaded audio at index 206, shape: torch.Size([160000]), label: 501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  34%|â–ˆâ–ˆâ–ˆâ–      | 208/610 [00:57<01:58,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Rapping_1.wav, shape: (4839782,), dtype: float32\n",
      "Processed audio shape: torch.Size([4839782])\n",
      "Successfully loaded audio at index 207, shape: torch.Size([160000]), label: 402\n",
      "Successfully loaded audio: audiosets/ontology/Train wheels squealing_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 208, shape: torch.Size([160000]), label: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–      | 211/610 [00:58<01:19,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Skidding_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 209, shape: torch.Size([160000]), label: 209\n",
      "Successfully loaded audio: audiosets/ontology/Domestic animals, pets_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 210, shape: torch.Size([160000]), label: 358\n",
      "Successfully loaded audio: audiosets/ontology/Fireworks_7.wav, shape: (15188080,), dtype: float32\n",
      "Processed audio shape: torch.Size([15188080])\n",
      "Successfully loaded audio at index 211, shape: torch.Size([160000]), label: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–      | 213/610 [01:00<03:46,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Jet engine_3.wav, shape: (7725744,), dtype: float32\n",
      "Processed audio shape: torch.Size([7725744])\n",
      "Successfully loaded audio at index 212, shape: torch.Size([160000]), label: 63\n",
      "Successfully loaded audio: audiosets/ontology/Motorcycle_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 213, shape: torch.Size([160000]), label: 507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 215/610 [01:00<02:22,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Hubbub, speech noise, speech babble_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 214, shape: torch.Size([160000]), label: 495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 216/610 [01:01<02:28,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Clock_1.wav, shape: (3338287,), dtype: float32\n",
      "Processed audio shape: torch.Size([3338287])\n",
      "Successfully loaded audio at index 215, shape: torch.Size([160000]), label: 4\n",
      "Successfully loaded audio: audiosets/ontology/Civil defense siren_6.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 216, shape: torch.Size([160000]), label: 411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 217/610 [01:01<01:57,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Coin (dropping)_0.wav, shape: (14648634,), dtype: float32\n",
      "Processed audio shape: torch.Size([14648634])\n",
      "Successfully loaded audio at index 217, shape: torch.Size([160000]), label: 249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 219/610 [01:02<03:19,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Civil defense siren_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 218, shape: torch.Size([160000]), label: 322\n",
      "Successfully loaded audio: audiosets/ontology/Steelpan_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 219, shape: torch.Size([160000]), label: 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 221/610 [01:03<02:00,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Civil defense siren_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 220, shape: torch.Size([160000]), label: 322\n",
      "Successfully loaded audio: audiosets/ontology/Female singing_5.wav, shape: (649045,), dtype: float32\n",
      "Processed audio shape: torch.Size([649045])\n",
      "Successfully loaded audio at index 221, shape: torch.Size([160000]), label: 181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 224/610 [01:03<01:10,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Telephone_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 222, shape: torch.Size([160000]), label: 310\n",
      "Successfully loaded audio: audiosets/ontology/Whimper (dog)_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 223, shape: torch.Size([160000]), label: 501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 225/610 [01:03<01:32,  4.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Gasp_0.wav, shape: (3371909,), dtype: float32\n",
      "Processed audio shape: torch.Size([3371909])\n",
      "Successfully loaded audio at index 224, shape: torch.Size([160000]), label: 69\n",
      "Successfully loaded audio: audiosets/ontology/Boom_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 225, shape: torch.Size([160000]), label: 246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 228/610 [01:04<01:02,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Whoop_0.wav, shape: (299631,), dtype: float32\n",
      "Processed audio shape: torch.Size([299631])\n",
      "Successfully loaded audio at index 226, shape: torch.Size([160000]), label: 213\n",
      "Successfully loaded audio: audiosets/ontology/Crack_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 227, shape: torch.Size([160000]), label: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 229/610 [01:04<01:00,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Howl_0.wav, shape: (666506,), dtype: float32\n",
      "Processed audio shape: torch.Size([666506])\n",
      "Successfully loaded audio at index 228, shape: torch.Size([160000]), label: 157\n",
      "Successfully loaded audio: audiosets/ontology/Groan_5.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 229, shape: torch.Size([160000]), label: 336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 231/610 [01:04<01:00,  6.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Fusillade_0.wav, shape: (1891962,), dtype: float32\n",
      "Processed audio shape: torch.Size([1891962])\n",
      "Successfully loaded audio at index 230, shape: torch.Size([160000]), label: 170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 232/610 [01:05<01:25,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Musical instrument_3.wav, shape: (3790797,), dtype: float32\n",
      "Processed audio shape: torch.Size([3790797])\n",
      "Successfully loaded audio at index 231, shape: torch.Size([160000]), label: 142\n",
      "Successfully loaded audio: audiosets/ontology/Fowl_10.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 232, shape: torch.Size([160000]), label: 157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 234/610 [01:05<01:07,  5.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Fowl_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 233, shape: torch.Size([160000]), label: 382\n",
      "Successfully loaded audio: audiosets/ontology/Piano_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 234, shape: torch.Size([160000]), label: 386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 235/610 [01:05<01:00,  6.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Steam whistle_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 235, shape: torch.Size([160000]), label: 188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 237/610 [01:05<01:06,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Snare drum_8.wav, shape: (2638902,), dtype: float32\n",
      "Processed audio shape: torch.Size([2638902])\n",
      "Successfully loaded audio at index 236, shape: torch.Size([160000]), label: 300\n",
      "Successfully loaded audio: audiosets/ontology/Chuckle, chortle_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 237, shape: torch.Size([160000]), label: 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 239/610 [01:06<01:31,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Steam whistle_5.wav, shape: (6059851,), dtype: float32\n",
      "Processed audio shape: torch.Size([6059851])\n",
      "Successfully loaded audio at index 238, shape: torch.Size([160000]), label: 373\n",
      "Successfully loaded audio: audiosets/ontology/Tuning fork_3.wav, shape: (12895574,), dtype: float32\n",
      "Processed audio shape: torch.Size([12895574])\n",
      "Successfully loaded audio at index 239, shape: torch.Size([160000]), label: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 240/610 [01:08<03:07,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Boat, Water vehicle_6.wav, shape: (12422827,), dtype: float32\n",
      "Processed audio shape: torch.Size([12422827])\n",
      "Successfully loaded audio at index 240, shape: torch.Size([160000]), label: 184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 241/610 [01:09<04:10,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Race car, auto racing_5.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 241, shape: torch.Size([160000]), label: 43\n",
      "Successfully loaded audio: audiosets/ontology/Wheeze_1.wav, shape: (146379,), dtype: float32\n",
      "Processed audio shape: torch.Size([146379])\n",
      "Successfully loaded audio at index 242, shape: torch.Size([160000]), label: 136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 244/610 [01:09<02:19,  2.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Glass_5.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 243, shape: torch.Size([160000]), label: 318\n",
      "Successfully loaded audio: audiosets/ontology/Motor vehicle (road)_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 244, shape: torch.Size([160000]), label: 198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 245/610 [01:09<01:56,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Tabla_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 245, shape: torch.Size([160000]), label: 465\n",
      "Successfully loaded audio: audiosets/ontology/Wind noise (microphone)_3.wav, shape: (13654358,), dtype: float32\n",
      "Processed audio shape: torch.Size([13654358])\n",
      "Successfully loaded audio at index 246, shape: torch.Size([160000]), label: 388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 248/610 [01:11<02:15,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Train horn_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 247, shape: torch.Size([160000]), label: 108\n",
      "Successfully loaded audio: audiosets/ontology/Livestock, farm animals, working animals_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 248, shape: torch.Size([160000]), label: 358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 250/610 [01:11<01:47,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Skidding_0.wav, shape: (1604221,), dtype: float32\n",
      "Processed audio shape: torch.Size([1604221])\n",
      "Successfully loaded audio at index 249, shape: torch.Size([160000]), label: 316\n",
      "Successfully loaded audio: audiosets/ontology/Whale vocalization_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 250, shape: torch.Size([160000]), label: 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 252/610 [01:11<01:19,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Rapping_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 251, shape: torch.Size([160000]), label: 402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 253/610 [01:12<01:24,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Drum machine_4.wav, shape: (1365334,), dtype: float32\n",
      "Processed audio shape: torch.Size([1365334])\n",
      "Successfully loaded audio at index 252, shape: torch.Size([160000]), label: 300\n",
      "Successfully loaded audio: audiosets/ontology/Howl (wind)_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 253, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Turkey_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 254, shape: torch.Size([160000]), label: 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 257/610 [01:12<00:49,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Wind instrument, woodwind instrument_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 255, shape: torch.Size([160000]), label: 296\n",
      "Successfully loaded audio: audiosets/ontology/Keys jangling_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 256, shape: torch.Size([160000]), label: 285\n",
      "Successfully loaded audio: audiosets/ontology/Knock_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 257, shape: torch.Size([160000]), label: 264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 259/610 [01:12<01:01,  5.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Narration, monologue_7.wav, shape: (3693274,), dtype: float32\n",
      "Processed audio shape: torch.Size([3693274])\n",
      "Successfully loaded audio at index 258, shape: torch.Size([160000]), label: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 260/610 [01:13<01:49,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Fire engine, fire truck (siren)_4.wav, shape: (9383277,), dtype: float32\n",
      "Processed audio shape: torch.Size([9383277])\n",
      "Successfully loaded audio at index 259, shape: torch.Size([160000]), label: 411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 261/610 [01:14<01:46,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Civil defense siren_2.wav, shape: (2042613,), dtype: float32\n",
      "Processed audio shape: torch.Size([2042613])\n",
      "Successfully loaded audio at index 260, shape: torch.Size([160000]), label: 411\n",
      "Successfully loaded audio: audiosets/ontology/Air brake_3.wav, shape: (149723,), dtype: float32\n",
      "Processed audio shape: torch.Size([149723])\n",
      "Successfully loaded audio at index 261, shape: torch.Size([160000]), label: 303\n",
      "Successfully loaded audio: audiosets/ontology/Railroad car, train wagon_4.wav, shape: (25440526,), dtype: float32\n",
      "Processed audio shape: torch.Size([25440526])\n",
      "Successfully loaded audio at index 262, shape: torch.Size([160000]), label: 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 264/610 [01:16<03:01,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Splash, splatter_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 263, shape: torch.Size([160000]), label: 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 265/610 [01:16<02:55,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Ukulele_4.wav, shape: (4111019,), dtype: float32\n",
      "Processed audio shape: torch.Size([4111019])\n",
      "Successfully loaded audio at index 264, shape: torch.Size([160000]), label: 378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 266/610 [01:18<03:41,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Bird vocalization, bird call, bird song_1.wav, shape: (11449853,), dtype: float32\n",
      "Processed audio shape: torch.Size([11449853])\n",
      "Successfully loaded audio at index 265, shape: torch.Size([160000]), label: 158\n",
      "Successfully loaded audio: audiosets/ontology/Whimper (dog)_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 266, shape: torch.Size([160000]), label: 337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 268/610 [01:18<02:41,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Frying (food)_7.wav, shape: (3402560,), dtype: float32\n",
      "Processed audio shape: torch.Size([3402560])\n",
      "Successfully loaded audio at index 267, shape: torch.Size([160000]), label: 519\n",
      "Successfully loaded audio: audiosets/ontology/Hubbub, speech noise, speech babble_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 268, shape: torch.Size([160000]), label: 401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 270/610 [01:18<02:08,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Hammond organ_1.wav, shape: (2858841,), dtype: float32\n",
      "Processed audio shape: torch.Size([2858841])\n",
      "Successfully loaded audio at index 269, shape: torch.Size([160000]), label: 276\n",
      "Successfully loaded audio: audiosets/ontology/Wind noise (microphone)_3.wav, shape: (13654358,), dtype: float32\n",
      "Processed audio shape: torch.Size([13654358])\n",
      "Successfully loaded audio at index 270, shape: torch.Size([160000]), label: 296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 272/610 [01:20<02:50,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Baby laughter_5.wav, shape: (1407501,), dtype: float32\n",
      "Processed audio shape: torch.Size([1407501])\n",
      "Successfully loaded audio at index 271, shape: torch.Size([160000]), label: 25\n",
      "Successfully loaded audio: audiosets/ontology/Car alarm_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 272, shape: torch.Size([160000]), label: 297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 274/610 [01:21<02:55,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Synthesizer_1.wav, shape: (9291698,), dtype: float32\n",
      "Processed audio shape: torch.Size([9291698])\n",
      "Successfully loaded audio at index 273, shape: torch.Size([160000]), label: 475\n",
      "Successfully loaded audio: audiosets/ontology/Growling_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 274, shape: torch.Size([160000]), label: 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 276/610 [01:21<01:54,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Propeller, airscrew_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 275, shape: torch.Size([160000]), label: 360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 277/610 [01:21<01:46,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Harp_2.wav, shape: (1420133,), dtype: float32\n",
      "Processed audio shape: torch.Size([1420133])\n",
      "Successfully loaded audio at index 276, shape: torch.Size([160000]), label: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 278/610 [01:22<01:54,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Train whistle_0.wav, shape: (4012966,), dtype: float32\n",
      "Processed audio shape: torch.Size([4012966])\n",
      "Successfully loaded audio at index 277, shape: torch.Size([160000]), label: 342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 279/610 [01:23<02:50,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Bowed string instrument_5.wav, shape: (11172328,), dtype: float32\n",
      "Processed audio shape: torch.Size([11172328])\n",
      "Successfully loaded audio at index 278, shape: torch.Size([160000]), label: 142\n",
      "Successfully loaded audio: audiosets/ontology/Ratchet, pawl_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 279, shape: torch.Size([160000]), label: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 283/610 [01:23<01:13,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Electronic organ_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 280, shape: torch.Size([160000]), label: 380\n",
      "Successfully loaded audio: audiosets/ontology/Sliding door_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 281, shape: torch.Size([160000]), label: 316\n",
      "Successfully loaded audio: audiosets/ontology/Rowboat, canoe, kayak_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 282, shape: torch.Size([160000]), label: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 284/610 [01:24<01:48,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Police car (siren)_2.wav, shape: (7164587,), dtype: float32\n",
      "Processed audio shape: torch.Size([7164587])\n",
      "Successfully loaded audio at index 283, shape: torch.Size([160000]), label: 411\n",
      "Successfully loaded audio: audiosets/ontology/Splash, splatter_4.wav, shape: (445081,), dtype: float32\n",
      "Processed audio shape: torch.Size([445081])\n",
      "Successfully loaded audio at index 284, shape: torch.Size([160000]), label: 406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 285/610 [01:24<01:32,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Emergency vehicle_6.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 285, shape: torch.Size([160000]), label: 456\n",
      "Successfully loaded audio: audiosets/ontology/Air horn, truck horn_7.wav, shape: (947003,), dtype: float32\n",
      "Processed audio shape: torch.Size([947003])\n",
      "Successfully loaded audio at index 286, shape: torch.Size([160000]), label: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 287/610 [01:24<01:10,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Cattle, bovinae_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 287, shape: torch.Size([160000]), label: 333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 289/610 [01:25<01:07,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Guitar_2.wav, shape: (2824290,), dtype: float32\n",
      "Processed audio shape: torch.Size([2824290])\n",
      "Successfully loaded audio at index 288, shape: torch.Size([160000]), label: 168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 290/610 [01:25<01:39,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Emergency vehicle_4.wav, shape: (7679490,), dtype: float32\n",
      "Processed audio shape: torch.Size([7679490])\n",
      "Successfully loaded audio at index 289, shape: torch.Size([160000]), label: 198\n",
      "Successfully loaded audio: audiosets/ontology/Wheeze_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 290, shape: torch.Size([160000]), label: 136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 292/610 [01:25<01:12,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Motor vehicle (road)_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 291, shape: torch.Size([160000]), label: 64\n",
      "Successfully loaded audio: audiosets/ontology/Frying (food)_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 292, shape: torch.Size([160000]), label: 519\n",
      "Successfully loaded audio: audiosets/ontology/Children shouting_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 293, shape: torch.Size([160000]), label: 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 294/610 [01:26<00:57,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Sewing machine_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 294, shape: torch.Size([160000]), label: 364\n",
      "Successfully loaded audio: audiosets/ontology/Fire engine, fire truck (siren)_7.wav, shape: (14234390,), dtype: float32\n",
      "Processed audio shape: torch.Size([14234390])\n",
      "Successfully loaded audio at index 295, shape: torch.Size([160000]), label: 404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 297/610 [01:27<01:30,  3.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Musical instrument_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 296, shape: torch.Size([160000]), label: 332\n",
      "Successfully loaded audio: audiosets/ontology/Wind instrument, woodwind instrument_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 297, shape: torch.Size([160000]), label: 291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 299/610 [01:27<01:08,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Applause_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 298, shape: torch.Size([160000]), label: 115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 300/610 [01:28<01:40,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Jet engine_1.wav, shape: (8009213,), dtype: float32\n",
      "Processed audio shape: torch.Size([8009213])\n",
      "Successfully loaded audio at index 299, shape: torch.Size([160000]), label: 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 301/610 [01:28<01:44,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Musical instrument_3.wav, shape: (3790797,), dtype: float32\n",
      "Processed audio shape: torch.Size([3790797])\n",
      "Successfully loaded audio at index 300, shape: torch.Size([160000]), label: 332\n",
      "Successfully loaded audio: audiosets/ontology/Emergency vehicle_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 301, shape: torch.Size([160000]), label: 198\n",
      "Successfully loaded audio: audiosets/ontology/Water_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 302, shape: torch.Size([160000]), label: 184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 304/610 [01:29<01:31,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Hands_0.wav, shape: (4801887,), dtype: float32\n",
      "Processed audio shape: torch.Size([4801887])\n",
      "Successfully loaded audio at index 303, shape: torch.Size([160000]), label: 211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 305/610 [01:29<01:23,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Ice cream truck, ice cream van_1.wav, shape: (952947,), dtype: float32\n",
      "Processed audio shape: torch.Size([952947])\n",
      "Successfully loaded audio at index 304, shape: torch.Size([160000]), label: 153\n",
      "Successfully loaded audio: audiosets/ontology/Canidae, dogs, wolves_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 305, shape: torch.Size([160000]), label: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 307/610 [01:30<01:49,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Civil defense siren_4.wav, shape: (9724332,), dtype: float32\n",
      "Processed audio shape: torch.Size([9724332])\n",
      "Successfully loaded audio at index 306, shape: torch.Size([160000]), label: 322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 308/610 [01:30<01:38,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Giggle_3.wav, shape: (910037,), dtype: float32\n",
      "Processed audio shape: torch.Size([910037])\n",
      "Successfully loaded audio at index 307, shape: torch.Size([160000]), label: 427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 309/610 [01:31<01:33,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Sawing_2.wav, shape: (1913139,), dtype: float32\n",
      "Processed audio shape: torch.Size([1913139])\n",
      "Successfully loaded audio at index 308, shape: torch.Size([160000]), label: 299\n",
      "Successfully loaded audio: audiosets/ontology/Bowed string instrument_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 309, shape: torch.Size([160000]), label: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 311/610 [01:31<01:06,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Fowl_9.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 310, shape: torch.Size([160000]), label: 157\n",
      "Successfully loaded audio: audiosets/ontology/Electric piano_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 311, shape: torch.Size([160000]), label: 386\n",
      "Successfully loaded audio: audiosets/ontology/Meow_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 312, shape: torch.Size([160000]), label: 483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 314/610 [01:31<00:44,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Canidae, dogs, wolves_7.wav, shape: (381365,), dtype: float32\n",
      "Processed audio shape: torch.Size([381365])\n",
      "Successfully loaded audio at index 313, shape: torch.Size([160000]), label: 133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 315/610 [01:31<00:55,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Church bell_5.wav, shape: (2129735,), dtype: float32\n",
      "Processed audio shape: torch.Size([2129735])\n",
      "Successfully loaded audio at index 314, shape: torch.Size([160000]), label: 183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 316/610 [01:32<01:49,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Railroad car, train wagon_7.wav, shape: (9600987,), dtype: float32\n",
      "Processed audio shape: torch.Size([9600987])\n",
      "Successfully loaded audio at index 315, shape: torch.Size([160000]), label: 431\n",
      "Successfully loaded audio: audiosets/ontology/Moo_0.wav, shape: (144521,), dtype: float32\n",
      "Processed audio shape: torch.Size([144521])\n",
      "Successfully loaded audio at index 316, shape: torch.Size([160000]), label: 156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 318/610 [01:33<01:11,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Waterfall_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 317, shape: torch.Size([160000]), label: 226\n",
      "Successfully loaded audio: audiosets/ontology/Whimper_5.wav, shape: (146751,), dtype: float32\n",
      "Processed audio shape: torch.Size([146751])\n",
      "Successfully loaded audio at index 318, shape: torch.Size([160000]), label: 501\n",
      "Successfully loaded audio: audiosets/ontology/Traffic noise, roadway noise_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 319, shape: torch.Size([160000]), label: 495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 320/610 [01:33<00:53,  5.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Whistle_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 320, shape: torch.Size([160000]), label: 289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 322/610 [01:33<00:56,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Train whistle_2.wav, shape: (3095871,), dtype: float32\n",
      "Processed audio shape: torch.Size([3095871])\n",
      "Successfully loaded audio at index 321, shape: torch.Size([160000]), label: 84\n",
      "Successfully loaded audio: audiosets/ontology/Subway, metro, underground_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 322, shape: torch.Size([160000]), label: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 324/610 [01:33<00:46,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Doorbell_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 323, shape: torch.Size([160000]), label: 217\n",
      "Successfully loaded audio: audiosets/ontology/Sigh_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 324, shape: torch.Size([160000]), label: 83\n",
      "Successfully loaded audio: audiosets/ontology/Stream_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 325, shape: torch.Size([160000]), label: 281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 327/610 [01:34<01:05,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Vibraphone_3.wav, shape: (6499173,), dtype: float32\n",
      "Processed audio shape: torch.Size([6499173])\n",
      "Successfully loaded audio at index 326, shape: torch.Size([160000]), label: 229\n",
      "Successfully loaded audio: audiosets/ontology/Chipmunk_1.wav, shape: (208980,), dtype: float32\n",
      "Processed audio shape: torch.Size([208980])\n",
      "Successfully loaded audio at index 327, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Writing_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 328, shape: torch.Size([160000]), label: 161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 329/610 [01:34<00:47,  5.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Door_1.wav, shape: (3923430,), dtype: float32\n",
      "Processed audio shape: torch.Size([3923430])\n",
      "Successfully loaded audio at index 329, shape: torch.Size([160000]), label: 217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 331/610 [01:35<01:15,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Cluck_3.wav, shape: (5352479,), dtype: float32\n",
      "Processed audio shape: torch.Size([5352479])\n",
      "Successfully loaded audio at index 330, shape: torch.Size([160000]), label: 346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 332/610 [01:36<01:19,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Ocean_3.wav, shape: (2594691,), dtype: float32\n",
      "Processed audio shape: torch.Size([2594691])\n",
      "Successfully loaded audio at index 331, shape: torch.Size([160000]), label: 294\n",
      "Successfully loaded audio: audiosets/ontology/Mandolin_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 332, shape: torch.Size([160000]), label: 408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 335/610 [01:36<00:51,  5.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Tools_2.wav, shape: (160086,), dtype: float32\n",
      "Processed audio shape: torch.Size([160086])\n",
      "Successfully loaded audio at index 333, shape: torch.Size([160000]), label: 202\n",
      "Successfully loaded audio: audiosets/ontology/Motor vehicle (road)_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 334, shape: torch.Size([160000]), label: 64\n",
      "Successfully loaded audio: audiosets/ontology/Bass drum_5.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 335, shape: torch.Size([160000]), label: 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 337/610 [01:36<00:49,  5.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Baby cry, infant cry_0.wav, shape: (1798525,), dtype: float32\n",
      "Processed audio shape: torch.Size([1798525])\n",
      "Successfully loaded audio at index 336, shape: torch.Size([160000]), label: 61\n",
      "Successfully loaded audio: audiosets/ontology/Rail transport_1.wav, shape: (12364905,), dtype: float32\n",
      "Processed audio shape: torch.Size([12364905])\n",
      "Successfully loaded audio at index 337, shape: torch.Size([160000]), label: 381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 339/610 [01:38<01:36,  2.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Wind instrument, woodwind instrument_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 338, shape: torch.Size([160000]), label: 291\n",
      "Successfully loaded audio: audiosets/ontology/Drum roll_0.wav, shape: (9470294,), dtype: float32\n",
      "Processed audio shape: torch.Size([9470294])\n",
      "Successfully loaded audio at index 339, shape: torch.Size([160000]), label: 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 340/610 [01:39<02:18,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Sewing machine_8.wav, shape: (7832556,), dtype: float32\n",
      "Processed audio shape: torch.Size([7832556])\n",
      "Successfully loaded audio at index 340, shape: torch.Size([160000]), label: 364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 341/610 [01:39<02:45,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Typing_4.wav, shape: (8160793,), dtype: float32\n",
      "Processed audio shape: torch.Size([8160793])\n",
      "Successfully loaded audio at index 341, shape: torch.Size([160000]), label: 249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 343/610 [01:41<02:25,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Motor vehicle (road)_7.wav, shape: (879758,), dtype: float32\n",
      "Processed audio shape: torch.Size([879758])\n",
      "Successfully loaded audio at index 342, shape: torch.Size([160000]), label: 64\n",
      "Successfully loaded audio: audiosets/ontology/Speech_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 343, shape: torch.Size([160000]), label: 423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 345/610 [01:41<01:27,  3.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Wild animals_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 344, shape: torch.Size([160000]), label: 358\n",
      "Successfully loaded audio: audiosets/ontology/Children playing_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 345, shape: torch.Size([160000]), label: 219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 347/610 [01:41<00:59,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Fowl_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 346, shape: torch.Size([160000]), label: 157\n",
      "Successfully loaded audio: audiosets/ontology/Whimper_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 347, shape: torch.Size([160000]), label: 501\n",
      "Successfully loaded audio: audiosets/ontology/Harmonica_6.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 348, shape: torch.Size([160000]), label: 268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 350/610 [01:41<00:48,  5.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Snoring_4.wav, shape: (1373136,), dtype: float32\n",
      "Processed audio shape: torch.Size([1373136])\n",
      "Successfully loaded audio at index 349, shape: torch.Size([160000]), label: 245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 351/610 [01:42<01:15,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Run_3.wav, shape: (6235580,), dtype: float32\n",
      "Processed audio shape: torch.Size([6235580])\n",
      "Successfully loaded audio at index 350, shape: torch.Size([160000]), label: 100\n",
      "Successfully loaded audio: audiosets/ontology/Drum machine_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 351, shape: torch.Size([160000]), label: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 354/610 [01:42<00:47,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Ding-dong_1.wav, shape: (152695,), dtype: float32\n",
      "Processed audio shape: torch.Size([152695])\n",
      "Successfully loaded audio at index 352, shape: torch.Size([160000]), label: 316\n",
      "Successfully loaded audio: audiosets/ontology/Music_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 353, shape: torch.Size([160000]), label: 332\n",
      "Successfully loaded audio: audiosets/ontology/Bird_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 354, shape: torch.Size([160000]), label: 158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 356/610 [01:43<00:51,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Drum roll_3.wav, shape: (2820018,), dtype: float32\n",
      "Processed audio shape: torch.Size([2820018])\n",
      "Successfully loaded audio at index 355, shape: torch.Size([160000]), label: 300\n",
      "Successfully loaded audio: audiosets/ontology/Crow_0.wav, shape: (14312781,), dtype: float32\n",
      "Processed audio shape: torch.Size([14312781])\n",
      "Successfully loaded audio at index 356, shape: torch.Size([160000]), label: 173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 358/610 [01:45<02:02,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Harp_5.wav, shape: (5487526,), dtype: float32\n",
      "Processed audio shape: torch.Size([5487526])\n",
      "Successfully loaded audio at index 357, shape: torch.Size([160000]), label: 42\n",
      "Successfully loaded audio: audiosets/ontology/Filing (rasp)_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 358, shape: torch.Size([160000]), label: 415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 361/610 [01:45<01:03,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Bowed string instrument_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 359, shape: torch.Size([160000]), label: 400\n",
      "Successfully loaded audio: audiosets/ontology/Velcro, hook and loop fastener_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 360, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Ukulele_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 361, shape: torch.Size([160000]), label: 378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 363/610 [01:46<01:29,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Wood_0.wav, shape: (10594304,), dtype: float32\n",
      "Processed audio shape: torch.Size([10594304])\n",
      "Successfully loaded audio at index 362, shape: torch.Size([160000]), label: 291\n",
      "Successfully loaded audio: audiosets/ontology/Tire squeal_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 363, shape: torch.Size([160000]), label: 309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 366/610 [01:46<00:56,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Babbling_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 364, shape: torch.Size([160000]), label: 347\n",
      "Successfully loaded audio: audiosets/ontology/Water tap, faucet_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 365, shape: torch.Size([160000]), label: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 367/610 [01:47<00:56,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Telephone_0.wav, shape: (1669051,), dtype: float32\n",
      "Processed audio shape: torch.Size([1669051])\n",
      "Successfully loaded audio at index 366, shape: torch.Size([160000]), label: 310\n",
      "Successfully loaded audio: audiosets/ontology/Bowed string instrument_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 367, shape: torch.Size([160000]), label: 400\n",
      "Successfully loaded audio: audiosets/ontology/Fireworks_8.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 368, shape: torch.Size([160000]), label: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 370/610 [01:47<00:39,  6.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Synthesizer_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 369, shape: torch.Size([160000]), label: 475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 371/610 [01:47<01:03,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Helicopter_3.wav, shape: (5796444,), dtype: float32\n",
      "Processed audio shape: torch.Size([5796444])\n",
      "Successfully loaded audio at index 370, shape: torch.Size([160000]), label: 10\n",
      "Successfully loaded audio: audiosets/ontology/Wind noise (microphone)_1.wav, shape: (710531,), dtype: float32\n",
      "Processed audio shape: torch.Size([710531])\n",
      "Successfully loaded audio at index 371, shape: torch.Size([160000]), label: 495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 374/610 [01:48<00:41,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Sliding door_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 372, shape: torch.Size([160000]), label: 316\n",
      "Successfully loaded audio: audiosets/ontology/Cowbell_3.wav, shape: (153438,), dtype: float32\n",
      "Processed audio shape: torch.Size([153438])\n",
      "Successfully loaded audio at index 373, shape: torch.Size([160000]), label: 183\n",
      "Successfully loaded audio: audiosets/ontology/Moo_7.wav, shape: (144521,), dtype: float32\n",
      "Processed audio shape: torch.Size([144521])\n",
      "Successfully loaded audio at index 374, shape: torch.Size([160000]), label: 156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 377/610 [01:48<00:31,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Canidae, dogs, wolves_7.wav, shape: (381365,), dtype: float32\n",
      "Processed audio shape: torch.Size([381365])\n",
      "Successfully loaded audio at index 375, shape: torch.Size([160000]), label: 432\n",
      "Successfully loaded audio: audiosets/ontology/Keys jangling_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 376, shape: torch.Size([160000]), label: 285\n",
      "Successfully loaded audio: audiosets/ontology/Plucked string instrument_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 377, shape: torch.Size([160000]), label: 212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 379/610 [01:48<00:27,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Smoke detector, smoke alarm_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 378, shape: torch.Size([160000]), label: 297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 380/610 [01:48<00:33,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Caterwaul_2.wav, shape: (1388182,), dtype: float32\n",
      "Processed audio shape: torch.Size([1388182])\n",
      "Successfully loaded audio at index 379, shape: torch.Size([160000]), label: 345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 381/610 [01:49<00:43,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Chainsaw_7.wav, shape: (2353947,), dtype: float32\n",
      "Processed audio shape: torch.Size([2353947])\n",
      "Successfully loaded audio at index 380, shape: torch.Size([160000]), label: 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 382/610 [01:49<00:46,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Liquid_0.wav, shape: (1492992,), dtype: float32\n",
      "Processed audio shape: torch.Size([1492992])\n",
      "Successfully loaded audio at index 381, shape: torch.Size([160000]), label: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 383/610 [01:49<00:55,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Vehicle_2.wav, shape: (3102186,), dtype: float32\n",
      "Processed audio shape: torch.Size([3102186])\n",
      "Successfully loaded audio at index 382, shape: torch.Size([160000]), label: 198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 384/610 [01:50<01:01,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/String section_0.wav, shape: (2349488,), dtype: float32\n",
      "Processed audio shape: torch.Size([2349488])\n",
      "Successfully loaded audio at index 383, shape: torch.Size([160000]), label: 89\n",
      "Successfully loaded audio: audiosets/ontology/Wind instrument, woodwind instrument_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 384, shape: torch.Size([160000]), label: 296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 386/610 [01:50<01:03,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Run_2.wav, shape: (4358665,), dtype: float32\n",
      "Processed audio shape: torch.Size([4358665])\n",
      "Successfully loaded audio at index 385, shape: torch.Size([160000]), label: 100\n",
      "Successfully loaded audio: audiosets/ontology/Strum_4.wav, shape: (652574,), dtype: float32\n",
      "Processed audio shape: torch.Size([652574])\n",
      "Successfully loaded audio at index 386, shape: torch.Size([160000]), label: 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 388/610 [01:51<00:48,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Train wheels squealing_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 387, shape: torch.Size([160000]), label: 84\n",
      "Successfully loaded audio: audiosets/ontology/Turkey_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 388, shape: torch.Size([160000]), label: 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 391/610 [01:51<00:33,  6.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Bark_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 389, shape: torch.Size([160000]), label: 218\n",
      "Successfully loaded audio: audiosets/ontology/Glockenspiel_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 390, shape: torch.Size([160000]), label: 206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 392/610 [01:51<00:30,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Aircraft engine_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 391, shape: torch.Size([160000]), label: 444\n",
      "Successfully loaded audio: audiosets/ontology/Turkey_0.wav, shape: (155296,), dtype: float32\n",
      "Processed audio shape: torch.Size([155296])\n",
      "Successfully loaded audio at index 392, shape: torch.Size([160000]), label: 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 394/610 [01:51<00:26,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Boat, Water vehicle_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 393, shape: torch.Size([160000]), label: 198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 395/610 [01:52<01:05,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Rain on surface_1.wav, shape: (7360912,), dtype: float32\n",
      "Processed audio shape: torch.Size([7360912])\n",
      "Successfully loaded audio at index 394, shape: torch.Size([160000]), label: 185\n",
      "Successfully loaded audio: audiosets/ontology/Male singing_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 395, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Mechanical fan_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 396, shape: torch.Size([160000]), label: 451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 398/610 [01:53<00:56,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Bicycle_0.wav, shape: (4114948,), dtype: float32\n",
      "Processed audio shape: torch.Size([4114948])\n",
      "Successfully loaded audio at index 397, shape: torch.Size([160000]), label: 439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 399/610 [01:53<01:21,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Traffic noise, roadway noise_2.wav, shape: (7352367,), dtype: float32\n",
      "Processed audio shape: torch.Size([7352367])\n",
      "Successfully loaded audio at index 398, shape: torch.Size([160000]), label: 495\n",
      "Successfully loaded audio: audiosets/ontology/Motor vehicle (road)_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 399, shape: torch.Size([160000]), label: 64\n",
      "Successfully loaded audio: audiosets/ontology/Door_5.wav, shape: (23576056,), dtype: float32\n",
      "Processed audio shape: torch.Size([23576056])\n",
      "Successfully loaded audio at index 400, shape: torch.Size([160000]), label: 217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 402/610 [01:56<02:02,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Boat, Water vehicle_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 401, shape: torch.Size([160000]), label: 198\n",
      "Successfully loaded audio: audiosets/ontology/Mallet percussion_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 402, shape: torch.Size([160000]), label: 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 404/610 [01:56<01:18,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Mosquito_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 403, shape: torch.Size([160000]), label: 131\n",
      "Successfully loaded audio: audiosets/ontology/Rattle (instrument)_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 404, shape: torch.Size([160000]), label: 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 406/610 [01:57<01:27,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Fireworks_2.wav, shape: (7819738,), dtype: float32\n",
      "Processed audio shape: torch.Size([7819738])\n",
      "Successfully loaded audio at index 405, shape: torch.Size([160000]), label: 109\n",
      "Successfully loaded audio: audiosets/ontology/Chatter_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 406, shape: torch.Size([160000]), label: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 408/610 [01:57<01:01,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Hammond organ_5.wav, shape: (1154125,), dtype: float32\n",
      "Processed audio shape: torch.Size([1154125])\n",
      "Successfully loaded audio at index 407, shape: torch.Size([160000]), label: 7\n",
      "Successfully loaded audio: audiosets/ontology/Cowbell_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 408, shape: torch.Size([160000]), label: 355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 410/610 [01:58<00:52,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Rain_5.wav, shape: (1734624,), dtype: float32\n",
      "Processed audio shape: torch.Size([1734624])\n",
      "Successfully loaded audio at index 409, shape: torch.Size([160000]), label: 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 411/610 [01:58<00:51,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Wind noise (microphone)_2.wav, shape: (1603478,), dtype: float32\n",
      "Processed audio shape: torch.Size([1603478])\n",
      "Successfully loaded audio at index 410, shape: torch.Size([160000]), label: 495\n",
      "Successfully loaded audio: audiosets/ontology/Boat, Water vehicle_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 411, shape: torch.Size([160000]), label: 341\n",
      "Successfully loaded audio: audiosets/ontology/Wind_8.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 412, shape: torch.Size([160000]), label: 296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 413/610 [01:58<00:35,  5.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Rowboat, canoe, kayak_2.wav, shape: (150466,), dtype: float32\n",
      "Processed audio shape: torch.Size([150466])\n",
      "Successfully loaded audio at index 413, shape: torch.Size([160000]), label: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 415/610 [01:58<00:34,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Pigeon, dove_4.wav, shape: (1682797,), dtype: float32\n",
      "Processed audio shape: torch.Size([1682797])\n",
      "Successfully loaded audio at index 414, shape: torch.Size([160000]), label: 137\n",
      "Successfully loaded audio: audiosets/ontology/Clock_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 415, shape: torch.Size([160000]), label: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 417/610 [01:59<01:05,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Chewing, mastication_2.wav, shape: (9673991,), dtype: float32\n",
      "Processed audio shape: torch.Size([9673991])\n",
      "Successfully loaded audio at index 416, shape: torch.Size([160000]), label: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 418/610 [02:00<01:29,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Jet engine_1.wav, shape: (8009213,), dtype: float32\n",
      "Processed audio shape: torch.Size([8009213])\n",
      "Successfully loaded audio at index 417, shape: torch.Size([160000]), label: 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 419/610 [02:00<01:12,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Church bell_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 418, shape: torch.Size([160000]), label: 242\n",
      "Successfully loaded audio: audiosets/ontology/Chatter_5.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 419, shape: torch.Size([160000]), label: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 421/610 [02:01<00:56,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Pigeon, dove_2.wav, shape: (1793138,), dtype: float32\n",
      "Processed audio shape: torch.Size([1793138])\n",
      "Successfully loaded audio at index 420, shape: torch.Size([160000]), label: 46\n",
      "Successfully loaded audio: audiosets/ontology/Hiss_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 421, shape: torch.Size([160000]), label: 365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 423/610 [02:01<00:38,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Cat_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 422, shape: torch.Size([160000]), label: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 424/610 [02:01<00:46,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Fire alarm_3.wav, shape: (2761875,), dtype: float32\n",
      "Processed audio shape: torch.Size([2761875])\n",
      "Successfully loaded audio at index 423, shape: torch.Size([160000]), label: 297\n",
      "Successfully loaded audio: audiosets/ontology/Children shouting_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 424, shape: torch.Size([160000]), label: 11\n",
      "Successfully loaded audio: audiosets/ontology/Camera_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 425, shape: torch.Size([160000]), label: 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 428/610 [02:02<00:25,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Motor vehicle (road)_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 426, shape: torch.Size([160000]), label: 198\n",
      "Successfully loaded audio: audiosets/ontology/Whimper (dog)_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 427, shape: torch.Size([160000]), label: 432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 429/610 [02:02<00:45,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Fire_3.wav, shape: (6766110,), dtype: float32\n",
      "Processed audio shape: torch.Size([6766110])\n",
      "Successfully loaded audio at index 428, shape: torch.Size([160000]), label: 60\n",
      "Successfully loaded audio: audiosets/ontology/Sneeze_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 429, shape: torch.Size([160000]), label: 145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 431/610 [02:03<00:43,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Zither_2.wav, shape: (2385408,), dtype: float32\n",
      "Processed audio shape: torch.Size([2385408])\n",
      "Successfully loaded audio at index 430, shape: torch.Size([160000]), label: 138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 432/610 [02:03<00:50,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Tools_1.wav, shape: (3493768,), dtype: float32\n",
      "Processed audio shape: torch.Size([3493768])\n",
      "Successfully loaded audio at index 431, shape: torch.Size([160000]), label: 202\n",
      "Successfully loaded audio: audiosets/ontology/Smoke detector, smoke alarm_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 432, shape: torch.Size([160000]), label: 446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 434/610 [02:04<00:44,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Snare drum_8.wav, shape: (2638902,), dtype: float32\n",
      "Processed audio shape: torch.Size([2638902])\n",
      "Successfully loaded audio at index 433, shape: torch.Size([160000]), label: 312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 435/610 [02:04<01:04,  2.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Thunder_8.wav, shape: (6799918,), dtype: float32\n",
      "Processed audio shape: torch.Size([6799918])\n",
      "Successfully loaded audio at index 434, shape: torch.Size([160000]), label: 228\n",
      "Successfully loaded audio: audiosets/ontology/Applause_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 435, shape: torch.Size([160000]), label: 115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 437/610 [02:05<00:44,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Ringtone_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 436, shape: torch.Size([160000]), label: 292\n",
      "Successfully loaded audio: audiosets/ontology/Caw_2.wav, shape: (155296,), dtype: float32\n",
      "Processed audio shape: torch.Size([155296])\n",
      "Successfully loaded audio at index 437, shape: torch.Size([160000]), label: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 439/610 [02:05<00:43,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Burst, pop_1.wav, shape: (2504040,), dtype: float32\n",
      "Processed audio shape: torch.Size([2504040])\n",
      "Successfully loaded audio at index 438, shape: torch.Size([160000]), label: 73\n",
      "Successfully loaded audio: audiosets/ontology/Bathtub (filling or washing)_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 439, shape: torch.Size([160000]), label: 463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 440/610 [02:05<00:35,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Tick-tock_4.wav, shape: (153438,), dtype: float32\n",
      "Processed audio shape: torch.Size([153438])\n",
      "Successfully loaded audio at index 440, shape: torch.Size([160000]), label: 324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 442/610 [02:06<00:34,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Wind_4.wav, shape: (1765832,), dtype: float32\n",
      "Processed audio shape: torch.Size([1765832])\n",
      "Successfully loaded audio at index 441, shape: torch.Size([160000]), label: 296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 443/610 [02:06<00:37,  4.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Heart sounds, heartbeat_0.wav, shape: (2064347,), dtype: float32\n",
      "Processed audio shape: torch.Size([2064347])\n",
      "Successfully loaded audio at index 442, shape: torch.Size([160000]), label: 469\n",
      "Successfully loaded audio: audiosets/ontology/Insect_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 443, shape: torch.Size([160000]), label: 485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 446/610 [02:06<00:24,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Drum kit_8.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 444, shape: torch.Size([160000]), label: 300\n",
      "Successfully loaded audio: audiosets/ontology/Wind noise (microphone)_5.wav, shape: (1042112,), dtype: float32\n",
      "Processed audio shape: torch.Size([1042112])\n",
      "Successfully loaded audio at index 445, shape: torch.Size([160000]), label: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 447/610 [02:06<00:25,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Cat_2.wav, shape: (991585,), dtype: float32\n",
      "Processed audio shape: torch.Size([991585])\n",
      "Successfully loaded audio at index 446, shape: torch.Size([160000]), label: 48\n",
      "Successfully loaded audio: audiosets/ontology/Harmonica_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 447, shape: torch.Size([160000]), label: 118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 449/610 [02:07<00:24,  6.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Electronic organ_0.wav, shape: (1603663,), dtype: float32\n",
      "Processed audio shape: torch.Size([1603663])\n",
      "Successfully loaded audio at index 448, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Bathtub (filling or washing)_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 449, shape: torch.Size([160000]), label: 463\n",
      "Successfully loaded audio: audiosets/ontology/Brass instrument_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 450, shape: torch.Size([160000]), label: 417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 452/610 [02:07<00:19,  7.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Zipper (clothing)_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 451, shape: torch.Size([160000]), label: 375\n",
      "Successfully loaded audio: audiosets/ontology/Babbling_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 452, shape: torch.Size([160000]), label: 347\n",
      "Successfully loaded audio: audiosets/ontology/Burping, eructation_6.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 453, shape: torch.Size([160000]), label: 249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 454/610 [02:07<00:17,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Car alarm_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 454, shape: torch.Size([160000]), label: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 456/610 [02:08<00:40,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Firecracker_5.wav, shape: (10004139,), dtype: float32\n",
      "Processed audio shape: torch.Size([10004139])\n",
      "Successfully loaded audio at index 455, shape: torch.Size([160000]), label: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 458/610 [02:08<00:32,  4.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Caw_4.wav, shape: (997901,), dtype: float32\n",
      "Processed audio shape: torch.Size([997901])\n",
      "Successfully loaded audio at index 456, shape: torch.Size([160000]), label: 22\n",
      "Successfully loaded audio: audiosets/ontology/Child speech, kid speaking_7.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 457, shape: torch.Size([160000]), label: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 459/610 [02:09<00:28,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Railroad car, train wagon_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 458, shape: torch.Size([160000]), label: 84\n",
      "Successfully loaded audio: audiosets/ontology/Buzz_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 459, shape: torch.Size([160000]), label: 254\n",
      "Successfully loaded audio: audiosets/ontology/Chime_3.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 460, shape: torch.Size([160000]), label: 510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 462/610 [02:09<00:23,  6.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Sawing_3.wav, shape: (1174930,), dtype: float32\n",
      "Processed audio shape: torch.Size([1174930])\n",
      "Successfully loaded audio at index 461, shape: torch.Size([160000]), label: 299\n",
      "Successfully loaded audio: audiosets/ontology/Steam whistle_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 462, shape: torch.Size([160000]), label: 188\n",
      "Successfully loaded audio: audiosets/ontology/Railroad car, train wagon_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 463, shape: torch.Size([160000]), label: 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 466/610 [02:09<00:16,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Train wheels squealing_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 464, shape: torch.Size([160000]), label: 13\n",
      "Successfully loaded audio: audiosets/ontology/Snort_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 465, shape: torch.Size([160000]), label: 421\n",
      "Successfully loaded audio: audiosets/ontology/Frog_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 466, shape: torch.Size([160000]), label: 497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 468/610 [02:09<00:15,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Tapping (guitar technique)_0.wav, shape: (427248,), dtype: float32\n",
      "Processed audio shape: torch.Size([427248])\n",
      "Successfully loaded audio at index 467, shape: torch.Size([160000]), label: 168\n",
      "Successfully loaded audio: audiosets/ontology/Tapping (guitar technique)_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 468, shape: torch.Size([160000]), label: 168\n",
      "Successfully loaded audio: audiosets/ontology/Singing bowl_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 469, shape: torch.Size([160000]), label: 157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 472/610 [02:10<00:13, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Ratchet, pawl_3.wav, shape: (87308,), dtype: float32\n",
      "Processed audio shape: torch.Size([87308])\n",
      "Successfully loaded audio at index 470, shape: torch.Size([160000]), label: 200\n",
      "Successfully loaded audio: audiosets/ontology/Tinnitus, ringing in the ears_3.wav, shape: (939015,), dtype: float32\n",
      "Processed audio shape: torch.Size([939015])\n",
      "Successfully loaded audio at index 471, shape: torch.Size([160000]), label: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 474/610 [02:10<00:15,  8.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Bark_4.wav, shape: (1120131,), dtype: float32\n",
      "Processed audio shape: torch.Size([1120131])\n",
      "Successfully loaded audio at index 472, shape: torch.Size([160000]), label: 218\n",
      "Successfully loaded audio: audiosets/ontology/Double bass_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 473, shape: torch.Size([160000]), label: 23\n",
      "Successfully loaded audio: audiosets/ontology/Emergency vehicle_8.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 474, shape: torch.Size([160000]), label: 198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 478/610 [02:10<00:12, 10.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Snap_3.wav, shape: (136534,), dtype: float32\n",
      "Processed audio shape: torch.Size([136534])\n",
      "Successfully loaded audio at index 475, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Aircraft engine_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 476, shape: torch.Size([160000]), label: 444\n",
      "Successfully loaded audio: audiosets/ontology/Belly laugh_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 477, shape: torch.Size([160000]), label: 183\n",
      "Successfully loaded audio: audiosets/ontology/Goat_4.wav, shape: (8206861,), dtype: float32\n",
      "Processed audio shape: torch.Size([8206861])\n",
      "Successfully loaded audio at index 478, shape: torch.Size([160000]), label: 361\n",
      "Successfully loaded audio: audiosets/ontology/Plucked string instrument_6.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 479, shape: torch.Size([160000]), label: 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 481/610 [02:11<00:22,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Tapping (guitar technique)_0.wav, shape: (427248,), dtype: float32\n",
      "Processed audio shape: torch.Size([427248])\n",
      "Successfully loaded audio at index 480, shape: torch.Size([160000]), label: 391\n",
      "Successfully loaded audio: audiosets/ontology/Air horn, truck horn_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 481, shape: torch.Size([160000]), label: 404\n",
      "Successfully loaded audio: audiosets/ontology/Moo_6.wav, shape: (258021,), dtype: float32\n",
      "Processed audio shape: torch.Size([258021])\n",
      "Successfully loaded audio at index 482, shape: torch.Size([160000]), label: 156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 483/610 [02:12<00:18,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Aircraft engine_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 483, shape: torch.Size([160000]), label: 110\n",
      "Successfully loaded audio: audiosets/ontology/Air brake_0.wav, shape: (605948,), dtype: float32\n",
      "Processed audio shape: torch.Size([605948])\n",
      "Successfully loaded audio at index 484, shape: torch.Size([160000]), label: 303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 487/610 [02:12<00:14,  8.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Sailboat, sailing ship_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 485, shape: torch.Size([160000]), label: 258\n",
      "Successfully loaded audio: audiosets/ontology/Tick_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 486, shape: torch.Size([160000]), label: 478\n",
      "Successfully loaded audio: audiosets/ontology/Fire engine, fire truck (siren)_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 487, shape: torch.Size([160000]), label: 343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 489/610 [02:13<00:21,  5.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Animal_2.wav, shape: (4620400,), dtype: float32\n",
      "Processed audio shape: torch.Size([4620400])\n",
      "Successfully loaded audio at index 488, shape: torch.Size([160000]), label: 358\n",
      "Successfully loaded audio: audiosets/ontology/Stomach rumble_1.wav, shape: (341241,), dtype: float32\n",
      "Processed audio shape: torch.Size([341241])\n",
      "Successfully loaded audio at index 489, shape: torch.Size([160000]), label: 392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 492/610 [02:13<00:15,  7.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Whispering_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 490, shape: torch.Size([160000]), label: 233\n",
      "Successfully loaded audio: audiosets/ontology/Fire alarm_0.wav, shape: (158268,), dtype: float32\n",
      "Processed audio shape: torch.Size([158268])\n",
      "Successfully loaded audio at index 491, shape: torch.Size([160000]), label: 297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 494/610 [02:13<00:14,  7.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Wind chime_2.wav, shape: (532573,), dtype: float32\n",
      "Processed audio shape: torch.Size([532573])\n",
      "Successfully loaded audio at index 492, shape: torch.Size([160000]), label: 296\n",
      "Successfully loaded audio: audiosets/ontology/Smoke detector, smoke alarm_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 493, shape: torch.Size([160000]), label: 446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 495/610 [02:13<00:16,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Squawk_0.wav, shape: (1086137,), dtype: float32\n",
      "Processed audio shape: torch.Size([1086137])\n",
      "Successfully loaded audio at index 494, shape: torch.Size([160000]), label: 122\n",
      "Successfully loaded audio: audiosets/ontology/Aircraft_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 495, shape: torch.Size([160000]), label: 444\n",
      "Successfully loaded audio: audiosets/ontology/Cough_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 496, shape: torch.Size([160000]), label: 499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 499/610 [02:13<00:10, 10.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Bowed string instrument_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 497, shape: torch.Size([160000]), label: 142\n",
      "Successfully loaded audio: audiosets/ontology/Male singing_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 498, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Engine_4.wav, shape: (2268160,), dtype: float32\n",
      "Processed audio shape: torch.Size([2268160])\n",
      "Successfully loaded audio at index 499, shape: torch.Size([160000]), label: 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 501/610 [02:15<00:27,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Electric guitar_1.wav, shape: (9293369,), dtype: float32\n",
      "Processed audio shape: torch.Size([9293369])\n",
      "Successfully loaded audio at index 500, shape: torch.Size([160000]), label: 66\n",
      "Successfully loaded audio: audiosets/ontology/Velcro, hook and loop fastener_1.wav, shape: (488548,), dtype: float32\n",
      "Processed audio shape: torch.Size([488548])\n",
      "Successfully loaded audio at index 501, shape: torch.Size([160000]), label: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 503/610 [02:15<00:21,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Thunderstorm_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 502, shape: torch.Size([160000]), label: 228\n",
      "Successfully loaded audio: audiosets/ontology/Toot_3.wav, shape: (4860401,), dtype: float32\n",
      "Processed audio shape: torch.Size([4860401])\n",
      "Successfully loaded audio at index 503, shape: torch.Size([160000]), label: 295\n",
      "Successfully loaded audio: audiosets/ontology/Screaming_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 504, shape: torch.Size([160000]), label: 462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 506/610 [02:16<00:24,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Rimshot_7.wav, shape: (1967938,), dtype: float32\n",
      "Processed audio shape: torch.Size([1967938])\n",
      "Successfully loaded audio at index 505, shape: torch.Size([160000]), label: 283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 507/610 [02:16<00:26,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Drum kit_7.wav, shape: (2799770,), dtype: float32\n",
      "Processed audio shape: torch.Size([2799770])\n",
      "Successfully loaded audio at index 506, shape: torch.Size([160000]), label: 305\n",
      "Successfully loaded audio: audiosets/ontology/Chatter_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 507, shape: torch.Size([160000]), label: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 509/610 [02:16<00:20,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Synthetic singing_1.wav, shape: (732637,), dtype: float32\n",
      "Processed audio shape: torch.Size([732637])\n",
      "Successfully loaded audio at index 508, shape: torch.Size([160000]), label: 181\n",
      "Successfully loaded audio: audiosets/ontology/Cupboard open or close_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 509, shape: torch.Size([160000]), label: 272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 511/610 [02:17<00:32,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Chirp, tweet_2.wav, shape: (8713242,), dtype: float32\n",
      "Processed audio shape: torch.Size([8713242])\n",
      "Successfully loaded audio at index 510, shape: torch.Size([160000]), label: 203\n",
      "Successfully loaded audio: audiosets/ontology/Train horn_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 511, shape: torch.Size([160000]), label: 431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 513/610 [02:17<00:22,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Ocean_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 512, shape: torch.Size([160000]), label: 294\n",
      "Successfully loaded audio: audiosets/ontology/Quack_0.wav, shape: (145636,), dtype: float32\n",
      "Processed audio shape: torch.Size([145636])\n",
      "Successfully loaded audio at index 513, shape: torch.Size([160000]), label: 394\n",
      "Successfully loaded audio: audiosets/ontology/Dog_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 514, shape: torch.Size([160000]), label: 432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 515/610 [02:17<00:17,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Buzz_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 515, shape: torch.Size([160000]), label: 254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 517/610 [02:18<00:19,  4.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Harmonica_0.wav, shape: (3670016,), dtype: float32\n",
      "Processed audio shape: torch.Size([3670016])\n",
      "Successfully loaded audio at index 516, shape: torch.Size([160000]), label: 118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 518/610 [02:18<00:22,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Fire alarm_5.wav, shape: (3732097,), dtype: float32\n",
      "Processed audio shape: torch.Size([3732097])\n",
      "Successfully loaded audio at index 517, shape: torch.Size([160000]), label: 60\n",
      "Successfully loaded audio: audiosets/ontology/Rodents, rats, mice_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 518, shape: torch.Size([160000]), label: 442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 520/610 [02:19<00:21,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Ambulance (siren)_5.wav, shape: (2266082,), dtype: float32\n",
      "Processed audio shape: torch.Size([2266082])\n",
      "Successfully loaded audio at index 519, shape: torch.Size([160000]), label: 190\n",
      "Successfully loaded audio: audiosets/ontology/Skidding_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 520, shape: torch.Size([160000]), label: 209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 524/610 [02:19<00:11,  7.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Computer keyboard_6.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 521, shape: torch.Size([160000]), label: 1\n",
      "Successfully loaded audio: audiosets/ontology/Howl (wind)_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 522, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Drum roll_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 523, shape: torch.Size([160000]), label: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 526/610 [02:20<00:13,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Burping, eructation_5.wav, shape: (2691658,), dtype: float32\n",
      "Processed audio shape: torch.Size([2691658])\n",
      "Successfully loaded audio at index 524, shape: torch.Size([160000]), label: 265\n",
      "Successfully loaded audio: audiosets/ontology/Baby laughter_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 525, shape: torch.Size([160000]), label: 487\n",
      "Successfully loaded audio: audiosets/ontology/Wind chime_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 526, shape: torch.Size([160000]), label: 239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 528/610 [02:20<00:17,  4.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Squeak_0.wav, shape: (3383241,), dtype: float32\n",
      "Processed audio shape: torch.Size([3383241])\n",
      "Successfully loaded audio at index 527, shape: torch.Size([160000]), label: 372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 529/610 [02:21<00:24,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Chirp, tweet_6.wav, shape: (5031300,), dtype: float32\n",
      "Processed audio shape: torch.Size([5031300])\n",
      "Successfully loaded audio at index 528, shape: torch.Size([160000]), label: 203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 530/610 [02:21<00:32,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Idling_7.wav, shape: (5885795,), dtype: float32\n",
      "Processed audio shape: torch.Size([5885795])\n",
      "Successfully loaded audio at index 529, shape: torch.Size([160000]), label: 275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 531/610 [02:22<00:26,  2.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Hiccup_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 530, shape: torch.Size([160000]), label: 107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 532/610 [02:22<00:25,  3.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Bass guitar_4.wav, shape: (1883789,), dtype: float32\n",
      "Processed audio shape: torch.Size([1883789])\n",
      "Successfully loaded audio at index 531, shape: torch.Size([160000]), label: 168\n",
      "Successfully loaded audio: audiosets/ontology/Speech synthesizer_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 532, shape: torch.Size([160000]), label: 423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 534/610 [02:22<00:25,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Ringtone_1.wav, shape: (4481080,), dtype: float32\n",
      "Processed audio shape: torch.Size([4481080])\n",
      "Successfully loaded audio at index 533, shape: torch.Size([160000]), label: 292\n",
      "Successfully loaded audio: audiosets/ontology/Male speech, man speaking_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 534, shape: torch.Size([160000]), label: 423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 536/610 [02:23<00:16,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Skidding_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 535, shape: torch.Size([160000]), label: 316\n",
      "Successfully loaded audio: audiosets/ontology/Patter_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 536, shape: torch.Size([160000]), label: 220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 538/610 [02:23<00:13,  5.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Rattle_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 537, shape: torch.Size([160000]), label: 418\n",
      "Successfully loaded audio: audiosets/ontology/Hubbub, speech noise, speech babble_1.wav, shape: (15067150,), dtype: float32\n",
      "Processed audio shape: torch.Size([15067150])\n",
      "Successfully loaded audio at index 538, shape: torch.Size([160000]), label: 401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 540/610 [02:25<00:36,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Train whistle_0.wav, shape: (4012966,), dtype: float32\n",
      "Processed audio shape: torch.Size([4012966])\n",
      "Successfully loaded audio at index 539, shape: torch.Size([160000]), label: 431\n",
      "Successfully loaded audio: audiosets/ontology/Domestic animals, pets_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 540, shape: torch.Size([160000]), label: 287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 543/610 [02:25<00:17,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Male speech, man speaking_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 541, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Bellow_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 542, shape: torch.Size([160000]), label: 267\n",
      "Successfully loaded audio: audiosets/ontology/Power tool_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 543, shape: torch.Size([160000]), label: 504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 546/610 [02:25<00:10,  6.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Rapping_9.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 544, shape: torch.Size([160000]), label: 249\n",
      "Successfully loaded audio: audiosets/ontology/Wolf-whistling_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 545, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Bass guitar_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 546, shape: torch.Size([160000]), label: 168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 548/610 [02:26<00:12,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Cattle, bovinae_7.wav, shape: (3383612,), dtype: float32\n",
      "Processed audio shape: torch.Size([3383612])\n",
      "Successfully loaded audio at index 547, shape: torch.Size([160000]), label: 333\n",
      "Successfully loaded audio: audiosets/ontology/Gobble_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 548, shape: torch.Size([160000]), label: 263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 549/610 [02:26<00:10,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Shatter_1.wav, shape: (21579140,), dtype: float32\n",
      "Processed audio shape: torch.Size([21579140])\n",
      "Successfully loaded audio at index 549, shape: torch.Size([160000]), label: 144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 551/610 [02:28<00:31,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Violin, fiddle_4.wav, shape: (160086,), dtype: float32\n",
      "Processed audio shape: torch.Size([160086])\n",
      "Successfully loaded audio at index 550, shape: torch.Size([160000]), label: 367\n",
      "Successfully loaded audio: audiosets/ontology/Chewing, mastication_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 551, shape: torch.Size([160000]), label: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 553/610 [02:29<00:18,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Hammond organ_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 552, shape: torch.Size([160000]), label: 7\n",
      "Successfully loaded audio: audiosets/ontology/Fire engine, fire truck (siren)_7.wav, shape: (14234390,), dtype: float32\n",
      "Processed audio shape: torch.Size([14234390])\n",
      "Successfully loaded audio at index 553, shape: torch.Size([160000]), label: 411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 555/610 [02:30<00:31,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Gunshot, gunfire_4.wav, shape: (3140824,), dtype: float32\n",
      "Processed audio shape: torch.Size([3140824])\n",
      "Successfully loaded audio at index 554, shape: torch.Size([160000]), label: 425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 556/610 [02:31<00:31,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Ambulance (siren)_6.wav, shape: (5362138,), dtype: float32\n",
      "Processed audio shape: torch.Size([5362138])\n",
      "Successfully loaded audio at index 555, shape: torch.Size([160000]), label: 411\n",
      "Successfully loaded audio: audiosets/ontology/Camera_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 556, shape: torch.Size([160000]), label: 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 558/610 [02:31<00:18,  2.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Power windows, electric windows_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 557, shape: torch.Size([160000]), label: 296\n",
      "Successfully loaded audio: audiosets/ontology/Doorbell_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 558, shape: torch.Size([160000]), label: 467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 560/610 [02:31<00:11,  4.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Power tool_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 559, shape: torch.Size([160000]), label: 504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 561/610 [02:33<00:22,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Buzzer_1.wav, shape: (12141808,), dtype: float32\n",
      "Processed audio shape: torch.Size([12141808])\n",
      "Successfully loaded audio at index 560, shape: torch.Size([160000]), label: 514\n",
      "Successfully loaded audio: audiosets/ontology/Bowed string instrument_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 561, shape: torch.Size([160000]), label: 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 563/610 [02:34<00:22,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Drum roll_0.wav, shape: (9470294,), dtype: float32\n",
      "Processed audio shape: torch.Size([9470294])\n",
      "Successfully loaded audio at index 562, shape: torch.Size([160000]), label: 397\n",
      "Successfully loaded audio: audiosets/ontology/Fire engine, fire truck (siren)_7.wav, shape: (14234390,), dtype: float32\n",
      "Processed audio shape: torch.Size([14234390])\n",
      "Successfully loaded audio at index 563, shape: torch.Size([160000]), label: 110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 565/610 [02:35<00:25,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Turkey_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 564, shape: torch.Size([160000]), label: 74\n",
      "Successfully loaded audio: audiosets/ontology/Wind instrument, woodwind instrument_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 565, shape: torch.Size([160000]), label: 440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 567/610 [02:35<00:15,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Choir_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 566, shape: torch.Size([160000]), label: 474\n",
      "Successfully loaded audio: audiosets/ontology/Roaring cats (lions, tigers)_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 567, shape: torch.Size([160000]), label: 48\n",
      "Successfully loaded audio: audiosets/ontology/Timpani_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 568, shape: torch.Size([160000]), label: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 570/610 [02:36<00:08,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Fixed-wing aircraft, airplane_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 569, shape: torch.Size([160000]), label: 424\n",
      "Successfully loaded audio: audiosets/ontology/Percussion_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 570, shape: torch.Size([160000]), label: 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 572/610 [02:36<00:06,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Clip-clop_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 571, shape: torch.Size([160000]), label: 518\n",
      "Successfully loaded audio: audiosets/ontology/Cymbal_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 572, shape: torch.Size([160000]), label: 155\n",
      "Successfully loaded audio: audiosets/ontology/Wind_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 573, shape: torch.Size([160000]), label: 296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 576/610 [02:36<00:03,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Wild animals_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 574, shape: torch.Size([160000]), label: 308\n",
      "Successfully loaded audio: audiosets/ontology/Drill_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 575, shape: torch.Size([160000]), label: 162\n",
      "Successfully loaded audio: audiosets/ontology/Crowing, cock-a-doodle-doo_7.wav, shape: (360003,), dtype: float32\n",
      "Processed audio shape: torch.Size([360003])\n",
      "Successfully loaded audio at index 576, shape: torch.Size([160000]), label: 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 578/610 [02:37<00:08,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Raindrop_1.wav, shape: (9466683,), dtype: float32\n",
      "Processed audio shape: torch.Size([9466683])\n",
      "Successfully loaded audio at index 577, shape: torch.Size([160000]), label: 84\n",
      "Successfully loaded audio: audiosets/ontology/Yawn_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 578, shape: torch.Size([160000]), label: -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 580/610 [02:37<00:05,  5.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Cattle, bovinae_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 579, shape: torch.Size([160000]), label: 48\n",
      "Successfully loaded audio: audiosets/ontology/Steam whistle_2.wav, shape: (1866513,), dtype: float32\n",
      "Processed audio shape: torch.Size([1866513])\n",
      "Successfully loaded audio at index 580, shape: torch.Size([160000]), label: 289\n",
      "Successfully loaded audio: audiosets/ontology/Bass guitar_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 581, shape: torch.Size([160000]), label: 168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 583/610 [02:39<00:09,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Electric guitar_3.wav, shape: (10330280,), dtype: float32\n",
      "Processed audio shape: torch.Size([10330280])\n",
      "Successfully loaded audio at index 582, shape: torch.Size([160000]), label: 66\n",
      "Successfully loaded audio: audiosets/ontology/Keyboard (musical)_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 583, shape: torch.Size([160000]), label: 332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 585/610 [02:40<00:11,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Gargling_3.wav, shape: (9901361,), dtype: float32\n",
      "Processed audio shape: torch.Size([9901361])\n",
      "Successfully loaded audio at index 584, shape: torch.Size([160000]), label: 354\n",
      "Successfully loaded audio: audiosets/ontology/Cello_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 585, shape: torch.Size([160000]), label: 151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 587/610 [02:40<00:07,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Banjo_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 586, shape: torch.Size([160000]), label: 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 588/610 [02:41<00:10,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Squeak_5.wav, shape: (9803094,), dtype: float32\n",
      "Processed audio shape: torch.Size([9803094])\n",
      "Successfully loaded audio at index 587, shape: torch.Size([160000]), label: 372\n",
      "Successfully loaded audio: audiosets/ontology/Brass instrument_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 588, shape: torch.Size([160000]), label: 417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 590/610 [02:41<00:06,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Steam whistle_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 589, shape: torch.Size([160000]), label: 188\n",
      "Successfully loaded audio: audiosets/ontology/Writing_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 590, shape: torch.Size([160000]), label: 161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 592/610 [02:42<00:03,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Firecracker_1.wav, shape: (507496,), dtype: float32\n",
      "Processed audio shape: torch.Size([507496])\n",
      "Successfully loaded audio at index 591, shape: torch.Size([160000]), label: 472\n",
      "Successfully loaded audio: audiosets/ontology/Vibraphone_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 592, shape: torch.Size([160000]), label: 229\n",
      "Successfully loaded audio: audiosets/ontology/Car_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 593, shape: torch.Size([160000]), label: 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 594/610 [02:42<00:02,  6.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Baby cry, infant cry_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 594, shape: torch.Size([160000]), label: 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 596/610 [02:42<00:02,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Acoustic guitar_0.wav, shape: (3946836,), dtype: float32\n",
      "Processed audio shape: torch.Size([3946836])\n",
      "Successfully loaded audio at index 595, shape: torch.Size([160000]), label: 168\n",
      "Successfully loaded audio: audiosets/ontology/Pizzicato_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 596, shape: torch.Size([160000]), label: 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 598/610 [02:43<00:01,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Vibraphone_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 597, shape: torch.Size([160000]), label: 229\n",
      "Successfully loaded audio: audiosets/ontology/Sink (filling or washing)_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 598, shape: torch.Size([160000]), label: 311\n",
      "Successfully loaded audio: audiosets/ontology/Helicopter_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 599, shape: torch.Size([160000]), label: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 602/610 [02:43<00:01,  7.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Ratchet, pawl_1.wav, shape: (157896,), dtype: float32\n",
      "Processed audio shape: torch.Size([157896])\n",
      "Successfully loaded audio at index 600, shape: torch.Size([160000]), label: 200\n",
      "Successfully loaded audio: audiosets/ontology/Tambourine_3.wav, shape: (153809,), dtype: float32\n",
      "Processed audio shape: torch.Size([153809])\n",
      "Successfully loaded audio at index 601, shape: torch.Size([160000]), label: 509\n",
      "Successfully loaded audio: audiosets/ontology/Chainsaw_5.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 602, shape: torch.Size([160000]), label: 132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 604/610 [02:43<00:00,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Change ringing (campanology)_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 603, shape: torch.Size([160000]), label: 114\n",
      "Successfully loaded audio: audiosets/ontology/Fart_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 604, shape: torch.Size([160000]), label: 458\n",
      "Successfully loaded audio: audiosets/ontology/Hoot_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 605, shape: torch.Size([160000]), label: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 608/610 [02:43<00:00,  9.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Sliding door_1.wav, shape: (146008,), dtype: float32\n",
      "Processed audio shape: torch.Size([146008])\n",
      "Successfully loaded audio at index 606, shape: torch.Size([160000]), label: 217\n",
      "Successfully loaded audio: audiosets/ontology/Saxophone_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 607, shape: torch.Size([160000]), label: 34\n",
      "Successfully loaded audio: audiosets/ontology/Typewriter_0.wav, shape: (10036965,), dtype: float32\n",
      "Processed audio shape: torch.Size([10036965])\n",
      "Successfully loaded audio at index 608, shape: torch.Size([160000]), label: 366\n",
      "Successfully loaded audio: audiosets/ontology/Gargling_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 609, shape: torch.Size([160000]), label: 354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 610/610 [02:45<00:00,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.17%\n",
      "F1-Score (weighted): 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm  # Optional: for progress bar\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "import torch\n",
    "import librosa\n",
    "from collections import namedtuple\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Set to block CUDA errors\n",
    "\n",
    "# Load the class map CSV (mapping from mid to index)\n",
    "class_map_df = pd.read_csv('yamnet_class_map.csv')\n",
    "class_map = pd.read_csv('yamnet_class_map.csv').set_index('display_name').to_dict()['mid']\n",
    "\n",
    "# Create a mapping from mid (string) to index (integer)\n",
    "mid_to_index = {mid: idx for idx, mid in enumerate(set(class_map.values()))}\n",
    "\n",
    "# Initialize the model and processor\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained('facebook/wav2vec2-base-960h', num_labels=521)\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Moved model to GPU if available\")\n",
    "\n",
    "\n",
    "# Define a namedtuple for dataset items\n",
    "AudioSample = namedtuple(\"AudioSample\", [\"input_values\", \"labels\"])\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_directory, ontology_file, mid_to_index):\n",
    "        with open(ontology_file, 'r') as f:\n",
    "            self.ontology_data = json.load(f)\n",
    "\n",
    "        self.mid_to_index = mid_to_index\n",
    "        self.audio_directory = audio_directory\n",
    "        self.audio_files = glob.glob(os.path.join(self.audio_directory, '**', '*.wav'), recursive=True)\n",
    "        \n",
    "        # Populate the dataset by calling prepare_data\n",
    "        self.data = self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        data = []\n",
    "        for category in self.ontology_data:\n",
    "            if \"positive_examples\" in category:\n",
    "                category_name = category[\"name\"]\n",
    "                mid = category[\"id\"]  # Get the mid for the current category\n",
    "\n",
    "                # Use the mid to get the index from the mid_to_index\n",
    "                if mid in self.mid_to_index:\n",
    "                    label = self.mid_to_index[mid]  # Get the integer index as the label\n",
    "                else:\n",
    "                    label = -1  # Default to -1 if not found\n",
    "\n",
    "                for audio_file in self.audio_files:\n",
    "                    if category_name.lower() in audio_file.lower():\n",
    "                        audio_file = audio_file.replace(\"\\\\\", \"/\")\n",
    "                        data.append({\"audio\": audio_file, \"label\": label})\n",
    "        return data\n",
    "    \n",
    "    def load_audio(self, file_path):\n",
    "        \"\"\"Load and preprocess audio using Wav2Vec2Processor.\"\"\"\n",
    "        try:\n",
    "            if not os.path.isfile(file_path):\n",
    "                raise FileNotFoundError(f\"WAV file not found: {file_path}\")\n",
    "\n",
    "            # Load audio using librosa and resample to 16kHz\n",
    "            audio_data, sr = librosa.load(file_path, sr=16000)\n",
    "            print(f\"Successfully loaded audio: {file_path}, shape: {audio_data.shape}, dtype: {audio_data.dtype}\")\n",
    "\n",
    "            # Preprocess using Wav2Vec2Processor\n",
    "            inputs = processor(audio_data, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "            processed_audio = inputs.input_values.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "            print(f\"Processed audio shape: {processed_audio.shape}\")\n",
    "\n",
    "            # Return processed audio\n",
    "            return processed_audio\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get one item (audio, label) for the dataset.\"\"\"\n",
    "        sample = self.data[idx]\n",
    "        audio_data = self.load_audio(sample[\"audio\"])\n",
    "        label = sample[\"label\"]\n",
    "\n",
    "        # Ensure audio_data is valid\n",
    "        if audio_data is None:\n",
    "            print(f\"Error loading audio at index {idx}, returning dummy data.\")\n",
    "            return {\"input_values\": torch.zeros(1), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "        # Trim or pad audio to max_length\n",
    "        max_length = 160000  # Set a max_length for padding/truncating\n",
    "        if audio_data.shape[0] < max_length:\n",
    "            padding = torch.zeros(max_length - audio_data.shape[0])\n",
    "            audio_data = torch.cat([audio_data, padding])\n",
    "        else:\n",
    "            audio_data = audio_data[:max_length]\n",
    "\n",
    "        print(f\"Successfully loaded audio at index {idx}, shape: {audio_data.shape}, label: {label}\")\n",
    "        \n",
    "        return {\"input_values\": audio_data.clone().detach(), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Initialize the dataset and dataloaders\n",
    "audio_directory = r\"audiosets/ontology\"\n",
    "ontology_file = 'ontology.json'\n",
    "\n",
    "# Initialize dataset and prepare data\n",
    "dataset = AudioDataset(audio_directory, ontology_file, mid_to_index)\n",
    "\n",
    "# Now split the dataset into train and test sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(dataset.data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize train and test datasets using the split data\n",
    "train_dataset = AudioDataset(audio_directory, ontology_file, mid_to_index)\n",
    "test_dataset = AudioDataset(audio_directory, ontology_file, mid_to_index)\n",
    "\n",
    "# Assign the split data to the datasets\n",
    "train_dataset.data = train_data\n",
    "test_dataset.data = test_data\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    save_steps=10,\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Initialize train and test dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model(model, test_dataset, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    # Iterate over test dataset\n",
    "    with torch.no_grad():  # Disable gradient calculation during evaluation\n",
    "        for batch in tqdm(test_dataset, desc=\"Evaluating\"):\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Skip batches where label is -1\n",
    "            valid_indices = labels != -1\n",
    "            if valid_indices.sum() == 0:\n",
    "                continue  # Skip this batch if all labels are -1\n",
    "\n",
    "            # Only select valid indices\n",
    "            input_values = input_values[valid_indices]\n",
    "            labels = labels[valid_indices]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_values)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get predictions (highest logits)\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Append to the lists\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predictions.cpu().numpy())\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')  # You can change the average type ('micro', 'macro', 'weighted')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"F1-Score (weighted): {f1:.4f}\")\n",
    "    \n",
    "    return accuracy, f1\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "accuracy, f1 = evaluate_model(model, test_dataset, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class index: 417\n",
      "Predicted class name: /m/01kcd\n",
      "Display name for /m/01kcd: Brass instrument\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained model and processor\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained('facebook/wav2vec2-base-960h', num_labels=521)\n",
    "processor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def predict_audio(file_path):\n",
    "    \"\"\"Predict the class of an audio file.\"\"\"\n",
    "    \n",
    "    # Step 1: Load the audio file\n",
    "    audio_data, sr = librosa.load(file_path, sr=16000)  # Ensure the sample rate is 16kHz as required by the model\n",
    "    \n",
    "    # Step 2: Preprocess audio with the Wav2Vec2 processor\n",
    "    inputs = processor(audio_data, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    input_values = inputs.input_values.to(device)\n",
    "\n",
    "    # Step 3: Perform inference\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Get the model's output (logits)\n",
    "        outputs = model(input_values)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Step 4: Get the predicted class (index of highest logit)\n",
    "    predicted_class_idx = torch.argmax(logits, dim=-1).item()\n",
    "\n",
    "    # You can map the predicted index to a class label if needed\n",
    "    print(f\"Predicted class index: {predicted_class_idx}\")\n",
    "    \n",
    "    return predicted_class_idx\n",
    "\n",
    "# Example usage\n",
    "file_path = \"test.wav\"\n",
    "predicted_class_idx = predict_audio(file_path)\n",
    "\n",
    "# Assuming mid_to_index is the original mapping from class name (or MID) to index\n",
    "# Reverse the mapping to go from index to class name\n",
    "index_to_mid = {v: k for k, v in mid_to_index.items()}\n",
    "\n",
    "# Get the class name from the predicted class index\n",
    "predicted_mid_name = index_to_mid.get(predicted_class_idx, \"Unknown\")\n",
    "print(f\"Predicted class name: {predicted_mid_name}\")\n",
    "\n",
    "# Load the class map CSV (mapping from mid to index)\n",
    "# Load the class map CSV (mapping from mid to display_name)\n",
    "class_map_df = pd.read_csv('yamnet_class_map.csv')\n",
    "\n",
    "# Create a mapping from 'mid' to 'display_name'\n",
    "mid_to_display_name = class_map_df.set_index('mid')['display_name'].to_dict()\n",
    "\n",
    "# Example: Get the display name from a given mid\n",
    "\n",
    "display_name = mid_to_display_name.get(predicted_m  id_name, \"Unknown_mid\")\n",
    "print(f\"Display name for {predicted_mid_name}: {display_name}\")\n",
    "class_map = pd.read_csv('yamnet_class_map.csv').set_index('display_name').to_dict()['mid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1851\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1849\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1851\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1852\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1863\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1863\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1864\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1865\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1866\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1867\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1868\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:42\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Type, Union\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     43\u001b[0m     get_reporting_integration_callbacks,\n\u001b[0;32m     44\u001b[0m )\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# isort: on\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhf_hub_utils\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1851\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1849\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1851\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1852\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1863\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1863\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1864\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1865\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1866\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1867\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1868\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:36\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, TFPreTrainedModel\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m version\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     39\u001b[0m     PushToHubMixin,\n\u001b[0;32m     40\u001b[0m     flatten_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     logging,\n\u001b[0;32m     46\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1851\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1849\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1851\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1852\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1863\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1861\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1863\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1864\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1865\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1866\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1867\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1868\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\__internal__\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\__internal__\\backend\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _initialize_variables \u001b[38;5;28;01mas\u001b[39;00m initialize_variables\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m track_variable\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\__init__.py:21\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the TF-Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\applications\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras Applications are premade architectures with pre-trained weights.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtBase\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtLarge\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConvNeXtSmall\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\applications\\convnext.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m initializers\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_coordinator_utils \u001b[38;5;28;01mas\u001b[39;00m dc\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtensor_api \u001b[38;5;28;01mas\u001b[39;00m dtensor\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_tensor\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow_util\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m object_identity\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\engine\\keras_tensor.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras Input Tensor used to track functional API Topology.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m object_identity\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structure\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\utils\\__init__.py:53\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_file\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Preprocessing utils\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_space\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeatureSpace\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Internal\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_source_inputs\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\utils\\feature_space.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_lib\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization_lib\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer.py:43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_precision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m policy\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization_lib\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layer_serialization\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generic_utils\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layer_utils\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\saving\\legacy\\saved_model\\layer_serialization.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_serialization\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_impl\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialized_attributes\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLayerSavedModelSaver\u001b[39;00m(base_serialization\u001b[38;5;241m.\u001b[39mSavedModelSaver):\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\saving\\legacy\\saved_model\\save_impl.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_spec\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmixed_precision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autocast_variable\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_utils\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constants\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load \u001b[38;5;28;01mas\u001b[39;00m keras_load\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\saving\\legacy\\saving_utils.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m losses\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizers\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer_utils\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976\u001b[0m\n\u001b[0;32m   2969\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m identifier\n\u001b[0;32m   2970\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2971\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret loss function identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2972\u001b[0m     )\n\u001b[0;32m   2975\u001b[0m LABEL_DTYPES_FOR_LOSSES \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m-> 2976\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_softmax_cross_entropy\u001b[49m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2977\u001b[0m     sparse_categorical_crossentropy: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2978\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\module_wrapper.py:213\u001b[0m, in \u001b[0;36mTFModuleWrapper._getattribute\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    208\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m attr\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Print deprecations, only cache functions after deprecation warnings have\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# stopped.\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfmw_print_deprecation_warnings \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tfmw_add_deprecation_warning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m    214\u001b[0m   func__fastdict_insert(name, attr)\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attr\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\module_wrapper.py:146\u001b[0m, in \u001b[0;36mTFModuleWrapper._tfmw_add_deprecation_warning\u001b[1;34m(self, name, attr)\u001b[0m\n\u001b[0;32m    144\u001b[0m rename \u001b[38;5;241m=\u001b[39m get_rename_v2(full_name)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rename \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_deprecation_decorator(attr):\n\u001b[1;32m--> 146\u001b[0m   call_location \u001b[38;5;241m=\u001b[39m \u001b[43m_call_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m   \u001b[38;5;66;03m# skip locations in Python source\u001b[39;00m\n\u001b[0;32m    148\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m call_location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\module_wrapper.py:44\u001b[0m, in \u001b[0;36m_call_location\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_location\u001b[39m():\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Extracts the caller filename and line number as a string.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    A string describing the caller source location.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m   frame \u001b[38;5;241m=\u001b[39m \u001b[43mtf_inspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrentframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m frame\u001b[38;5;241m.\u001b[39mf_back\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_tfmw_add_deprecation_warning\u001b[39m\u001b[38;5;124m'\u001b[39m, (\n\u001b[0;32m     46\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis function should be called directly from \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     47\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_tfmw_add_deprecation_warning, as the caller is identified \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     48\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheuristically by chopping off the top stack frames.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     50\u001b[0m   \u001b[38;5;66;03m# We want to get stack frame 3 frames up from current frame,\u001b[39;00m\n\u001b[0;32m     51\u001b[0m   \u001b[38;5;66;03m# i.e. above __getattr__, _tfmw_add_deprecation_warning,\u001b[39;00m\n\u001b[0;32m     52\u001b[0m   \u001b[38;5;66;03m# and _call_location calls.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py:123\u001b[0m, in \u001b[0;36mcurrentframe\u001b[1;34m()\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcurrentframe\u001b[39m():\n\u001b[0;32m    122\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"TFDecorator-aware replacement for inspect.currentframe.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_inspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:1777\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(context)\u001b[0m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack\u001b[39m(context\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1777\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetouterframes\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getframe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:1752\u001b[0m, in \u001b[0;36mgetouterframes\u001b[1;34m(frame, context)\u001b[0m\n\u001b[0;32m   1750\u001b[0m framelist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m frame:\n\u001b[1;32m-> 1752\u001b[0m     traceback_info \u001b[38;5;241m=\u001b[39m \u001b[43mgetframeinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1753\u001b[0m     frameinfo \u001b[38;5;241m=\u001b[39m (frame,) \u001b[38;5;241m+\u001b[39m traceback_info\n\u001b[0;32m   1754\u001b[0m     framelist\u001b[38;5;241m.\u001b[39mappend(FrameInfo(\u001b[38;5;241m*\u001b[39mframeinfo, positions\u001b[38;5;241m=\u001b[39mtraceback_info\u001b[38;5;241m.\u001b[39mpositions))\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:1714\u001b[0m, in \u001b[0;36mgetframeinfo\u001b[1;34m(frame, context)\u001b[0m\n\u001b[0;32m   1712\u001b[0m start \u001b[38;5;241m=\u001b[39m lineno \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m context\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m   1713\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1714\u001b[0m     lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1716\u001b[0m     lines \u001b[38;5;241m=\u001b[39m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:1090\u001b[0m, in \u001b[0;36mfindsource\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m   1087\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (file\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m   1088\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource code not available\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1090\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mgetmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module:\n\u001b[0;32m   1092\u001b[0m     lines \u001b[38;5;241m=\u001b[39m linecache\u001b[38;5;241m.\u001b[39mgetlines(file, module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:1013\u001b[0m, in \u001b[0;36mgetmodule\u001b[1;34m(object, _filename)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m _filesbymodname[modname] \u001b[38;5;241m=\u001b[39m f\n\u001b[1;32m-> 1013\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mgetabsfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;66;03m# Always map to the name the module knows itself by\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m modulesbyfile[f] \u001b[38;5;241m=\u001b[39m modulesbyfile[\n\u001b[0;32m   1016\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(f)] \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:982\u001b[0m, in \u001b[0;36mgetabsfile\u001b[1;34m(object, _filename)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return an absolute path to the source or compiled file for an object.\u001b[39;00m\n\u001b[0;32m    978\u001b[0m \n\u001b[0;32m    979\u001b[0m \u001b[38;5;124;03mThe idea is for each object to have a unique origin, so this routine\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;124;03mnormalizes the result as much as possible.\"\"\"\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 982\u001b[0m     _filename \u001b[38;5;241m=\u001b[39m \u001b[43mgetsourcefile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m getfile(\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormcase(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(_filename))\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\inspect.py:967\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m linecache\u001b[38;5;241m.\u001b[39mcache:\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filename\n\u001b[0;32m    969\u001b[0m \u001b[38;5;66;03m# only return a non-existent filename if the module has a PEP 302 loader\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "import torch\n",
    "import librosa\n",
    "#from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import namedtuple\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Load the class map CSV (mapping from mid to index)\n",
    "class_map_df = pd.read_csv('yamnet_class_map.csv')\n",
    "class_map = pd.read_csv('yamnet_class_map.csv').set_index('display_name').to_dict()['mid']\n",
    "\n",
    "# Create a mapping from mid (string) to index (integer)\n",
    "mid_to_index = {mid: idx for idx, mid in enumerate(set(class_map.values()))}\n",
    "\n",
    "# Initialize the model and processor\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained('facebook/wav2vec2-base-960h', num_labels=521)\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Moved model to GPU if available\")\n",
    "\n",
    "# Define a namedtuple for dataset items\n",
    "AudioSample = namedtuple(\"AudioSample\", [\"input_values\", \"labels\"])\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_directory, ontology_file, mid_to_index):\n",
    "        with open(ontology_file, 'r') as f:\n",
    "            self.ontology_data = json.load(f)\n",
    "\n",
    "        self.mid_to_index = mid_to_index\n",
    "        self.audio_directory = audio_directory\n",
    "        self.audio_files = glob.glob(os.path.join(self.audio_directory, '**', '*.wav'), recursive=True)\n",
    "        \n",
    "        # Populate the dataset by calling prepare_data\n",
    "        self.data = self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        data = []\n",
    "        for category in self.ontology_data:\n",
    "            if \"positive_examples\" in category:\n",
    "                category_name = category[\"name\"]\n",
    "                mid = category[\"id\"]  # Get the mid for the current category\n",
    "\n",
    "                # Use the mid to get the index from the mid_to_index\n",
    "                if mid in self.mid_to_index:\n",
    "                    label = self.mid_to_index[mid]  # Get the integer index as the label\n",
    "                else:\n",
    "                    label = -1  # Default to -1 if not found\n",
    "\n",
    "                for audio_file in self.audio_files:\n",
    "                    if category_name.lower() in audio_file.lower():\n",
    "                        audio_file = audio_file.replace(\"\\\\\", \"/\")\n",
    "                        data.append({\"audio\": audio_file, \"label\": label})\n",
    "        return data\n",
    "    \n",
    "    def load_audio(self, file_path):\n",
    "        \"\"\"Load and preprocess audio using Wav2Vec2Processor.\"\"\"\n",
    "        try:\n",
    "            if not os.path.isfile(file_path):\n",
    "                raise FileNotFoundError(f\"WAV file not found: {file_path}\")\n",
    "\n",
    "            # Load audio using librosa and resample to 16kHz\n",
    "            audio_data, sr = librosa.load(file_path, sr=16000)\n",
    "            print(f\"Successfully loaded audio: {file_path}, shape: {audio_data.shape}, dtype: {audio_data.dtype}\")\n",
    "\n",
    "            # Preprocess using Wav2Vec2Processor\n",
    "            inputs = processor(audio_data, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "            processed_audio = inputs.input_values.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "            print(f\"Processed audio shape: {processed_audio.shape}\")\n",
    "\n",
    "            # Return processed audio\n",
    "            return processed_audio\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get one item (audio, label) for the dataset.\"\"\"\n",
    "        sample = self.data[idx]\n",
    "        audio_data = self.load_audio(sample[\"audio\"])\n",
    "        label = sample[\"label\"]\n",
    "\n",
    "        # Ensure audio_data is valid\n",
    "        if audio_data is None:\n",
    "            print(f\"Error loading audio at index {idx}, returning dummy data.\")\n",
    "            return {\"input_values\": torch.zeros(1), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "        # Trim or pad audio to max_length\n",
    "        max_length = 160000  # Set a max_length for padding/truncating\n",
    "        if audio_data.shape[0] < max_length:\n",
    "            padding = torch.zeros(max_length - audio_data.shape[0])\n",
    "            audio_data = torch.cat([audio_data, padding])\n",
    "        else:\n",
    "            audio_data = audio_data[:max_length]\n",
    "\n",
    "        print(f\"Successfully loaded audio at index {idx}, shape: {audio_data.shape}, label: {label}\")\n",
    "        \n",
    "        return {\"input_values\": audio_data.clone().detach(), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Initialize the dataset and dataloaders\n",
    "audio_directory = r\"audiosets/ontology\"\n",
    "ontology_file = 'ontology.json'\n",
    "\n",
    "# Initialize dataset and prepare data\n",
    "dataset = AudioDataset(audio_directory, ontology_file, mid_to_index)\n",
    "\n",
    "# Now split the dataset into train and test sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(dataset.data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize train and test datasets using the split data\n",
    "train_dataset = AudioDataset(audio_directory, ontology_file, mid_to_index)\n",
    "test_dataset = AudioDataset(audio_directory, ontology_file, mid_to_index)\n",
    "\n",
    "# Assign the split data to the datasets\n",
    "train_dataset.data = train_data\n",
    "test_dataset.data = test_data\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    save_steps=10,\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Initialize train and test dataloaders with the custom collate_fn\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Pass the datasets without specifying the dataloaders in Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Wind instrument, woodwind instrument_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 0, shape: torch.Size([160000]), dtype: torch.float32\n",
      "Loss for the sample: 6.1997270584106445\n",
      "Logits: tensor([[-0.1580, -0.1213, -0.0112,  0.0322, -0.0616, -0.0335, -0.0438,  0.1859,\n",
      "          0.0470,  0.0446,  0.0949, -0.0042,  0.0030, -0.1470,  0.0284, -0.1508,\n",
      "          0.0313, -0.0552,  0.0296, -0.0260,  0.2222, -0.0082,  0.0016, -0.0531,\n",
      "         -0.0294,  0.0156, -0.0054,  0.1366,  0.0968,  0.0012, -0.0760,  0.0263,\n",
      "         -0.0250,  0.1603, -0.0762, -0.0318,  0.0198, -0.1156, -0.1020, -0.0716,\n",
      "         -0.0972, -0.0438, -0.0795, -0.1077,  0.0280, -0.1169, -0.0596,  0.0981,\n",
      "         -0.1082,  0.0638, -0.0292, -0.0571,  0.0552, -0.0542,  0.0653, -0.0605,\n",
      "         -0.0336,  0.0578,  0.0866, -0.0536,  0.0024, -0.0595, -0.0607,  0.0596,\n",
      "         -0.0023, -0.1483,  0.0867, -0.1273, -0.0376,  0.0045,  0.0997,  0.0640,\n",
      "          0.0448,  0.0127,  0.0651,  0.0610, -0.0374,  0.0605, -0.0922,  0.0656,\n",
      "          0.0785,  0.0026, -0.0031,  0.0228,  0.0466,  0.1103, -0.0056,  0.0873,\n",
      "         -0.0200,  0.1151,  0.0174,  0.0487,  0.1225,  0.1706,  0.0302,  0.0439,\n",
      "          0.0147,  0.0352, -0.1143,  0.1712,  0.0032, -0.0451,  0.0278, -0.0782,\n",
      "          0.2380, -0.1260, -0.0293,  0.0059, -0.0919,  0.0285,  0.0913,  0.1479,\n",
      "          0.1985,  0.1001,  0.0039, -0.0061,  0.0331, -0.2161, -0.0322,  0.1555,\n",
      "         -0.1159, -0.0062, -0.0558, -0.0621, -0.0404, -0.0300, -0.0154, -0.0276,\n",
      "         -0.0400,  0.0989, -0.0102, -0.0126,  0.1079,  0.0081,  0.0129, -0.0661,\n",
      "         -0.0298, -0.0941, -0.0918, -0.1205,  0.0143,  0.0521,  0.0962, -0.0584,\n",
      "          0.0067, -0.0420,  0.0042,  0.0362, -0.1450, -0.0164, -0.1218,  0.1048,\n",
      "         -0.0314, -0.0311, -0.0029, -0.0251, -0.0302, -0.0997,  0.0013,  0.0302,\n",
      "          0.0432, -0.0376, -0.0059,  0.0109,  0.0433,  0.1233,  0.0965,  0.1397,\n",
      "         -0.0138,  0.0500,  0.0600, -0.0024, -0.0267,  0.1174,  0.0162,  0.0781,\n",
      "         -0.0198, -0.0133, -0.0341,  0.0294, -0.0269,  0.0456, -0.0794, -0.0254,\n",
      "          0.0867, -0.0672, -0.0423,  0.0890,  0.0168, -0.0015, -0.1253,  0.0986,\n",
      "          0.0250, -0.0773,  0.0639, -0.0024, -0.0328,  0.0247,  0.1344, -0.1504,\n",
      "          0.0446, -0.1063, -0.1089, -0.1263,  0.0334,  0.0435, -0.0255,  0.1026,\n",
      "          0.0583,  0.0013,  0.0965,  0.0768,  0.0694,  0.0204,  0.0050,  0.0688,\n",
      "          0.0523, -0.0539, -0.0757, -0.0988,  0.0332, -0.0375, -0.0383,  0.0361,\n",
      "          0.0844,  0.0145, -0.0669, -0.1115,  0.1077, -0.0233, -0.0016, -0.0181,\n",
      "         -0.0139,  0.1366,  0.0322, -0.1454, -0.0229, -0.0091,  0.1504,  0.0580,\n",
      "         -0.1715,  0.0061,  0.1095,  0.0940,  0.0309,  0.0442,  0.0026,  0.1452,\n",
      "         -0.0927,  0.0305,  0.0879, -0.0109, -0.0058, -0.0689, -0.0294,  0.0524,\n",
      "          0.0433,  0.0298, -0.0283, -0.0230,  0.0446,  0.1055, -0.0705, -0.0529,\n",
      "         -0.0982, -0.2170,  0.1522,  0.0204, -0.0323, -0.0330, -0.0246, -0.0792,\n",
      "          0.0211, -0.0983,  0.0164, -0.1145, -0.0017, -0.0073,  0.0598,  0.0081,\n",
      "          0.1048,  0.1086,  0.0598,  0.0475,  0.0716,  0.0798,  0.0151, -0.0384,\n",
      "         -0.0404, -0.1070, -0.0035,  0.0260,  0.0025,  0.1118, -0.0153, -0.0706,\n",
      "         -0.0731, -0.0676,  0.0016, -0.0653,  0.0051,  0.0188, -0.1787,  0.0285,\n",
      "         -0.0030,  0.1375,  0.0466,  0.0497, -0.0672,  0.0744, -0.1976, -0.0117,\n",
      "         -0.0451,  0.0668,  0.1097, -0.0541, -0.0350,  0.0572, -0.0331,  0.0136,\n",
      "         -0.0118, -0.0717,  0.0852, -0.2203, -0.0687, -0.0805, -0.0213,  0.0479,\n",
      "          0.0288, -0.1926,  0.0576, -0.0288, -0.0425,  0.1013,  0.0839, -0.0716,\n",
      "         -0.0742,  0.1017, -0.0289, -0.0021, -0.0465, -0.1560, -0.1307, -0.0232,\n",
      "          0.0791, -0.0163, -0.0456, -0.0622,  0.1167, -0.0438, -0.0024, -0.0081,\n",
      "         -0.0988,  0.0794, -0.0842,  0.0637, -0.0377,  0.0952,  0.1217,  0.1114,\n",
      "          0.0328, -0.1056, -0.0273, -0.1165, -0.0070, -0.0379, -0.0440,  0.2041,\n",
      "         -0.0855,  0.0245, -0.0950,  0.1347, -0.0285, -0.0979, -0.0500, -0.0488,\n",
      "          0.0423,  0.0403,  0.1079, -0.0711,  0.0667,  0.0948, -0.0724,  0.0856,\n",
      "         -0.1437, -0.0102, -0.0437, -0.0905, -0.0139, -0.1004, -0.0617,  0.0653,\n",
      "          0.0375,  0.0559, -0.0619,  0.0868, -0.0965, -0.1661, -0.0090, -0.1005,\n",
      "         -0.0326, -0.0135, -0.0665, -0.0028,  0.0410,  0.0862,  0.0355,  0.0788,\n",
      "          0.0152,  0.0294, -0.0612, -0.1225, -0.1213, -0.0330,  0.0513,  0.1164,\n",
      "         -0.0288,  0.0686, -0.0634, -0.0472,  0.0519,  0.0215,  0.1060, -0.0805,\n",
      "         -0.1874, -0.0310,  0.0498,  0.0693,  0.1283,  0.0648, -0.1628, -0.0947,\n",
      "         -0.0962,  0.0193,  0.0207, -0.0289,  0.0460, -0.0233,  0.1374,  0.0194,\n",
      "          0.0906,  0.0500, -0.0175, -0.0084,  0.0992, -0.0446,  0.0115, -0.0212,\n",
      "         -0.0040, -0.1463, -0.0562,  0.0191,  0.0700,  0.0201,  0.1630,  0.1206,\n",
      "         -0.0027, -0.0742,  0.0681, -0.0586,  0.0107, -0.0154, -0.0989,  0.1166,\n",
      "          0.0630,  0.1223,  0.0295, -0.0173,  0.0102,  0.0488,  0.1719, -0.1365,\n",
      "          0.0343, -0.0164,  0.0419, -0.0405,  0.0106,  0.1130,  0.0791, -0.0441,\n",
      "          0.0478,  0.0611, -0.0645,  0.0347,  0.0346, -0.0725, -0.0262,  0.0833,\n",
      "          0.0403, -0.0260,  0.0716, -0.1420,  0.0705, -0.0149, -0.0650,  0.0150,\n",
      "          0.1317, -0.1421, -0.0088,  0.0302,  0.1264,  0.0527,  0.0641, -0.0218,\n",
      "          0.0635,  0.1123, -0.1380, -0.1059, -0.0353, -0.0093, -0.0879, -0.0317,\n",
      "         -0.0473, -0.1203, -0.0369, -0.0564,  0.0757, -0.0074,  0.1317,  0.1876,\n",
      "          0.0189]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Take one sample from the dataset\n",
    "sample = train_dataset[0]  # Take one sample\n",
    "input_values = sample['input_values'].unsqueeze(0)  # Add batch dimension\n",
    "labels = sample['labels'].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Ensure the label is within bounds\n",
    "assert 0 <= labels.item() <= 520, f\"Label {labels.item()} out of bounds!\"\n",
    "\n",
    "# Pass input and labels to the model\n",
    "outputs = model(input_values, labels=labels)  # Make sure to pass the labels\n",
    "\n",
    "# The output contains 'loss' and 'logits' (among other things)\n",
    "loss = outputs.loss  # Access the loss\n",
    "logits = outputs.logits  # Access the logits\n",
    "\n",
    "print(f\"Loss for the sample: {loss.item()}\")\n",
    "print(f\"Logits: {logits}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m#device = torch.device(\"cpu\")\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMoved model to GPU if available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m namedtuple\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:3162\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   3158\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3159\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3160\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3161\u001b[0m         )\n\u001b[1;32m-> 3162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: Module._apply at line 903 (2 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1324\u001b[0m             device,\n\u001b[0;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1326\u001b[0m             non_blocking,\n\u001b[0;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1328\u001b[0m         )\n\u001b[1;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "import torch\n",
    "import librosa\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Load the class map CSV (mapping from mid to index)\n",
    "class_map_df = pd.read_csv('yamnet_class_map.csv')\n",
    "class_map = pd.read_csv('yamnet_class_map.csv').set_index('display_name').to_dict()['mid']\n",
    "\n",
    "# Create a mapping from mid (string) to index (integer)\n",
    "#mid_to_index = class_map_df.set_index('mid').index.tolist()\n",
    "mid_to_index = {mid: idx for idx, mid in enumerate(set(class_map.values()))}\n",
    "\n",
    "# Initialize the model and processor\n",
    "#model = Wav2Vec2ForSequenceClassification.from_pretrained('facebook/wav2vec2-large-960h', num_labels=len(mid_to_index))\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained('facebook/wav2vec2-large-960h', num_labels=521)\n",
    "\n",
    "#model =  model.to_bettertransformer()\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-large-960h')\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "print(\"Moved model to GPU if available\")\n",
    "from collections import namedtuple\n",
    "\n",
    "# Define a namedtuple for dataset items\n",
    "AudioSample = namedtuple(\"AudioSample\", [\"input_values\", \"labels\"])\n",
    "\n",
    "\n",
    "def collate_fn( batch, max_length=160000):  # Set a max_length that works for your use case\n",
    "    \"\"\"Collate function to handle padding and truncation.\"\"\"\n",
    "    audio_data = [item[\"input_values\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "    # Pad or truncate audio sequences to the specified max_length\n",
    "    audio_data_padded = []\n",
    "    for audio in audio_data:\n",
    "        if audio.shape[0] < max_length:\n",
    "            # Pad if audio is shorter than max_length\n",
    "            padding = torch.zeros(max_length - audio.shape[0])\n",
    "            audio_data_padded.append(torch.cat([audio, padding]))\n",
    "        else:\n",
    "            # Truncate if audio is longer than max_length\n",
    "            audio_data_padded.append(audio[:max_length])\n",
    "\n",
    "    # Stack the labels into a tensor\n",
    "    labels = torch.stack(labels, dim=0)\n",
    "\n",
    "    # Stack the audio data into a batch tensor\n",
    "    audio_data_padded = torch.stack(audio_data_padded, dim=0)\n",
    "\n",
    "    return {\"input_values\": audio_data_padded, \"labels\": labels}\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_directory, ontology_file, mid_to_index):\n",
    "        with open(ontology_file, 'r') as f:\n",
    "            self.ontology_data = json.load(f)\n",
    "\n",
    "        self.mid_to_index = mid_to_index\n",
    "        self.audio_directory = audio_directory\n",
    "        self.audio_files = glob.glob(os.path.join(self.audio_directory, '**', '*.wav'), recursive=True)\n",
    "        \n",
    "        # Populate the dataset by calling prepare_data\n",
    "        self.data = self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        data = []\n",
    "        for category in self.ontology_data:\n",
    "            if \"positive_examples\" in category:\n",
    "                category_name = category[\"name\"]\n",
    "                mid = category[\"id\"]  # Get the mid for the current category\n",
    "\n",
    "                # Use the mid to get the index from the mid_to_index\n",
    "                if mid in self.mid_to_index:\n",
    "                    label = self.mid_to_index[mid]  # Get the integer index as the label\n",
    "                else:\n",
    "                    label = -1  # Default to -1 if not found\n",
    "\n",
    "                for audio_file in self.audio_files:\n",
    "                    if category_name.lower() in audio_file.lower():\n",
    "                        audio_file = audio_file.replace(\"\\\\\", \"/\")\n",
    "                        data.append({\"audio\": audio_file, \"label\": label})\n",
    "        return data\n",
    "    \n",
    "    def load_audio(self, file_path):\n",
    "        \"\"\"Load and preprocess audio using Wav2Vec2Processor.\"\"\"\n",
    "        try:\n",
    "            if not os.path.isfile(file_path):\n",
    "                raise FileNotFoundError(f\"WAV file not found: {file_path}\")\n",
    "\n",
    "            # Load audio using librosa and resample to 16kHz\n",
    "            audio_data, sr = librosa.load(file_path, sr=16000)\n",
    "            print(f\"Successfully loaded audio: {file_path}, shape: {audio_data.shape}, dtype: {audio_data.dtype}\")\n",
    "\n",
    "            # Preprocess using Wav2Vec2Processor\n",
    "            inputs = processor(audio_data, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "            processed_audio = inputs.input_values.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "            print(f\"Processed audio shape: {processed_audio.shape}\")\n",
    "\n",
    "            # Return processed audio\n",
    "            return processed_audio\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get one item (audio, label) for the dataset.\"\"\"\n",
    "        sample = self.data[idx]\n",
    "        audio_data = self.load_audio(sample[\"audio\"])\n",
    "        label = sample[\"label\"]\n",
    "\n",
    "        max_length=160000\n",
    "        if audio_data.shape[0] < max_length:\n",
    "            # Pad if audio is shorter than max_length\n",
    "            padding = torch.zeros(max_length - audio_data.shape[0])\n",
    "            audio_data = torch.cat([audio_data, padding])\n",
    "        else:\n",
    "            # Truncate if audio is longer than max_length\n",
    "            audio_data = audio_data[:max_length]\n",
    "            # Ensure audio_data is valid\n",
    "            \n",
    "        if audio_data is None:\n",
    "            print(f\"Error loading audio at index {idx}, returning dummy data.\")\n",
    "            return {\"input_values\": torch.zeros(1), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "        # Check if audio_data is valid (numpy array or tensor)\n",
    "        if isinstance(audio_data, (np.ndarray, torch.Tensor)):\n",
    "            print(f\"Successfully loaded audio at index {idx}, shape: {audio_data.shape}, dtype: {audio_data.dtype}\")\n",
    "            \n",
    "            # Return valid audio data as a dictionary\n",
    "            return {\"input_values\": audio_data.clone().detach(), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
    "        else:\n",
    "            print(f\"Unexpected audio_data type at index {idx}: {type(audio_data)}, returning dummy data.\")\n",
    "            return {\"input_values\": torch.zeros(1), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def collate_fn(self, batch, max_length=160000):  # Set a max_length that works for your use case\n",
    "        \"\"\"Collate function to handle padding and truncation.\"\"\"\n",
    "        audio_data = [item[\"input_values\"] for item in batch]\n",
    "        labels = [item[\"labels\"] for item in batch]\n",
    "\n",
    "        # Pad or truncate audio sequences to the specified max_length\n",
    "        audio_data_padded = []\n",
    "        for audio in audio_data:\n",
    "            if audio.shape[0] < max_length:\n",
    "                # Pad if audio is shorter than max_length\n",
    "                padding = torch.zeros(max_length - audio.shape[0])\n",
    "                audio_data_padded.append(torch.cat([audio, padding]))\n",
    "            else:\n",
    "                # Truncate if audio is longer than max_length\n",
    "                audio_data_padded.append(audio[:max_length])\n",
    "\n",
    "        # Stack the labels into a tensor\n",
    "        labels = torch.stack(labels, dim=0)\n",
    "\n",
    "        # Stack the audio data into a batch tensor\n",
    "        audio_data_padded = torch.stack(audio_data_padded, dim=0)\n",
    "\n",
    "        return {\"input_values\": audio_data_padded, \"labels\": labels}\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the dataset and dataloaders\n",
    "audio_directory = r\"audiosets/ontology\"\n",
    "ontology_file = 'ontology.json'\n",
    "\n",
    "# Initialize dataset and prepare data\n",
    "dataset = AudioDataset(audio_directory, ontology_file, mid_to_index)\n",
    "\n",
    "# Now split the dataset into train and test sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(dataset.data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize train and test datasets using the split data\n",
    "train_dataset = AudioDataset(audio_directory, ontology_file, mid_to_index)\n",
    "test_dataset = AudioDataset(audio_directory, ontology_file, mid_to_index)\n",
    "\n",
    "# Assign the split data to the datasets\n",
    "train_dataset.data = train_data\n",
    "test_dataset.data = test_data\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    save_steps=10,\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Initialize the dataset and dataloaders with the custom collate_fn\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# Pass the datasets without specifying the dataloaders in Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved model to GPU if available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Snort_0.wav, shape: (1857411,), dtype: float32\n",
      "Processed audio data type: <class 'torch.Tensor'>, shape: torch.Size([1857411])\n",
      "Successfully loaded audio at index 649, shape: torch.Size([1857411]), dtype: torch.float32\n",
      "Successfully loaded audio: audiosets/ontology/Electronic organ_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio data type: <class 'torch.Tensor'>, shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 360, shape: torch.Size([160125]), dtype: torch.float32\n",
      "Successfully loaded audio: audiosets/ontology/Blender_6.wav, shape: (159754,), dtype: float32\n",
      "Processed audio data type: <class 'torch.Tensor'>, shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1288, shape: torch.Size([159754]), dtype: torch.float32\n",
      "Successfully loaded audio: audiosets/ontology/Tubular bells_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio data type: <class 'torch.Tensor'>, shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1995, shape: torch.Size([160125]), dtype: torch.float32\n",
      "Successfully loaded audio: audiosets/ontology/Hubbub, speech noise, speech babble_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio data type: <class 'torch.Tensor'>, shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1492, shape: torch.Size([160125]), dtype: torch.float32\n",
      "Successfully loaded audio: audiosets/ontology/Ice cream truck, ice cream van_5.wav, shape: (435235,), dtype: float32\n",
      "Processed audio data type: <class 'torch.Tensor'>, shape: torch.Size([435235])\n",
      "Successfully loaded audio at index 76, shape: torch.Size([435235]), dtype: torch.float32\n",
      "Successfully loaded audio: audiosets/ontology/Chopping (food)_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio data type: <class 'torch.Tensor'>, shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 977, shape: torch.Size([160125]), dtype: torch.float32\n",
      "Successfully loaded audio: audiosets/ontology/Duck_6.wav, shape: (3829621,), dtype: float32\n",
      "Processed audio data type: <class 'torch.Tensor'>, shape: torch.Size([3829621])\n",
      "Successfully loaded audio at index 1191, shape: torch.Size([3829621]), dtype: torch.float32\n",
      "Successfully loaded audio: audiosets/ontology/Drum machine_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio data type: <class 'torch.Tensor'>, shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1870, shape: torch.Size([160125]), dtype: torch.float32\n",
      "Successfully loaded audio: audiosets/ontology/Cap gun_0.wav, shape: (150466,), dtype: float32\n",
      "Processed audio data type: <class 'torch.Tensor'>, shape: torch.Size([150466])\n",
      "Successfully loaded audio at index 2206, shape: torch.Size([150466]), dtype: torch.float32\n",
      "Successfully loaded audio: audiosets/ontology/Electric guitar_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio data type: <class 'torch.Tensor'>, shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1049, shape: torch.Size([160125]), dtype: torch.float32\n",
      "Successfully loaded audio: audiosets/ontology/Whispering_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio data type: <class 'torch.Tensor'>, shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1581, shape: torch.Size([159754]), dtype: torch.float32\n",
      "Successfully loaded audio: audiosets/ontology/Truck_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio data type: <class 'torch.Tensor'>, shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 197, shape: torch.Size([160125]), dtype: torch.float32\n",
      "Successfully loaded audio: audiosets/ontology/Squeak_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio data type: <class 'torch.Tensor'>, shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1687, shape: torch.Size([160125]), dtype: torch.float32\n",
      "Successfully loaded audio: audiosets/ontology/Female singing_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio data type: <class 'torch.Tensor'>, shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 11, shape: torch.Size([160125]), dtype: torch.float32\n",
      "Successfully loaded audio: audiosets/ontology/Breathing_6.wav, shape: (159754,), dtype: float32\n",
      "Processed audio data type: <class 'torch.Tensor'>, shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 647, shape: torch.Size([159754]), dtype: torch.float32\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "vars() argument must have __dict__ attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 171\u001b[0m\n\u001b[0;32m    162\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m    163\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    164\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m    168\u001b[0m )\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2500\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2498\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2499\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2500\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2501\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[0;32m   2502\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:5180\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches)\u001b[0m\n\u001b[0;32m   5178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5179\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5180\u001b[0m         batch_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m   5181\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5182\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\data_loader.py:564\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 564\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    566\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer_utils.py:848\u001b[0m, in \u001b[0;36mRemoveColumnsCollator.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[\u001b[38;5;28mdict\u001b[39m]):\n\u001b[0;32m    847\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remove_columns(feature) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m--> 848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:92\u001b[0m, in \u001b[0;36mdefault_data_collator\u001b[1;34m(features, return_tensors)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# In this function we'll make the assumption that all `features` in the batch\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# have the same attributes.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# So we will look at the first element as a proxy for what attributes exist\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# on the whole batch.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_default_data_collator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf_default_data_collator(features)\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\data\\data_collator.py:131\u001b[0m, in \u001b[0;36mtorch_default_data_collator\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features[\u001b[38;5;241m0\u001b[39m], Mapping):\n\u001b[1;32m--> 131\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m    132\u001b[0m first \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    133\u001b[0m batch \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mTypeError\u001b[0m: vars() argument must have __dict__ attribute"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "# Load class map CSV (mapping from mid to index)\n",
    "class_map_df = pd.read_csv('yamnet_class_map.csv')\n",
    "\n",
    "# Create a mapping from mid to index\n",
    "mid_to_index = class_map_df.set_index('mid').index.tolist()\n",
    "\n",
    "# Initialize the model and processor\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained('facebook/wav2vec2-large-960h', num_labels=len(class_map))\n",
    "processor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-large-960h')\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Moved model to GPU if available\")\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_directory, ontology_file, class_map):\n",
    "        with open(ontology_file, 'r') as f:\n",
    "            self.ontology_data = json.load(f)\n",
    "\n",
    "        self.class_map = class_map\n",
    "        self.audio_directory = audio_directory\n",
    "        self.audio_files = glob.glob(os.path.join(self.audio_directory, '**', '*.wav'), recursive=True)\n",
    "        \n",
    "        # Populate the dataset by calling prepare_data\n",
    "        self.data = self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        data = []\n",
    "        for category in self.ontology_data:\n",
    "            if \"positive_examples\" in category:\n",
    "                category_name = category[\"name\"]\n",
    "                mid = category[\"id\"]  # Get the mid for the current category\n",
    "\n",
    "                # Use the mid to get the index from the class_map\n",
    "                if mid in mid_to_index:\n",
    "                    label = mid_to_index.index(mid)  # Use the index as the numeric label\n",
    "                else:\n",
    "                    label = -1  # Default to -1 if not found\n",
    "\n",
    "                for audio_file in self.audio_files:\n",
    "                    if category_name.lower() in audio_file.lower():\n",
    "                        audio_file = audio_file.replace(\"\\\\\", \"/\")\n",
    "                        data.append({\"audio\": audio_file, \"label\": label})\n",
    "        return data\n",
    "    \n",
    "    def load_audio(self, file_path):\n",
    "        \"\"\"Load and preprocess audio using Wav2Vec2Processor.\"\"\"\n",
    "        try:\n",
    "            if not os.path.isfile(file_path):\n",
    "                raise FileNotFoundError(f\"WAV file not found: {file_path}\")\n",
    "\n",
    "            # Load audio using librosa and resample to 16kHz\n",
    "            audio_data, sr = librosa.load(file_path, sr=16000)\n",
    "            print(f\"Successfully loaded audio: {file_path}, shape: {audio_data.shape}, dtype: {audio_data.dtype}\")\n",
    "\n",
    "            # Preprocess using Wav2Vec2Processor\n",
    "            inputs = processor(audio_data, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "            processed_audio = inputs.input_values.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "            # Check if audio_data is a tensor and not a string\n",
    "            if isinstance(processed_audio, torch.Tensor):\n",
    "                print(f\"Processed audio data type: {type(processed_audio)}, shape: {processed_audio.shape}\")\n",
    "                return processed_audio\n",
    "            else:\n",
    "                print(f\"Unexpected processed audio data type: {type(processed_audio)}\")\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get one item (audio, label) for the dataset.\"\"\"\n",
    "        sample = self.data[idx]\n",
    "        audio_data = self.load_audio(sample[\"audio\"])\n",
    "        label = sample[\"label\"]\n",
    "\n",
    "        # Ensure audio_data is valid\n",
    "        if audio_data is None:\n",
    "            print(f\"Error loading audio at index {idx}, returning dummy data.\")\n",
    "            return torch.zeros(1), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        # Check if audio_data is valid (numpy array or tensor)\n",
    "        if isinstance(audio_data, (np.ndarray, torch.Tensor)):\n",
    "            print(f\"Successfully loaded audio at index {idx}, shape: {audio_data.shape}, dtype: {audio_data.dtype}\")\n",
    "            \n",
    "            # Ensure audio_data is not a string or file path\n",
    "            if isinstance(audio_data, str):\n",
    "                print(f\"Unexpected audio_data type at index {idx}: {type(audio_data)}, returning dummy data.\")\n",
    "                return torch.zeros(1), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "            # Return valid audio data\n",
    "            return audio_data.clone().detach(), torch.tensor(label, dtype=torch.long)\n",
    "        else:\n",
    "            print(f\"Unexpected audio_data type at index {idx}: {type(audio_data)}, returning dummy data.\")\n",
    "            return torch.zeros(1), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "# Initialize the dataset and dataloaders\n",
    "audio_directory = r\"audiosets/ontology\"\n",
    "ontology_file = 'ontology.json'\n",
    "class_map = pd.read_csv('yamnet_class_map.csv').set_index('display_name').to_dict()['mid']\n",
    "\n",
    "# Initialize dataset and prepare data\n",
    "dataset = AudioDataset(audio_directory, ontology_file, class_map)\n",
    "\n",
    "# Now split the dataset into train and test sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(dataset.data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize train and test datasets using the split data\n",
    "train_dataset = AudioDataset(audio_directory, ontology_file, class_map)\n",
    "test_dataset = AudioDataset(audio_directory, ontology_file, class_map)\n",
    "\n",
    "# Assign the split data to the datasets\n",
    "train_dataset.data = train_data\n",
    "test_dataset.data = test_data\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    save_steps=10,\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded audio: audiosets/ontology/Snort_0.wav, shape: (1857411,), dtype: float32\n",
      "Processed audio data type: <class 'torch.Tensor'>, shape: torch.Size([1857411])\n",
      "Audio Tensor: tensor([-2.8886e-05, -2.8886e-05, -2.8886e-05,  ..., -2.8886e-05,\n",
      "        -2.8886e-05, -2.8886e-05])\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import torch\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "# Initialize processor\n",
    "processor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-large-960h')\n",
    "\n",
    "def load_and_preprocess_audio(file_path):\n",
    "    try:\n",
    "        # Load audio using librosa and resample to 16kHz (standard for Wav2Vec2)\n",
    "        audio_data, sr = librosa.load(file_path, sr=16000)\n",
    "        print(f\"Successfully loaded audio: {file_path}, shape: {audio_data.shape}, dtype: {audio_data.dtype}\")\n",
    "\n",
    "        # Process the audio with Wav2Vec2Processor\n",
    "        inputs = processor(audio_data, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "        processed_audio = inputs.input_values.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "        # Check if the result is a tensor\n",
    "        print(f\"Processed audio data type: {type(processed_audio)}, shape: {processed_audio.shape}\")\n",
    "        return processed_audio\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test with one audio file\n",
    "file_path = \"audiosets/ontology/Snort_0.wav\"  # Use a sample path to one of your .wav files\n",
    "processed_audio = load_and_preprocess_audio(file_path)\n",
    "\n",
    "# Check the result\n",
    "if processed_audio is not None:\n",
    "    print(f\"Audio Tensor: {processed_audio}\")\n",
    "else:\n",
    "    print(\"Failed to process audio.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
