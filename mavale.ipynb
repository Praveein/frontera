{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.wav:\n",
      "  Crying, sobbing: 0.150\n",
      "  Speech      : 0.135\n",
      "  Whimper     : 0.113\n",
      "  Baby cry, infant cry: 0.091\n",
      "  Animal      : 0.074\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import resampy\n",
    "import soundfile as sf\n",
    "import tensorflow as tf\n",
    "\n",
    "import params as yamnet_params\n",
    "import yamnet as yamnet_model\n",
    "import features  # Import features.py\n",
    "\n",
    "# Load YAMNet model and weights\n",
    "params = yamnet_params.Params()\n",
    "yamnet = yamnet_model.yamnet_frames_model(params)\n",
    "yamnet.load_weights('yamnet.h5')\n",
    "yamnet_classes = yamnet_model.class_names('yamnet_class_map.csv')\n",
    "\n",
    "def process_audio(file_name, params, yamnet):\n",
    "    # Load and process the audio file\n",
    "    wav_data, sr = sf.read(file_name, dtype=np.int16)\n",
    "    waveform = wav_data / 32768.0  # Convert to [-1.0, +1.0]\n",
    "    waveform = waveform.astype('float32')\n",
    "\n",
    "    # Convert to mono and resample to the required sample rate\n",
    "    if len(waveform.shape) > 1:\n",
    "        waveform = np.mean(waveform, axis=1)\n",
    "    if sr != params.sample_rate:\n",
    "        waveform = resampy.resample(waveform, sr, params.sample_rate)\n",
    "\n",
    "    # Pad waveform to get an integral number of patches\n",
    "    padded_waveform = features.pad_waveform(waveform, params)\n",
    "\n",
    "    # Convert the waveform to log mel spectrogram patches\n",
    "    log_mel_spectrogram, feature_patches = features.waveform_to_log_mel_spectrogram_patches(padded_waveform, params)\n",
    "\n",
    "    # Make prediction using YAMNet\n",
    "    scores, embeddings, spectrogram = yamnet(waveform)\n",
    "    prediction = np.mean(scores, axis=0)  # Average scores over time\n",
    "\n",
    "    return prediction\n",
    "\n",
    "# Example usage\n",
    "file_name = 'test.wav'  # Replace with your audio file\n",
    "prediction = process_audio(file_name, params, yamnet)\n",
    "\n",
    "# Get top 5 predictions\n",
    "top5_i = np.argsort(prediction)[::-1][:5]\n",
    "print(f\"{file_name}:\\n\" + \n",
    "      '\\n'.join(f'  {yamnet_classes[i]:12s}: {prediction[i]:.3f}' for i in top5_i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import resampy\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_audio(audio_path, params):\n",
    "    # Load audio file and resample to 16kHz\n",
    "    waveform, sample_rate = tf.audio.decode_wav(tf.io.read_file(audio_path))\n",
    "    waveform = tf.squeeze(waveform, axis=-1)\n",
    "    waveform = resampy.resample(waveform.numpy(), sample_rate, params.sample_rate)\n",
    "\n",
    "    # Convert the waveform to log mel spectrogram\n",
    "    spectrogram, features = waveform_to_log_mel_spectrogram_patches(waveform, params)\n",
    "    return features, spectrogram\n",
    "\n",
    "def waveform_to_log_mel_spectrogram_patches(waveform, params):\n",
    "  \"\"\"Compute log mel spectrogram patches of a 1-D waveform.\"\"\"\n",
    "  with tf.name_scope('log_mel_features'):\n",
    "    # waveform has shape [<# samples>]\n",
    "\n",
    "    # Convert waveform into spectrogram using a Short-Time Fourier Transform.\n",
    "    # Note that tf.signal.stft() uses a periodic Hann window by default.\n",
    "    window_length_samples = int(\n",
    "      round(params.sample_rate * params.stft_window_seconds))\n",
    "    hop_length_samples = int(\n",
    "      round(params.sample_rate * params.stft_hop_seconds))\n",
    "    fft_length = 2 ** int(np.ceil(np.log(window_length_samples) / np.log(2.0)))\n",
    "    num_spectrogram_bins = fft_length // 2 + 1\n",
    "    if params.tflite_compatible:\n",
    "      magnitude_spectrogram = _tflite_stft_magnitude(\n",
    "          signal=waveform,\n",
    "          frame_length=window_length_samples,\n",
    "          frame_step=hop_length_samples,\n",
    "          fft_length=fft_length)\n",
    "    else:\n",
    "      magnitude_spectrogram = tf.abs(tf.signal.stft(\n",
    "          signals=waveform,\n",
    "          frame_length=window_length_samples,\n",
    "          frame_step=hop_length_samples,\n",
    "          fft_length=fft_length))\n",
    "    # magnitude_spectrogram has shape [<# STFT frames>, num_spectrogram_bins]\n",
    "\n",
    "    # Convert spectrogram into log mel spectrogram.\n",
    "    linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "        num_mel_bins=params.mel_bands,\n",
    "        num_spectrogram_bins=num_spectrogram_bins,\n",
    "        sample_rate=params.sample_rate,\n",
    "        lower_edge_hertz=params.mel_min_hz,\n",
    "        upper_edge_hertz=params.mel_max_hz)\n",
    "    mel_spectrogram = tf.matmul(\n",
    "      magnitude_spectrogram, linear_to_mel_weight_matrix)\n",
    "    log_mel_spectrogram = tf.math.log(mel_spectrogram + params.log_offset)\n",
    "    # log_mel_spectrogram has shape [<# STFT frames>, params.mel_bands]\n",
    "\n",
    "    # Frame spectrogram (shape [<# STFT frames>, params.mel_bands]) into patches\n",
    "    # (the input examples). Only complete frames are emitted, so if there is\n",
    "    # less than params.patch_window_seconds of waveform then nothing is emitted\n",
    "    # (to avoid this, zero-pad before processing).\n",
    "    spectrogram_hop_length_samples = int(\n",
    "      round(params.sample_rate * params.stft_hop_seconds))\n",
    "    spectrogram_sample_rate = params.sample_rate / spectrogram_hop_length_samples\n",
    "    patch_window_length_samples = int(\n",
    "      round(spectrogram_sample_rate * params.patch_window_seconds))\n",
    "    patch_hop_length_samples = int(\n",
    "      round(spectrogram_sample_rate * params.patch_hop_seconds))\n",
    "    features = tf.signal.frame(\n",
    "        signal=log_mel_spectrogram,\n",
    "        frame_length=patch_window_length_samples,\n",
    "        frame_step=patch_hop_length_samples,\n",
    "        axis=0)\n",
    "    # features has shape [<# patches>, <# STFT frames in an patch>, params.mel_bands]\n",
    "\n",
    "    return log_mel_spectrogram, features\n",
    "\n",
    "\n",
    "def pad_waveform(waveform, params):\n",
    "  \"\"\"Pads waveform with silence if needed to get an integral number of patches.\"\"\"\n",
    "  # In order to produce one patch of log mel spectrogram input to YAMNet, we\n",
    "  # need at least one patch window length of waveform plus enough extra samples\n",
    "  # to complete the final STFT analysis window.\n",
    "  min_waveform_seconds = (\n",
    "      params.patch_window_seconds +\n",
    "      params.stft_window_seconds - params.stft_hop_seconds)\n",
    "  min_num_samples = tf.cast(min_waveform_seconds * params.sample_rate, tf.int32)\n",
    "  num_samples = tf.shape(waveform)[0]\n",
    "  num_padding_samples = tf.maximum(0, min_num_samples - num_samples)\n",
    "\n",
    "  # In addition, there might be enough waveform for one or more additional\n",
    "  # patches formed by hopping forward. If there are more samples than one patch,\n",
    "  # round up to an integral number of hops.\n",
    "  num_samples = tf.maximum(num_samples, min_num_samples)\n",
    "  num_samples_after_first_patch = num_samples - min_num_samples\n",
    "  hop_samples = tf.cast(params.patch_hop_seconds * params.sample_rate, tf.int32)\n",
    "  num_hops_after_first_patch = tf.cast(tf.math.ceil(\n",
    "          tf.cast(num_samples_after_first_patch, tf.float32) /\n",
    "          tf.cast(hop_samples, tf.float32)), tf.int32)\n",
    "  num_padding_samples += (\n",
    "      hop_samples * num_hops_after_first_patch - num_samples_after_first_patch)\n",
    "\n",
    "  padded_waveform = tf.pad(waveform, [[0, num_padding_samples]],\n",
    "                           mode='CONSTANT', constant_values=0.0)\n",
    "  return padded_waveform\n",
    "\n",
    "\n",
    "def _tflite_stft_magnitude(signal, frame_length, frame_step, fft_length):\n",
    "  \"\"\"TF-Lite-compatible version of tf.abs(tf.signal.stft()).\"\"\"\n",
    "  def _hann_window():\n",
    "    return tf.reshape(\n",
    "      tf.constant(\n",
    "          (0.5 - 0.5 * np.cos(2 * np.pi * np.arange(0, 1.0, 1.0 / frame_length))\n",
    "          ).astype(np.float32),\n",
    "          name='hann_window'), [1, frame_length])\n",
    "\n",
    "  def _dft_matrix(dft_length):\n",
    "    \"\"\"Calculate the full DFT matrix in NumPy.\"\"\"\n",
    "    # See https://en.wikipedia.org/wiki/DFT_matrix\n",
    "    omega = (0 + 1j) * 2.0 * np.pi / float(dft_length)\n",
    "    # Don't include 1/sqrt(N) scaling, tf.signal.rfft doesn't apply it.\n",
    "    return np.exp(omega * np.outer(np.arange(dft_length), np.arange(dft_length)))\n",
    "\n",
    "  def _rdft(framed_signal, fft_length):\n",
    "    \"\"\"Implement real-input Discrete Fourier Transform by matmul.\"\"\"\n",
    "    # We are right-multiplying by the DFT matrix, and we are keeping only the\n",
    "    # first half (\"positive frequencies\").  So discard the second half of rows,\n",
    "    # but transpose the array for right-multiplication.  The DFT matrix is\n",
    "    # symmetric, so we could have done it more directly, but this reflects our\n",
    "    # intention better.\n",
    "    complex_dft_matrix_kept_values = _dft_matrix(fft_length)[:(\n",
    "        fft_length // 2 + 1), :].transpose()\n",
    "    real_dft_matrix = tf.constant(\n",
    "        np.real(complex_dft_matrix_kept_values).astype(np.float32),\n",
    "        name='real_dft_matrix')\n",
    "    imag_dft_matrix = tf.constant(\n",
    "        np.imag(complex_dft_matrix_kept_values).astype(np.float32),\n",
    "        name='imaginary_dft_matrix')\n",
    "    signal_frame_length = tf.shape(framed_signal)[-1]\n",
    "    half_pad = (fft_length - signal_frame_length) // 2\n",
    "    padded_frames = tf.pad(\n",
    "        framed_signal,\n",
    "        [\n",
    "            # Don't add any padding in the frame dimension.\n",
    "            [0, 0],\n",
    "            # Pad before and after the signal within each frame.\n",
    "            [half_pad, fft_length - signal_frame_length - half_pad]\n",
    "        ],\n",
    "        mode='CONSTANT',\n",
    "        constant_values=0.0)\n",
    "    real_stft = tf.matmul(padded_frames, real_dft_matrix)\n",
    "    imag_stft = tf.matmul(padded_frames, imag_dft_matrix)\n",
    "    return real_stft, imag_stft\n",
    "\n",
    "  def _complex_abs(real, imag):\n",
    "    return tf.sqrt(tf.add(real * real, imag * imag))\n",
    "\n",
    "  framed_signal = tf.signal.frame(signal, frame_length, frame_step)\n",
    "  windowed_signal = framed_signal * _hann_window()\n",
    "  real_stft, imag_stft = _rdft(windowed_signal, fft_length)\n",
    "  stft_magnitude = _complex_abs(real_stft, imag_stft)\n",
    "  return stft_magnitude\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Use MobileNetV2 as the base model (similar to MobileNetV1)\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    include_top=False, weights='imagenet', input_shape=(None, None, 3))\n",
    "\n",
    "# Freeze layers if you're fine-tuning\n",
    "base_model.trainable = False\n",
    "\n",
    "num_classes = 521  # Adjust this to match the number of classes in your dataset\n",
    "\n",
    "# Add a new classification head (modify the number of output classes)\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Func  (None, None, None, 1280   2257984   \n",
      " tional)                     )                                   \n",
      "                                                                 \n",
      " global_average_pooling2d_2  (None, 1280)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 521)               667401    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2925385 (11.16 MB)\n",
      "Trainable params: 667401 (2.55 MB)\n",
      "Non-trainable params: 2257984 (8.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Add a new classification head (modify the number of output classes)\n",
    "model = tf.keras.Sequential([\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Now you can compile and train the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Example: model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class '__main__.AudioDataset'>, <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(), \n\u001b[0;32m      2\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      3\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\engine\\data_adapter.py:1102\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1099\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m ALL_ADAPTER_CLS \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;66;03m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to find data adapter that can handle input: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1104\u001b[0m             _type_name(x), _type_name(y)\n\u001b[0;32m   1105\u001b[0m         )\n\u001b[0;32m   1106\u001b[0m     )\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(adapter_cls) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData adapters should be mutually exclusive for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandling inputs. Found multiple adapters \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1112\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class '__main__.AudioDataset'>, <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_dataset, epochs=3, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved model to GPU if available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n",
      "Error loading audio file audiosets/ontology/Electric guitar_4.wav: cannot access local variable 'parent' where it is not associated with a value\n",
      "Error loading audio at index 1324, returning dummy data.\n",
      "Successfully loaded audio: audiosets/ontology/Howl (wind)_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 530, shape: torch.Size([160000]), label: 115\n",
      "Successfully loaded audio: audiosets/ontology/Waves, surf_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 2218, shape: torch.Size([160000]), label: 413\n",
      "Successfully loaded audio: audiosets/ontology/Dog_5.wav, shape: (1473074,), dtype: float32\n",
      "Processed audio shape: torch.Size([1473074])\n",
      "Successfully loaded audio at index 1756, shape: torch.Size([160000]), label: 85\n",
      "Successfully loaded audio: audiosets/ontology/Howl_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1339, shape: torch.Size([160000]), label: 145\n",
      "Successfully loaded audio: audiosets/ontology/Organ_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2001, shape: torch.Size([160000]), label: 159\n",
      "Successfully loaded audio: audiosets/ontology/Baby cry, infant cry_8.wav, shape: (148237,), dtype: float32\n",
      "Processed audio shape: torch.Size([148237])\n",
      "Successfully loaded audio at index 928, shape: torch.Size([160000]), label: 64\n",
      "Successfully loaded audio: audiosets/ontology/Screaming_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 170, shape: torch.Size([160000]), label: 137\n",
      "Successfully loaded audio: audiosets/ontology/Motorboat, speedboat_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 256, shape: torch.Size([160000]), label: 3\n",
      "Successfully loaded audio: audiosets/ontology/Electric shaver, electric razor_6.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 156, shape: torch.Size([160000]), label: 396\n",
      "Successfully loaded audio: audiosets/ontology/Speech synthesizer_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2133, shape: torch.Size([160000]), label: 411\n",
      "Successfully loaded audio: audiosets/ontology/Drip_1.wav, shape: (1693014,), dtype: float32\n",
      "Processed audio shape: torch.Size([1693014])\n",
      "Successfully loaded audio at index 794, shape: torch.Size([160000]), label: 397\n",
      "Successfully loaded audio: audiosets/ontology/Electric guitar_2.wav, shape: (8762840,), dtype: float32\n",
      "Processed audio shape: torch.Size([8762840])\n",
      "Successfully loaded audio at index 593, shape: torch.Size([160000]), label: 134\n",
      "Successfully loaded audio: audiosets/ontology/Gunshot, gunfire_0.wav, shape: (2049858,), dtype: float32\n",
      "Processed audio shape: torch.Size([2049858])\n",
      "Successfully loaded audio at index 730, shape: torch.Size([160000]), label: 439\n",
      "Successfully loaded audio: audiosets/ontology/Owl_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2037, shape: torch.Size([160000]), label: 145\n",
      "Successfully loaded audio: audiosets/ontology/Acoustic guitar_0.wav, shape: (3946836,), dtype: float32\n",
      "Processed audio shape: torch.Size([3946836])\n",
      "Successfully loaded audio at index 1966, shape: torch.Size([160000]), label: 498\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1] at entry 0 and [160000] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 176\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    175\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_values\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    171\u001b[0m         {\n\u001b[1;32m--> 172\u001b[0m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[0;32m    176\u001b[0m         }\n\u001b[0;32m    177\u001b[0m     )\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1] at entry 0 and [160000] at entry 1"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "import torch\n",
    "import librosa\n",
    "from collections import namedtuple\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Set to block CUDA errors\n",
    "\n",
    "# Load the class map CSV (mapping from mid to index)\n",
    "class_map_df = pd.read_csv('yamnet_class_map.csv')\n",
    "class_map = pd.read_csv('yamnet_class_map.csv').set_index('display_name').to_dict()['mid']\n",
    "\n",
    "# Create a mapping from mid (string) to index (integer)\n",
    "mid_to_index = {mid: idx for idx, mid in enumerate(set(class_map.values()))}\n",
    "\n",
    "# Initialize the model and processor\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained('facebook/wav2vec2-base-960h', num_labels=521)\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Moved model to GPU if available\")\n",
    "\n",
    "# Define a namedtuple for dataset items\n",
    "AudioSample = namedtuple(\"AudioSample\", [\"input_values\", \"labels\"])\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_directory, ontology_file, mid_to_index):\n",
    "        with open(ontology_file, 'r') as f:\n",
    "            self.ontology_data = json.load(f)\n",
    "\n",
    "        self.mid_to_index = mid_to_index\n",
    "        self.audio_directory = audio_directory\n",
    "        self.audio_files = glob.glob(os.path.join(self.audio_directory, '**', '*.wav'), recursive=True)\n",
    "        \n",
    "        # Populate the dataset by calling prepare_data\n",
    "        self.data = self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        data = []\n",
    "        for category in self.ontology_data:\n",
    "            if \"positive_examples\" in category:\n",
    "                category_name = category[\"name\"]\n",
    "                mid = category[\"id\"]  # Get the mid for the current category\n",
    "\n",
    "                # Use the mid to get the index from the mid_to_index\n",
    "                if mid in self.mid_to_index:\n",
    "                    label = self.mid_to_index[mid]  # Get the integer index as the label\n",
    "                else:\n",
    "                    label = -1  # Default to -1 if not found\n",
    "\n",
    "                for audio_file in self.audio_files:\n",
    "                    if category_name.lower() in audio_file.lower():\n",
    "                        audio_file = audio_file.replace(\"\\\\\", \"/\")\n",
    "                        data.append({\"audio\": audio_file, \"label\": label})\n",
    "        return data\n",
    "    \n",
    "    def load_audio(self, file_path):\n",
    "        \"\"\"Load and preprocess audio using Wav2Vec2Processor.\"\"\"\n",
    "        try:\n",
    "            if not os.path.isfile(file_path):\n",
    "                raise FileNotFoundError(f\"WAV file not found: {file_path}\")\n",
    "\n",
    "            # Load audio using librosa and resample to 16kHz\n",
    "            audio_data, sr = librosa.load(file_path, sr=16000)\n",
    "            print(f\"Successfully loaded audio: {file_path}, shape: {audio_data.shape}, dtype: {audio_data.dtype}\")\n",
    "\n",
    "            # Preprocess using Wav2Vec2Processor\n",
    "            inputs = processor(audio_data, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "            processed_audio = inputs.input_values.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "            print(f\"Processed audio shape: {processed_audio.shape}\")\n",
    "\n",
    "            # Return processed audio\n",
    "            return processed_audio\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get one item (audio, label) for the dataset.\"\"\"\n",
    "        sample = self.data[idx]\n",
    "        audio_data = self.load_audio(sample[\"audio\"])\n",
    "        label = sample[\"label\"]\n",
    "\n",
    "        # Ensure audio_data is valid\n",
    "        if audio_data is None:\n",
    "            print(f\"Error loading audio at index {idx}, returning dummy data.\")\n",
    "            return {\"input_values\": torch.zeros(1), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "        # Trim or pad audio to max_length\n",
    "        max_length = 160000  # Set a max_length for padding/truncating\n",
    "        if audio_data.shape[0] < max_length:\n",
    "            padding = torch.zeros(max_length - audio_data.shape[0])\n",
    "            audio_data = torch.cat([audio_data, padding])\n",
    "        else:\n",
    "            audio_data = audio_data[:max_length]\n",
    "\n",
    "        print(f\"Successfully loaded audio at index {idx}, shape: {audio_data.shape}, label: {label}\")\n",
    "        \n",
    "        return {\"input_values\": audio_data.clone().detach(), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Initialize the dataset and dataloaders\n",
    "audio_directory = r\"audiosets/ontology\"\n",
    "ontology_file = 'ontology.json'\n",
    "\n",
    "# Initialize dataset and prepare data\n",
    "dataset = AudioDataset(audio_directory, ontology_file, mid_to_index)\n",
    "\n",
    "# Now split the dataset into train and test sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(dataset.data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize train and test datasets using the split data\n",
    "train_dataset = AudioDataset(audio_directory, ontology_file, mid_to_index)\n",
    "test_dataset = AudioDataset(audio_directory, ontology_file, mid_to_index)\n",
    "\n",
    "# Assign the split data to the datasets\n",
    "train_dataset.data = train_data\n",
    "test_dataset.data = test_data\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    save_steps=10,\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Initialize train and test dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Pass the datasets without specifying the dataloaders in Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Training loop with memory clearing after each batch\n",
    "for epoch in range(int(training_args.num_train_epochs)):\n",
    "    print(f\"Training epoch {epoch + 1}\")\n",
    "    model.train()  # Set model to training mode\n",
    "    for batch in train_dataloader:\n",
    "        input_values = batch['input_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()  # Backward pass to compute gradients\n",
    "        \n",
    "        # Clear GPU cache to prevent memory overflow\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Run evaluation after each epoch (optional)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            print(f\"Evaluation loss: {loss.item()}\")\n",
    "\n",
    "    torch.cuda.empty_cache()  # Clear GPU cache after each epoch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
