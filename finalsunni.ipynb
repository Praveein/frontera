{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 1, 106560]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     73\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.wav\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with your audio file\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[43mprocess_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myamnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwav2vec2_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor_wav2vec2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 58\u001b[0m, in \u001b[0;36mprocess_audio\u001b[1;34m(file_name, yamnet, wav2vec2_model, processor_wav2vec2, params)\u001b[0m\n\u001b[0;32m     55\u001b[0m yamnet_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(yamnet_scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Average the scores over time\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Get Wav2Vec2 predictions (logits)\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m wav2vec2_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mwav2vec2_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs_wav2vec2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m wav2vec2_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(wav2vec2_outputs\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Check the shape of the wav2vec2_scores\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:2357\u001b[0m, in \u001b[0;36mWav2Vec2ForSequenceClassification.forward\u001b[1;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[0;32m   2354\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m   2355\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_weighted_layer_sum \u001b[38;5;28;01melse\u001b[39;00m output_hidden_states\n\u001b[1;32m-> 2357\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwav2vec2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2360\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2361\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2363\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_weighted_layer_sum:\n\u001b[0;32m   2366\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs[_HIDDEN_STATES_START_POSITION]\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1808\u001b[0m, in \u001b[0;36mWav2Vec2Model.forward\u001b[1;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1803\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1804\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m   1805\u001b[0m )\n\u001b[0;32m   1806\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1808\u001b[0m extract_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1809\u001b[0m extract_features \u001b[38;5;241m=\u001b[39m extract_features\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   1811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1812\u001b[0m     \u001b[38;5;66;03m# compute reduced attention_mask corresponding to feature vectors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:463\u001b[0m, in \u001b[0;36mWav2Vec2FeatureEncoder.forward\u001b[1;34m(self, input_values)\u001b[0m\n\u001b[0;32m    458\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    459\u001b[0m             conv_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    460\u001b[0m             hidden_states,\n\u001b[0;32m    461\u001b[0m         )\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 463\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mconv_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:360\u001b[0m, in \u001b[0;36mWav2Vec2GroupNormConvLayer.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m--> 360\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    361\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m    362\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 1, 106560]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "import tensorflow as tf\n",
    "import soundfile as sf\n",
    "import resampy  # Ensure this is imported\n",
    "\n",
    "import params as yamnet_params\n",
    "import yamnet as yamnet_model\n",
    "import features  # Import features.py\n",
    "\n",
    "# Initialize the Params class to access model parameters\n",
    "params = yamnet_params.Params()\n",
    "\n",
    "# Load YAMNet model and processor\n",
    "yamnet = yamnet_model.yamnet_frames_model(params)\n",
    "yamnet.load_weights('yamnet.h5')\n",
    "yamnet_classes = yamnet_model.class_names('yamnet_class_map.csv')\n",
    "\n",
    "# Load Wav2Vec2 model and processor\n",
    "wav2vec2_model = Wav2Vec2ForSequenceClassification.from_pretrained('facebook/wav2vec2-base-960h', num_labels=521)\n",
    "processor_wav2vec2 = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wav2vec2_model.to(device)\n",
    "\n",
    "# Ensemble method: Averaging Probabilities from both models\n",
    "def ensemble_average(yamnet_scores, wav2vec2_scores):\n",
    "    \"\"\"Combine predictions from YAMNet and Wav2Vec2 by averaging probabilities.\"\"\"\n",
    "    avg_scores = (yamnet_scores + wav2vec2_scores) / 2\n",
    "    return np.argmax(avg_scores, axis=1)\n",
    "\n",
    "def process_audio(file_name, yamnet, wav2vec2_model, processor_wav2vec2, params):\n",
    "    \"\"\"Load, preprocess the audio and make ensemble predictions.\"\"\"\n",
    "    # Load and preprocess the audio for YAMNet\n",
    "    wav_data, sr = sf.read(file_name, dtype=np.int16)\n",
    "    waveform = wav_data / 32768.0  # Normalize to [-1.0, +1.0]\n",
    "    waveform = waveform.astype('float32')\n",
    "\n",
    "    # Convert to mono and resample to the required sample rate\n",
    "    if len(waveform.shape) > 1:\n",
    "        waveform = np.mean(waveform, axis=1)\n",
    "    if sr != params.sample_rate:\n",
    "        waveform = resampy.resample(waveform, sr, params.sample_rate)  # Resample to 16kHz\n",
    "\n",
    "    # Preprocess audio for Wav2Vec2 (use Wav2Vec2Processor)\n",
    "    inputs_wav2vec2 = processor_wav2vec2(waveform, return_tensors=\"pt\", sampling_rate=params.sample_rate)\n",
    "\n",
    "    # Ensure the input tensor has a batch dimension of 1 (for a batch of size 1)\n",
    "    inputs_wav2vec2 = {key: val.unsqueeze(0).to(device) for key, val in inputs_wav2vec2.items()}  # Add batch dimension\n",
    "\n",
    "    # Get YAMNet predictions (scores)\n",
    "    yamnet_scores, _, _ = yamnet(waveform)\n",
    "    yamnet_scores = np.mean(yamnet_scores, axis=0)  # Average the scores over time\n",
    "\n",
    "    # Get Wav2Vec2 predictions (logits)\n",
    "    wav2vec2_outputs = wav2vec2_model(**inputs_wav2vec2)\n",
    "    wav2vec2_scores = torch.nn.functional.softmax(wav2vec2_outputs.logits, dim=-1).cpu().detach().numpy()\n",
    "\n",
    "    # Check the shape of the wav2vec2_scores\n",
    "    print(f\"Shape of Wav2Vec2 scores: {wav2vec2_scores.shape}\")  # Should be [1, 521] for batch size 1\n",
    "\n",
    "    # Combine the predictions using ensemble averaging\n",
    "    final_predictions = ensemble_average(yamnet_scores, wav2vec2_scores)\n",
    "\n",
    "    # Get the top 5 predictions\n",
    "    top5_i = np.argsort(final_predictions)[::-1][:5]\n",
    "    print(f\"{file_name} predictions:\\n\" + \n",
    "          '\\n'.join(f'  {yamnet_classes[i]:12s}: {final_predictions[i]:.3f}' for i in top5_i))\n",
    "\n",
    "# Example usage\n",
    "file_name = 'test.wav'  # Replace with your audio file\n",
    "process_audio(file_name, yamnet, wav2vec2_model, processor_wav2vec2, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should print True if CUDA is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved model to GPU if available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n",
      "Successfully loaded audio: audiosets/ontology/Electric guitar_4.wav, shape: (2106143,), dtype: float32\n",
      "Processed audio shape: torch.Size([2106143])\n",
      "Successfully loaded audio at index 1324, shape: torch.Size([160000]), label: 464\n",
      "Successfully loaded audio: audiosets/ontology/Howl (wind)_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 530, shape: torch.Size([160000]), label: 224\n",
      "Successfully loaded audio: audiosets/ontology/Waves, surf_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 2218, shape: torch.Size([160000]), label: 75\n",
      "Successfully loaded audio: audiosets/ontology/Dog_5.wav, shape: (1473074,), dtype: float32\n",
      "Processed audio shape: torch.Size([1473074])\n",
      "Successfully loaded audio at index 1756, shape: torch.Size([160000]), label: 332\n",
      "Successfully loaded audio: audiosets/ontology/Howl_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1339, shape: torch.Size([160000]), label: 359\n",
      "Successfully loaded audio: audiosets/ontology/Organ_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2001, shape: torch.Size([160000]), label: 406\n",
      "Successfully loaded audio: audiosets/ontology/Baby cry, infant cry_8.wav, shape: (148237,), dtype: float32\n",
      "Processed audio shape: torch.Size([148237])\n",
      "Successfully loaded audio at index 928, shape: torch.Size([160000]), label: 419\n",
      "Successfully loaded audio: audiosets/ontology/Screaming_0.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 170, shape: torch.Size([160000]), label: 276\n",
      "Successfully loaded audio: audiosets/ontology/Motorboat, speedboat_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 256, shape: torch.Size([160000]), label: 248\n",
      "Successfully loaded audio: audiosets/ontology/Electric shaver, electric razor_6.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 156, shape: torch.Size([160000]), label: 352\n",
      "Successfully loaded audio: audiosets/ontology/Speech synthesizer_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2133, shape: torch.Size([160000]), label: 20\n",
      "Successfully loaded audio: audiosets/ontology/Drip_1.wav, shape: (1693014,), dtype: float32\n",
      "Processed audio shape: torch.Size([1693014])\n",
      "Successfully loaded audio at index 794, shape: torch.Size([160000]), label: 80\n",
      "Successfully loaded audio: audiosets/ontology/Electric guitar_2.wav, shape: (8762840,), dtype: float32\n",
      "Processed audio shape: torch.Size([8762840])\n",
      "Successfully loaded audio at index 593, shape: torch.Size([160000]), label: 464\n",
      "Successfully loaded audio: audiosets/ontology/Gunshot, gunfire_0.wav, shape: (2049858,), dtype: float32\n",
      "Processed audio shape: torch.Size([2049858])\n",
      "Successfully loaded audio at index 730, shape: torch.Size([160000]), label: 381\n",
      "Successfully loaded audio: audiosets/ontology/Owl_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2037, shape: torch.Size([160000]), label: 359\n",
      "Successfully loaded audio: audiosets/ontology/Acoustic guitar_0.wav, shape: (3946836,), dtype: float32\n",
      "Processed audio shape: torch.Size([3946836])\n",
      "Successfully loaded audio at index 1966, shape: torch.Size([160000]), label: 234\n",
      "Successfully loaded audio: audiosets/ontology/Dishes, pots, and pans_4.wav, shape: (16732857,), dtype: float32\n",
      "Processed audio shape: torch.Size([16732857])\n",
      "Successfully loaded audio at index 19, shape: torch.Size([160000]), label: 410\n",
      "Successfully loaded audio: audiosets/ontology/Rapping_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2022, shape: torch.Size([160000]), label: 183\n",
      "Successfully loaded audio: audiosets/ontology/Police car (siren)_1.wav, shape: (1026390,), dtype: float32\n",
      "Processed audio shape: torch.Size([1026390])\n",
      "Successfully loaded audio at index 450, shape: torch.Size([160000]), label: 365\n",
      "Successfully loaded audio: audiosets/ontology/Pizzicato_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1705, shape: torch.Size([160000]), label: 170\n",
      "Successfully loaded audio: audiosets/ontology/Keys jangling_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1633, shape: torch.Size([160000]), label: 453\n",
      "Successfully loaded audio: audiosets/ontology/Gurgling_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 369, shape: torch.Size([160000]), label: 369\n",
      "Successfully loaded audio: audiosets/ontology/Singing bowl_3.wav, shape: (4215072,), dtype: float32\n",
      "Processed audio shape: torch.Size([4215072])\n",
      "Successfully loaded audio at index 1809, shape: torch.Size([160000]), label: 168\n",
      "Successfully loaded audio: audiosets/ontology/Knock_0.wav, shape: (666506,), dtype: float32\n",
      "Processed audio shape: torch.Size([666506])\n",
      "Successfully loaded audio at index 1707, shape: torch.Size([160000]), label: 296\n",
      "Successfully loaded audio: audiosets/ontology/Skidding_3.wav, shape: (9735849,), dtype: float32\n",
      "Processed audio shape: torch.Size([9735849])\n",
      "Successfully loaded audio at index 1032, shape: torch.Size([160000]), label: 517\n",
      "Successfully loaded audio: audiosets/ontology/Cattle, bovinae_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 345, shape: torch.Size([160000]), label: 243\n",
      "Successfully loaded audio: audiosets/ontology/Brass instrument_8.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2125, shape: torch.Size([160000]), label: 344\n",
      "Successfully loaded audio: audiosets/ontology/Alarm clock_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2374, shape: torch.Size([160000]), label: 165\n",
      "Successfully loaded audio: audiosets/ontology/Air conditioning_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2271, shape: torch.Size([160000]), label: 21\n",
      "Successfully loaded audio: audiosets/ontology/Humming_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1873, shape: torch.Size([160000]), label: 23\n",
      "Successfully loaded audio: audiosets/ontology/Cheering_7.wav, shape: (9303958,), dtype: float32\n",
      "Processed audio shape: torch.Size([9303958])\n",
      "Successfully loaded audio at index 1074, shape: torch.Size([160000]), label: 422\n",
      "Successfully loaded audio: audiosets/ontology/Moo_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2386, shape: torch.Size([160000]), label: 394\n",
      "Successfully loaded audio: audiosets/ontology/Eruption_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2098, shape: torch.Size([160000]), label: 100\n",
      "Successfully loaded audio: audiosets/ontology/Finger snapping_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 535, shape: torch.Size([160000]), label: 183\n",
      "Successfully loaded audio: audiosets/ontology/Subway, metro, underground_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1442, shape: torch.Size([160000]), label: 457\n",
      "Successfully loaded audio: audiosets/ontology/Chirp, tweet_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1688, shape: torch.Size([160000]), label: 47\n",
      "Successfully loaded audio: audiosets/ontology/Acoustic guitar_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1285, shape: torch.Size([160000]), label: 234\n",
      "Successfully loaded audio: audiosets/ontology/Laughter_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1519, shape: torch.Size([160000]), label: 64\n",
      "Successfully loaded audio: audiosets/ontology/Bark_6.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2190, shape: torch.Size([160000]), label: 4\n",
      "Successfully loaded audio: audiosets/ontology/Pant_7.wav, shape: (9656344,), dtype: float32\n",
      "Processed audio shape: torch.Size([9656344])\n",
      "Successfully loaded audio at index 525, shape: torch.Size([160000]), label: 117\n",
      "Successfully loaded audio: audiosets/ontology/Car alarm_4.wav, shape: (1509669,), dtype: float32\n",
      "Processed audio shape: torch.Size([1509669])\n",
      "Successfully loaded audio at index 1137, shape: torch.Size([160000]), label: 436\n",
      "Successfully loaded audio: audiosets/ontology/Electronic organ_0.wav, shape: (1603663,), dtype: float32\n",
      "Processed audio shape: torch.Size([1603663])\n",
      "Successfully loaded audio at index 227, shape: torch.Size([160000]), label: 406\n",
      "Successfully loaded audio: audiosets/ontology/Air brake_2.wav, shape: (1097840,), dtype: float32\n",
      "Processed audio shape: torch.Size([1097840])\n",
      "Successfully loaded audio at index 1686, shape: torch.Size([160000]), label: 176\n",
      "Successfully loaded audio: audiosets/ontology/Cough_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1338, shape: torch.Size([160000]), label: 326\n",
      "Successfully loaded audio: audiosets/ontology/Crack_0.wav, shape: (2562555,), dtype: float32\n",
      "Processed audio shape: torch.Size([2562555])\n",
      "Successfully loaded audio at index 50, shape: torch.Size([160000]), label: 339\n",
      "Successfully loaded audio: audiosets/ontology/Writing_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1088, shape: torch.Size([160000]), label: 162\n",
      "Successfully loaded audio: audiosets/ontology/Church bell_7.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1006, shape: torch.Size([160000]), label: 60\n",
      "Successfully loaded audio: audiosets/ontology/Crackle_0.wav, shape: (3105530,), dtype: float32\n",
      "Processed audio shape: torch.Size([3105530])\n",
      "Successfully loaded audio at index 2419, shape: torch.Size([160000]), label: 339\n",
      "Successfully loaded audio: audiosets/ontology/Light engine (high frequency)_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2164, shape: torch.Size([160000]), label: 491\n",
      "Successfully loaded audio: audiosets/ontology/Wind instrument, woodwind instrument_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2103, shape: torch.Size([160000]), label: 344\n",
      "Successfully loaded audio: audiosets/ontology/Fire alarm_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1570, shape: torch.Size([160000]), label: 115\n",
      "Successfully loaded audio: audiosets/ontology/Busy signal_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1437, shape: torch.Size([160000]), label: 260\n",
      "Successfully loaded audio: audiosets/ontology/Hi-hat_4.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 2252, shape: torch.Size([160000]), label: 423\n",
      "Successfully loaded audio: audiosets/ontology/Chewing, mastication_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 869, shape: torch.Size([160000]), label: 170\n",
      "Successfully loaded audio: audiosets/ontology/Bleat_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2185, shape: torch.Size([160000]), label: 205\n",
      "Successfully loaded audio: audiosets/ontology/Fly, housefly_1.wav, shape: (4418665,), dtype: float32\n",
      "Processed audio shape: torch.Size([4418665])\n",
      "Successfully loaded audio at index 786, shape: torch.Size([160000]), label: 93\n",
      "Successfully loaded audio: audiosets/ontology/Car passing by_2.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1837, shape: torch.Size([160000]), label: 510\n",
      "Successfully loaded audio: audiosets/ontology/Ice cream truck, ice cream van_4.wav, shape: (3212156,), dtype: float32\n",
      "Processed audio shape: torch.Size([3212156])\n",
      "Successfully loaded audio at index 355, shape: torch.Size([160000]), label: 497\n",
      "Successfully loaded audio: audiosets/ontology/Ice cream truck, ice cream van_3.wav, shape: (156039,), dtype: float32\n",
      "Processed audio shape: torch.Size([156039])\n",
      "Successfully loaded audio at index 2048, shape: torch.Size([160000]), label: 497\n",
      "Successfully loaded audio: audiosets/ontology/Oink_6.wav, shape: (147494,), dtype: float32\n",
      "Processed audio shape: torch.Size([147494])\n",
      "Successfully loaded audio at index 398, shape: torch.Size([160000]), label: 125\n",
      "Successfully loaded audio: audiosets/ontology/Bell_5.wav, shape: (2208311,), dtype: float32\n",
      "Processed audio shape: torch.Size([2208311])\n",
      "Successfully loaded audio at index 2217, shape: torch.Size([160000]), label: 60\n",
      "Successfully loaded audio: audiosets/ontology/Mosquito_4.wav, shape: (143778,), dtype: float32\n",
      "Processed audio shape: torch.Size([143778])\n",
      "Successfully loaded audio at index 1892, shape: torch.Size([160000]), label: 345\n",
      "Successfully loaded audio: audiosets/ontology/Telephone bell ringing_6.wav, shape: (158639,), dtype: float32\n",
      "Processed audio shape: torch.Size([158639])\n",
      "Successfully loaded audio at index 839, shape: torch.Size([160000]), label: 60\n",
      "Successfully loaded audio: audiosets/ontology/Goose_5.wav, shape: (6773354,), dtype: float32\n",
      "Processed audio shape: torch.Size([6773354])\n",
      "Successfully loaded audio at index 1996, shape: torch.Size([160000]), label: 322\n",
      "Successfully loaded audio: audiosets/ontology/Squawk_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1374, shape: torch.Size([160000]), label: 96\n",
      "Successfully loaded audio: audiosets/ontology/Buzzer_7.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 698, shape: torch.Size([160000]), label: 495\n",
      "Successfully loaded audio: audiosets/ontology/Typing_0.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 293, shape: torch.Size([160000]), label: 208\n",
      "Successfully loaded audio: audiosets/ontology/Mechanical fan_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1124, shape: torch.Size([160000]), label: 319\n",
      "Successfully loaded audio: audiosets/ontology/Hubbub, speech noise, speech babble_3.wav, shape: (3888135,), dtype: float32\n",
      "Processed audio shape: torch.Size([3888135])\n",
      "Successfully loaded audio at index 1375, shape: torch.Size([160000]), label: 92\n",
      "Successfully loaded audio: audiosets/ontology/Crowing, cock-a-doodle-doo_6.wav, shape: (145636,), dtype: float32\n",
      "Processed audio shape: torch.Size([145636])\n",
      "Successfully loaded audio at index 2346, shape: torch.Size([160000]), label: 416\n",
      "Successfully loaded audio: audiosets/ontology/French horn_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2079, shape: torch.Size([160000]), label: 99\n",
      "Successfully loaded audio: audiosets/ontology/Baby cry, infant cry_6.wav, shape: (261179,), dtype: float32\n",
      "Processed audio shape: torch.Size([261179])\n",
      "Successfully loaded audio at index 1212, shape: torch.Size([160000]), label: 419\n",
      "Successfully loaded audio: audiosets/ontology/Motorboat, speedboat_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1488, shape: torch.Size([160000]), label: 248\n",
      "Successfully loaded audio: audiosets/ontology/Gong_2.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1249, shape: torch.Size([160000]), label: 450\n",
      "Successfully loaded audio: audiosets/ontology/Snare drum_1.wav, shape: (8128099,), dtype: float32\n",
      "Processed audio shape: torch.Size([8128099])\n",
      "Successfully loaded audio at index 554, shape: torch.Size([160000]), label: 492\n",
      "Successfully loaded audio: audiosets/ontology/Fowl_12.wav, shape: (2871659,), dtype: float32\n",
      "Processed audio shape: torch.Size([2871659])\n",
      "Successfully loaded audio at index 361, shape: torch.Size([160000]), label: 238\n",
      "Successfully loaded audio: audiosets/ontology/Crying, sobbing_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 945, shape: torch.Size([160000]), label: 413\n",
      "Successfully loaded audio: audiosets/ontology/Boom_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 73, shape: torch.Size([160000]), label: 400\n",
      "Successfully loaded audio: audiosets/ontology/Thunderstorm_1.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 1465, shape: torch.Size([160000]), label: 328\n",
      "Successfully loaded audio: audiosets/ontology/Wind noise (microphone)_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 818, shape: torch.Size([160000]), label: 301\n",
      "Successfully loaded audio: audiosets/ontology/Helicopter_5.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2431, shape: torch.Size([160000]), label: 45\n",
      "Successfully loaded audio: audiosets/ontology/Female singing_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 215, shape: torch.Size([160000]), label: 110\n",
      "Successfully loaded audio: audiosets/ontology/Hiss_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2397, shape: torch.Size([160000]), label: 137\n",
      "Successfully loaded audio: audiosets/ontology/Whoop_1.wav, shape: (4567458,), dtype: float32\n",
      "Processed audio shape: torch.Size([4567458])\n",
      "Successfully loaded audio at index 1939, shape: torch.Size([160000]), label: 127\n",
      "Successfully loaded audio: audiosets/ontology/Roaring cats (lions, tigers)_4.wav, shape: (2599149,), dtype: float32\n",
      "Processed audio shape: torch.Size([2599149])\n",
      "Successfully loaded audio at index 1082, shape: torch.Size([160000]), label: 146\n",
      "Successfully loaded audio: audiosets/ontology/Clapping_0.wav, shape: (1497223,), dtype: float32\n",
      "Processed audio shape: torch.Size([1497223])\n",
      "Successfully loaded audio at index 133, shape: torch.Size([160000]), label: 203\n",
      "Successfully loaded audio: audiosets/ontology/Singing_7.wav, shape: (3518845,), dtype: float32\n",
      "Processed audio shape: torch.Size([3518845])\n",
      "Successfully loaded audio at index 966, shape: torch.Size([160000]), label: 110\n",
      "Successfully loaded audio: audiosets/ontology/Animal_6.wav, shape: (1427935,), dtype: float32\n",
      "Processed audio shape: torch.Size([1427935])\n",
      "Successfully loaded audio at index 352, shape: torch.Size([160000]), label: 415\n",
      "Successfully loaded audio: audiosets/ontology/Fireworks_5.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 793, shape: torch.Size([160000]), label: 266\n",
      "Successfully loaded audio: audiosets/ontology/Livestock, farm animals, working animals_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 2080, shape: torch.Size([160000]), label: 432\n",
      "Successfully loaded audio: audiosets/ontology/Medium engine (mid frequency)_3.wav, shape: (418183,), dtype: float32\n",
      "Processed audio shape: torch.Size([418183])\n",
      "Successfully loaded audio at index 468, shape: torch.Size([160000]), label: 149\n",
      "Successfully loaded audio: audiosets/ontology/Bass guitar_3.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 2007, shape: torch.Size([160000]), label: 428\n",
      "Successfully loaded audio: audiosets/ontology/Tambourine_4.wav, shape: (160125,), dtype: float32\n",
      "Processed audio shape: torch.Size([160125])\n",
      "Successfully loaded audio at index 937, shape: torch.Size([160000]), label: 324\n",
      "Successfully loaded audio: audiosets/ontology/Bleat_6.wav, shape: (147494,), dtype: float32\n",
      "Processed audio shape: torch.Size([147494])\n",
      "Successfully loaded audio at index 1582, shape: torch.Size([160000]), label: 205\n",
      "Successfully loaded audio: audiosets/ontology/Snicker_1.wav, shape: (159754,), dtype: float32\n",
      "Processed audio shape: torch.Size([159754])\n",
      "Successfully loaded audio at index 1373, shape: torch.Size([160000]), label: -1\n",
      "Successfully loaded audio: audiosets/ontology/Bass guitar_5.wav, shape: (2306392,), dtype: float32\n",
      "Processed audio shape: torch.Size([2306392])\n",
      "Successfully loaded audio at index 519, shape: torch.Size([160000]), label: 428\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 180\u001b[0m\n\u001b[0;32m    177\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Forward passl\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m    182\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backward pass to compute gradients\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:2387\u001b[0m, in \u001b[0;36mWav2Vec2ForSequenceClassification.forward\u001b[1;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[0;32m   2385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2386\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m-> 2387\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   2390\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[_HIDDEN_STATES_START_POSITION:]\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1295\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\prave\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3493\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "import torch\n",
    "import librosa\n",
    "from collections import namedtuple\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Set to block CUDA errors\n",
    "\n",
    "# Load the class map CSV (mapping from mid to index)\n",
    "class_map_df = pd.read_csv('yamnet_class_map.csv')\n",
    "class_map = pd.read_csv('yamnet_class_map.csv').set_index('display_name').to_dict()['mid']\n",
    "\n",
    "# Create a mapping from mid (string) to index (integer)\n",
    "mid_to_index = {mid: idx for idx, mid in enumerate(set(class_map.values()))}\n",
    "\n",
    "# Initialize the model and processor\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained('facebook/wav2vec2-base-960h', num_labels=521)\n",
    "processor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Moved model to GPU if available\")\n",
    "\n",
    "# Define a namedtuple for dataset items\n",
    "AudioSample = namedtuple(\"AudioSample\", [\"input_values\", \"labels\"])\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_directory, ontology_file, mid_to_index):\n",
    "        with open(ontology_file, 'r') as f:\n",
    "            self.ontology_data = json.load(f)\n",
    "\n",
    "        self.mid_to_index = mid_to_index\n",
    "        self.audio_directory = audio_directory\n",
    "        self.audio_files = glob.glob(os.path.join(self.audio_directory, '**', '*.wav'), recursive=True)\n",
    "        \n",
    "        # Populate the dataset by calling prepare_data\n",
    "        self.data = self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        data = []\n",
    "        for category in self.ontology_data:\n",
    "            if \"positive_examples\" in category:\n",
    "                category_name = category[\"name\"]\n",
    "                mid = category[\"id\"]  # Get the mid for the current category\n",
    "\n",
    "                # Use the mid to get the index from the mid_to_index\n",
    "                if mid in self.mid_to_index:\n",
    "                    label = self.mid_to_index[mid]  # Get the integer index as the label\n",
    "                else:\n",
    "                    label = -1  # Default to -1 if not found\n",
    "\n",
    "                for audio_file in self.audio_files:\n",
    "                    if category_name.lower() in audio_file.lower():\n",
    "                        audio_file = audio_file.replace(\"\\\\\", \"/\")\n",
    "                        data.append({\"audio\": audio_file, \"label\": label})\n",
    "        return data\n",
    "    \n",
    "    def load_audio(self, file_path):\n",
    "        \"\"\"Load and preprocess audio using Wav2Vec2Processor.\"\"\"\n",
    "        try:\n",
    "            if not os.path.isfile(file_path):\n",
    "                raise FileNotFoundError(f\"WAV file not found: {file_path}\")\n",
    "\n",
    "            # Load audio using librosa and resample to 16kHz\n",
    "            audio_data, sr = librosa.load(file_path, sr=16000)\n",
    "            print(f\"Successfully loaded audio: {file_path}, shape: {audio_data.shape}, dtype: {audio_data.dtype}\")\n",
    "\n",
    "            # Preprocess using Wav2Vec2Processor\n",
    "            inputs = processor(audio_data, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
    "            processed_audio = inputs.input_values.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "            print(f\"Processed audio shape: {processed_audio.shape}\")\n",
    "\n",
    "            # Return processed audio\n",
    "            return processed_audio\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get one item (audio, label) for the dataset.\"\"\"\n",
    "        sample = self.data[idx]\n",
    "        audio_data = self.load_audio(sample[\"audio\"])\n",
    "        label = sample[\"label\"]\n",
    "\n",
    "        # Ensure audio_data is valid\n",
    "        if audio_data is None:\n",
    "            print(f\"Error loading audio at index {idx}, returning dummy data.\")\n",
    "            return {\"input_values\": torch.zeros(1), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "        # Trim or pad audio to max_length\n",
    "        max_length = 160000  # Set a max_length for padding/truncating\n",
    "        if audio_data.shape[0] < max_length:\n",
    "            padding = torch.zeros(max_length - audio_data.shape[0])\n",
    "            audio_data = torch.cat([audio_data, padding])\n",
    "        else:\n",
    "            audio_data = audio_data[:max_length]\n",
    "\n",
    "        print(f\"Successfully loaded audio at index {idx}, shape: {audio_data.shape}, label: {label}\")\n",
    "        \n",
    "        return {\"input_values\": audio_data.clone().detach(), \"labels\": torch.tensor(label, dtype=torch.long)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# Initialize the dataset and dataloaders\n",
    "audio_directory = r\"audiosets/ontology\"\n",
    "ontology_file = 'ontology.json'\n",
    "\n",
    "# Initialize dataset and prepare data\n",
    "dataset = AudioDataset(audio_directory, ontology_file, mid_to_index)\n",
    "\n",
    "# Now split the dataset into train and test sets (80% train, 20% test)\n",
    "train_data, test_data = train_test_split(dataset.data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize train and test datasets using the split data\n",
    "train_dataset = AudioDataset(audio_directory, ontology_file, mid_to_index)\n",
    "test_dataset = AudioDataset(audio_directory, ontology_file, mid_to_index)\n",
    "\n",
    "# Assign the split data to the datasets\n",
    "train_dataset.data = train_data\n",
    "test_dataset.data = test_data\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "# Training setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Directory where the model will be saved\n",
    "    evaluation_strategy=\"steps\",  # Save after each epoch\n",
    "    save_strategy=\"steps\",  # Save after each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=3,\n",
    "    save_steps=500,\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"tensorboard\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Initialize train and test dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Pass the datasets without specifying the dataloaders in Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Manually save the model after each epoch\n",
    "for epoch in range(int(training_args.num_train_epochs)):\n",
    "    print(f\"Training epoch {epoch + 1}\")\n",
    "    model.train()  # Set model to training mode\n",
    "    for batch in train_dataloader:\n",
    "        input_values = batch['input_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward passl\n",
    "        outputs = model(input_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()  # Backward pass to compute gradients\n",
    "        \n",
    "        # Clear GPU cache to prevent memory overflow\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Manually save the model after each epoch\n",
    "    model.save_pretrained(f\"./results/checkpoint-{epoch+1}\")\n",
    "    print(f\"Model saved at checkpoint-{epoch+1}\")\n",
    "    \n",
    "    # Run evaluation after each epoch (optional)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            print(f\"Evaluation loss: {loss.item()}\")\n",
    "\n",
    "    torch.cuda.empty_cache()  # Clear GPU cache after each epoch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
