{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:91: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:91: SyntaxWarning: invalid escape sequence '\\o'\n",
      "C:\\Users\\prave\\AppData\\Local\\Temp\\ipykernel_28100\\2131567478.py:91: SyntaxWarning: invalid escape sequence '\\o'\n",
      "  file_name = 'audiosets\\ontology\\Aircraft_0.wav'  # Replace with your audio file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audiosets\\ontology\\Aircraft_0.wav predictions:\n",
      "\n",
      "  Music       : 0.331\n",
      "  Silence     : 0.076\n",
      "  Speech      : 0.026\n",
      "  Scary music : 0.023\n",
      "  Rock music  : 0.007\n"
     ]
    }
   ],
   "source": [
    "    import numpy as np\n",
    "    import torch\n",
    "    import librosa\n",
    "    from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "    import tensorflow as tf\n",
    "    import soundfile as sf\n",
    "    import resampy  # Import resampy\n",
    "\n",
    "    import params as yamnet_params\n",
    "    import yamnet as yamnet_model\n",
    "    import features  # Import features.py\n",
    "\n",
    "    # Initialize the Params class to access model parameters\n",
    "    params = yamnet_params.Params()\n",
    "\n",
    "    # Load YAMNet model and processor\n",
    "    yamnet = yamnet_model.yamnet_frames_model(params)\n",
    "    yamnet.load_weights('yamnet.h5')\n",
    "    yamnet_classes = yamnet_model.class_names('yamnet_class_map.csv')\n",
    "\n",
    "    # Specify the directory where the model and config are saved\n",
    "    model_path = r\"results\\checkpoint-1\"  # Replace with your actual path\n",
    "\n",
    "    # Load the model from the saved checkpoint\n",
    "    wav2vec2_model = Wav2Vec2ForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "    # Use the processor from the base model as a fallback (if preprocessor_config.json is missing)\n",
    "    processor_wav2vec2 = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    wav2vec2_model.to(device)\n",
    "\n",
    "    # Ensemble method: Averaging Probabilities from both models\n",
    "    def ensemble_average(yamnet_scores, wav2vec2_scores):\n",
    "        \"\"\"Combine predictions from YAMNet and Wav2Vec2 by averaging probabilities.\"\"\"\n",
    "        avg_scores = (yamnet_scores + wav2vec2_scores) / 2\n",
    "        return avg_scores.flatten()  # Return the averaged scores (not the indices)\n",
    "\n",
    "    def process_audio(file_name, yamnet, wav2vec2_model, processor_wav2vec2, params):\n",
    "        \"\"\"Load, preprocess the audio and make ensemble predictions.\"\"\"\n",
    "        # Load and preprocess the audio for YAMNet\n",
    "        wav_data, sr = sf.read(file_name, dtype=np.int16)\n",
    "        waveform = wav_data / 32768.0  # Normalize to [-1.0, +1.0]\n",
    "        waveform = waveform.astype('float32')\n",
    "\n",
    "        # Convert to mono and resample to the required sample rate\n",
    "        if len(waveform.shape) > 1:\n",
    "            waveform = np.mean(waveform, axis=1)\n",
    "        if sr != params.sample_rate:\n",
    "            waveform = resampy.resample(waveform, sr, params.sample_rate)  # Resample to 16kHz\n",
    "\n",
    "        # Ensure waveform is of size 160000 (pad or trim)\n",
    "        max_length = 160000\n",
    "        if waveform.shape[0] < max_length:\n",
    "            # Pad the waveform if it's shorter than the required size\n",
    "            waveform = np.pad(waveform, (0, max_length - waveform.shape[0]))\n",
    "        else:\n",
    "            # Trim the waveform if it's longer than the required size\n",
    "            waveform = waveform[:max_length]\n",
    "\n",
    "        # Preprocess audio for Wav2Vec2 (use Wav2Vec2Processor)\n",
    "        inputs_wav2vec2 = processor_wav2vec2(waveform, return_tensors=\"pt\", sampling_rate=params.sample_rate)\n",
    "\n",
    "        # Ensure the input tensor has a batch dimension of 1 (for a batch of size 1)\n",
    "        inputs_wav2vec2 = {key: val.squeeze(0).unsqueeze(0).to(device) for key, val in inputs_wav2vec2.items()}  # Add batch dimension correctly\n",
    "\n",
    "        # Get YAMNet predictions (scores)\n",
    "        yamnet_scores, _, _ = yamnet(waveform)\n",
    "        yamnet_scores = np.mean(yamnet_scores, axis=0)  # Average the scores over time\n",
    "\n",
    "        # Get Wav2Vec2 predictions (logits)\n",
    "        wav2vec2_outputs = wav2vec2_model(**inputs_wav2vec2)\n",
    "        wav2vec2_scores = torch.nn.functional.softmax(wav2vec2_outputs.logits, dim=-1).cpu().detach().numpy()\n",
    "\n",
    "        # Combine the predictions using ensemble averaging\n",
    "        final_predictions = ensemble_average(yamnet_scores, wav2vec2_scores)\n",
    "        #print(np.shape(final_predictions))\n",
    "        #print(final_predictions)\n",
    "        # Get the top 5 predictions and their probabilities\n",
    "        top5_i = np.argsort(final_predictions)[::-1][:5]  # Sort indices in descending order to get top 5\n",
    "        #print(np.shape(top5_i))\n",
    "        #print(top5_i)\n",
    "        top5_probs = final_predictions[top5_i]  # Get the probabilities of the top 5 predictions\n",
    "        #print(top5_probs,np.max(final_predictions))\n",
    "        # Print the top 5 predictions with their probabilities\n",
    "        print(f\"{file_name} predictions:\\n\")\n",
    "        for i in range(5):\n",
    "            print(f'  {yamnet_classes[top5_i[i]]:12s}: {top5_probs[i]:.3f}')\n",
    "\n",
    "    # Example usage\n",
    "    file_name = 'audiosets\\ontology\\Aircraft_0.wav'  # Replace with your audio file\n",
    "    process_audio(file_name, yamnet, wav2vec2_model, processor_wav2vec2, params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.wav predictions:\n",
      "  Speech      : 494.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
    "import tensorflow as tf\n",
    "import soundfile as sf\n",
    "import resampy  # Import resampy\n",
    "\n",
    "import params as yamnet_params\n",
    "import yamnet as yamnet_model\n",
    "import features  # Import features.py\n",
    "\n",
    "# Initialize the Params class to access model parameters\n",
    "params = yamnet_params.Params()\n",
    "\n",
    "# Load YAMNet model and processor\n",
    "yamnet = yamnet_model.yamnet_frames_model(params)\n",
    "yamnet.load_weights('yamnet.h5')\n",
    "yamnet_classes = yamnet_model.class_names('yamnet_class_map.csv')\n",
    "\n",
    "# Specify the directory where the model and config are saved\n",
    "model_path = r\"results\\checkpoint-1\"  # Replace with your actual path\n",
    "\n",
    "# Load the model from the saved checkpoint\n",
    "wav2vec2_model = Wav2Vec2ForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# Use the processor from the base model as a fallback (if preprocessor_config.json is missing)\n",
    "processor_wav2vec2 = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wav2vec2_model.to(device)\n",
    "\n",
    "# Ensemble method: Averaging Probabilities from both models\n",
    "def ensemble_average(yamnet_scores, wav2vec2_scores):\n",
    "    \"\"\"Combine predictions from YAMNet and Wav2Vec2 by averaging probabilities.\"\"\"\n",
    "    avg_scores = (yamnet_scores + wav2vec2_scores) / 2\n",
    "    return np.argmax(avg_scores, axis=1)\n",
    "\n",
    "def process_audio(file_name, yamnet, wav2vec2_model, processor_wav2vec2, params):\n",
    "    \"\"\"Load, preprocess the audio and make ensemble predictions.\"\"\"\n",
    "    # Load and preprocess the audio for YAMNet\n",
    "    wav_data, sr = sf.read(file_name, dtype=np.int16)\n",
    "    waveform = wav_data / 32768.0  # Normalize to [-1.0, +1.0]\n",
    "    waveform = waveform.astype('float32')\n",
    "\n",
    "    # Convert to mono and resample to the required sample rate\n",
    "    if len(waveform.shape) > 1:\n",
    "        waveform = np.mean(waveform, axis=1)\n",
    "    if sr != params.sample_rate:\n",
    "        waveform = resampy.resample(waveform, sr, params.sample_rate)  # Resample to 16kHz\n",
    "\n",
    "    # Ensure waveform is of size 160000 (pad or trim)\n",
    "    max_length = 160000\n",
    "    if waveform.shape[0] < max_length:\n",
    "        # Pad the waveform if it's shorter than the required size\n",
    "        waveform = np.pad(waveform, (0, max_length - waveform.shape[0]))\n",
    "    else:\n",
    "        # Trim the waveform if it's longer than the required size\n",
    "        waveform = waveform[:max_length]\n",
    "\n",
    "    # Preprocess audio for Wav2Vec2 (use Wav2Vec2Processor)\n",
    "    inputs_wav2vec2 = processor_wav2vec2(waveform, return_tensors=\"pt\", sampling_rate=params.sample_rate)\n",
    "\n",
    "    # Ensure the input tensor has a batch dimension of 1 (for a batch of size 1)\n",
    "    inputs_wav2vec2 = {key: val.squeeze(0).unsqueeze(0).to(device) for key, val in inputs_wav2vec2.items()}  # Add batch dimension correctly\n",
    "\n",
    "    # Get YAMNet predictions (scores)\n",
    "    yamnet_scores, _, _ = yamnet(waveform)\n",
    "    yamnet_scores = np.mean(yamnet_scores, axis=0)  # Average the scores over time\n",
    "\n",
    "    # Get Wav2Vec2 predictions (logits)\n",
    "    wav2vec2_outputs = wav2vec2_model(**inputs_wav2vec2)\n",
    "    wav2vec2_scores = torch.nn.functional.softmax(wav2vec2_outputs.logits, dim=-1).cpu().detach().numpy()\n",
    "\n",
    "    # Check the shape of the wav2vec2_scores\n",
    "    #print(f\"Shape of Wav2Vec2 scores: {wav2vec2_scores.shape}\")  # Should be [1, 521] for batch size 1\n",
    "\n",
    "    # Combine the predictions using ensemble averaging\n",
    "    final_predictions = ensemble_average(yamnet_scores, wav2vec2_scores)\n",
    "\n",
    "    # Get the top 5 predictions\n",
    "    top5_i = np.argsort(final_predictions)[::-1][:5]\n",
    "    print(f\"{file_name} predictions:\\n\" + \n",
    "          '\\n'.join(f'  {yamnet_classes[i]:12s}: {final_predictions[i]:.3f}' for i in top5_i))\n",
    "\n",
    "# Example usage\n",
    "file_name = 'test.wav'  # Replace with your audio file\n",
    "process_audio(file_name, yamnet, wav2vec2_model, processor_wav2vec2, params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
